{
  "content": "A General Framework For Task-Oriented Network Inference Ivan Brugere\u0003Chris Kanich\u0003Tanya Y. Berger-Wolf\u0003 Abstract We present a brief introduction to a exible, general network inference framework which models data as a network space, sampled to optimize network structure to a particular task. We introduce a formal problem statement related to in u- ence maximization in networks, where the network structure is not given as input, but learned jointly with an in uence maximization solution. 1 Introduction Networks are extensively studied in machine learning, mathematics, physics, and other domain sciences [1, 4]. Often the entities and relationships are unambiguously known: two users are `friends' in a social network, or two road segments are adjacent in the network if they phys- ically intersect. Often, an underlying social, biological or other process generates data with latent relationships among entities. Rather than studying the process of in- terest through either coarse population-level statistics or isolated individual-level statistics, networks tend to represent complexity at multiple scales, and are general and reusable representations for di erent questions of interest on the process generating the original data. Previous work often focuses on ad-hoc, rule-based network construction, or model-based representational learning. Our exible, general framework encompasses and formalizes these approaches. Our framework also learns networks subject to many targets: cascade mod- eling and routing, node and edge classi cation, or in u- ence in networks. 2 General Framework Our framework transforms data from individual entities that are unambiguously known (e.g. users, IP addresses, genes) represented as nodes, into a space of networks which are then sampled under a set of cost constraints, and evaluated relative to a problem of interest. For a set of nodes fvi2Vg, a set of edge weight probability density functions fdij()2D: (i;j)2 (V\u0002V);dij()\u0018[0;1]g, a node attribute set fai2Ag, and a node label set fli2LgletG= (V;D;A;L) be aspace of weighted, attributed graphs.1A weighted, \u0003University of Illinois at Chicago 1Node labels are simply a speci c node attribute of interest forattributed graph G0= (V;E0;A;L) is drawn from G by sampling each edge weight distribution: E0=fe0 ij\u0018 dij() : (i;j)2(V\u0002V)g. Our general framework evaluates weighted graphs within Gaccording to some task T(G0;\u000f) subject to lossLT(G0). See Figure 1 for a schematic of this formalization. 3 Problem Formulation: Linear Threshold Process We instantiate a particular task on the above frame- work, to sample weighted networks which model a set of observed node labels Las the result of an Linear Threshold spreading process [3]. Given a weighted network, G0, the linear threshold process is initialized with klabeled nodes. At each time step, unlabeled nodes adopt the label of the neighborhood if the sum of neighbor weights exceeds the node's threshold. The process continues until label assignments stabilize.2For simplicity, we'll instantiate a global threshold ( ai= , for allai2A) binary label formulation ( li2f0;1gfor allli2L). Problem 1: Network Inference LT k-Seed Selec- tion Given: Graph-space G= (V;D;A;L) with A node linear thresholds Find: E0\u0018Dandknodes:S\u0012V Where: L is realized on E0through a linear threshold process initialized on seed-set S Minimizing: k This problem aims to nd the smallest set of nodes which produce the observed label set L, under thresholds A, when initializing the Linear Threshold model on the selected nodes S. A trivial solution exists wherek=jli= 1j, the total number of labeled nodes. We construct this solution by setting edges incident to li= 0 to 0 weight, and the nal Lis trivially realized on initialization. a subsequent task, de ned separately for notational convenience 2This assumes the susceptible-infected model. Nodes with li= 0 were necessarily never infected over the process.arXiv:1705.00645v1 [cs.SI] 1 May 2017\nij/u1D41Dij()01e*ije′ijaj= α lj= 1 wP(w)Figure 1: A schematic of our framework, showing a network space for 4 nodes. Each node vihas a label valueli(li=1 shaded), and a linear threshold attribute ai, set globally in this example. Each dashed edge between nodes denotes an edge weight density function. The shown dij() has an associated maximum likelihood, e\u0003 ij, an edge weight sample e0 ij, and the independent edge loss (red dashed lines). In the above problem, we are unconstrained by any loss functionL(G0). Therefore we are always ensured at leastjli= 1jminimal solutions, with k= 1. Proof sketch : Selecting any node where li= 1, we set edges incident to viwherelj= 1 such that ajis satis ed (simply: eij= 1), therefore we infer a star with binary edges of all labeled nodes for each solution. We allow our method to adjust e0 ijdirectly rather than sampling randomly from dij(). Recall that the range of dij() is [0;1]. Therefore even if P(dij() = 0) = 0, we allow setting e0 ij= 0. In the constrained case (below) we will be penalized for this unlikely or unobserved edge weight. 3.1 Independent Edge Loss We introduce a loss function measuring edge density function likelihood. This will incur cost when setting edge weights E0, conditioned on the respective edge density function. The independent edge loss measures the likelihood of a sampled edge, e0 ijagainst the edge's maximum likelihood estimate: e\u0003 ij=MLE(dij()):L(e0 ij;e\u0003 ij) =P(dij() =e\u0003 ij)\u0000P(dij() =e0 ij) (3.1) De ned over an entire realized graph, we get: L(G0) =X (i;j)2(V\u0002V)L(e0 ij;e\u0003 ij) (3.2) Problem 2: Budgeted Network Inference LT k- Seed Selection Given: Graph-space G= (V;D;A;L) with A node linear thresholds, budget \u0015 Find: E0\u0018Dandknodes:S\u0012V Where: L is realized on E0through a linear threshold process initialized on seed-set S Minimizing: k, subject toL(G0)\u0014\u0015 Problem 2 adds the Independent Edge Loss con- straint to the initial Network Inference LT k-Seed Se- lection problem, also accepting as input a loss budget \u0015. 3.2 Existence of solutions Problem 2 under in - nite budget \u0015=1is equivalent to Problem 1, yielding the samek= 1 solutions. Depending on nite \u0015, we cannot guarantee the existence of a solution. When \u0015= 0, there exists exactly one potential solution, the maximum likelihood edge weight set E\u0003, which may not produce Lunder any seeding. Proof, by example : Let E\u0003be a star with binary edge weights: E\u0003=fe0 ij2f0;1gg. Let the center of the star,vibe unlabeled: li= 0. The periphery of the star,S=Vviare all labeled: lj= 1. S must therefore be the seeds of the linear process. This is because viis unlabeled therefore will not propagate to any vj2S. Because all vj2Sare labeled, the binary edge weights incident tovisatisfy anythresholdai2[0;1], sovimust be labeled after the linear threshold process: li= 1. Therefore Lis not realizable on E\u0003 3.3 First Approximation Rather than formulating a solution to the weighted network case, let's consider only the binary case. In this case, we realize a network which satis es our loss budget L(G0)\u0014\u0015, wheree0 ij2 f0;1g, The In uence Maximization k-seed selection prob- lem generates candidates for the Budgeted Network In- ference LT k-Seed Selection problem, under this added constraint. A selected kseed labels cannot propagate through unlabeled vi. Therefore we assume edges inci- dent to unlabeled viwill not satisfy ai. This e ectively\ndisconnects each vi, yielding connected components of nodes labeled inL. An accepted seed-set for each sub- problem is the In uence Maximization k-seed selection solution which labels all nodes in the component. The total seed set is the union of these sub-problem seed sets. This approximates the optimal kfor one particular G0realization. However, it remains an open problem exploring the graph-space in an e\u000ecient way to improve a particular G0realization with respect to k. 4 Other Formulations This general pattern of \u0015-constrained graph sampling inG, subject to Independent Edge Loss generalizes to diverse tasks. For example, collective classi cation [5], is instantiated on labels Lused to train local classi ers on node attributes A. We sample G0on this task to maximizes classi er performance under \u0015loss constraints. In the area of in uence maximization and informa- tion networks, this framework can also incorporate dif- ferent transmission models (e.g. independent cascade), as well as parameterized rates of transmission on edges [2]. In information networks, edge weight density can be empirically measured from delay times between in- formation arrival at nodes. Once again, \u0015-constrained graph sampling in Grealize graphs to predict known cascades of information, which may perform better for prediction than the MLE graph G\u0003. 5 Conclusion and Open Problems This is only a brief outline of this general network inference framework for modeling non-network data for a particular task. We sample a space of networks from observed data, subject to loss constraints and a task objective. Future work will focus on e\u000ecient search strategies which take advantage of shared problems across tasks, and comparing graph-edit heuristics across di erent tasks. References [1] Ivan Brugere, Brian Gallagher, and Tanya Y Berger- Wolf. Network Structure Inference, A Survey: Motivations, Methods, and Applications. CoRR , abs/1610.0, 2016. URL http://arxiv.org/abs/ 1610.00782 . [2] Manuel Gomez Rodriguez, Jure Leskovec, and An- dreas Krause. Inferring networks of di usion and in uence. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2010) , volume 5 of KDD '10, pages 1019{1028, New York, NY, USA, feb2010. ACM. URL http://doi.acm.org/10.1145/ 1835804.1835933 . [3] David Kempe, Jon Kleinberg, and \u0013Eva Tardos. Max- imizing the Spread of In uence Through a Social Network. In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD '03, pages 137{146, New York, NY, USA, 2003. ACM. ISBN 1-58113-737- 0. URL http://doi.acm.org/10.1145/956750. 956769 . [4] Eric D. Kolaczyk. Network topology inference. InStatistical Analysis of Network Data SE - 7 , Springer Series in Statistics, pages 1{48. Springer New York, 2009. URL http://doi.acm.org/10. 1007/978-0-387-88146-1 . [5] Prithviraj Sen, Galileo Mark Namata, Mustafa Bil- gic, Lise Getoor, Brian Galligher, and Tina Eliassi- Rad. Collective classi cation in network data. AI magazine , 29(3):93{106, 2008.\n",
  "metadata": {
    "paper_id": "1705.00645v1",
    "downloaded_at": "2025-08-24T20:47:39.716808+00:00"
  },
  "processed_at": "2025-08-24T20:47:39.716821+00:00"
}