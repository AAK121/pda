{
  "content": "Jian Guo 1, 2 IDEA Research & The Hong Kong University of Science and Technology (Guangzhou) Saizhuo Wang 1 The Hong Kong University of Science and Technology & IDEA Research Lionel M. Ni The Hong Kong University of Science and Technology (Guangzhou) & The Hong Kong University of Science and Technology Heung - Yeung Shum 2 IDEA Research & The Hong Kong University of Science and Technology Keywords : AGI, Artificial Intelligence, AutoML, Causality Engineering, Deep Learning, Feature Engineering, Investment Engineering, Knowledge Graph, Knowledge Reasoning , Knowledge Representation, Model Compression, NAS, Quant 4.0, Quantitative Investment, Risk Graph, XAI idea.edu.cn IDEA Research Report Quant 4.0: Engineering Quantitative Investment with Automated, Explainable and Knowledge - driven Artificial Intelligence Future Trend and Perspective 1. Equal Contribution 2. Corresponding Author arXiv:2301.04020v1 [q-fin.CP] 13 Dec 2022\nTable of Contents 1 Introduction 1 1.1 Wealth Management and Quant . . . . . . . . . 1 1.2 Quant Strategies . . . . . . . . . . . . . . . . . 2 1.2.1 Components of Quant Strategy . . . . . 2 1.2.2 Examples of Popular Strategies . . . . 3 1.3 Fundamental Principles of Asset Management . 3 1.3.1 Fundamental Law of Active Management 3 1.3.2 Impossible Trinity of Investment . . . . 4 1.4 History of Quantitative Investment . . . . . . . 4 1.4.1 Q-Quant and P-Quant . . . . . . . . . 4 1.4.2 Landmarks in Q-Quant . . . . . . . . . 4 1.4.3 Landmarks in P-Quant . . . . . . . . . 6 1.4.4 Development of Quant in Industry . . . 7 1.5 Quant 4.0: Why and What . . . . . . . . . . . 8 1.5.1 Limitations of Quant 3.0 . . . . . . . . 8 1.5.2 What is Quant 4.0? . . . . . . . . . . . 8 2 Automated AI for Quant 4.0 9 2.1 Automating Quant Research Pipeline . . . . . . 9 2.1.1 Traditional Quant Pipeline . . . . . . . 9 2.1.2 Automated AI Quant Pipeline . . . . . 11 2.2 Automating Factor Mining . . . . . . . . . . . 11 2.2.1 Symbolic Factors . . . . . . . . . . . . 11 2.2.2 Machine Learning Factors . . . . . . . 13 2.3 Automated Modeling . . . . . . . . . . . . . . 13 2.3.1 Search Space . . . . . . . . . . . . . . 14 2.3.2 Search Algorithm . . . . . . . . . . . . 15 2.3.3 Accelerating Evaluation . . . . . . . . 15 2.4 Automated One-click Deployment . . . . . . . 16 2.4.1 Acceleration by Model Compilation . . 16 2.4.2 Acceleration by Model Compression . . 16 3 Explainable AI for Quant 4.0 16 3.1 Overview of Explainable AI . . . . . . . . . . 16 3.1.1 Model-intrinsic Explanation in XAI . . 17 3.1.2 Model-agnostic Explanation in XAI . . 19 3.2 Explainable AI for Quant . . . . . . . . . . . . 19 3.2.1 Explanation on Stock . . . . . . . . . . 19 3.2.2 Explanation on Time . . . . . . . . . . 20 3.2.3 Explanation on Factors . . . . . . . . . 21 4 Knowledge-driven AI for Quant 4.0 23 4.1 Knowledge Representation . . . . . . . . . . . 23 4.1.1 Knowledge Base Techniques . . . . . . 23 4.1.2 Knowledge Graph Techniques . . . . . 24 4.2 Knowledge Reasoning . . . . . . . . . . . . . 25 4.2.1 Symbolic Reasoning . . . . . . . . . . 25 4.2.2 Neural Reasoning . . . . . . . . . . . . 25 4.2.3 Neurosymbolic Reasoning . . . . . . . 25 4.3 Application in Quant . . . . . . . . . . . . . . 26 4.3.1 Building a Financial Knowledge Graph 26 4.3.2 Knowledge Reasoning for Quant . . . . 26 5 Building Quant 4.0: Engineering & Architecture 27 5.1 System for O \u000fine Research . . . . . . . . . . 275.1.1 Hardware Platform Architecture . . . . 27 5.1.2 Design of Data System . . . . . . . . . 29 5.1.3 Factor Mining System . . . . . . . . . 29 5.1.4 Knowledge-based System . . . . . . . 30 5.1.5 Modeling System . . . . . . . . . . . . 30 5.2 System for Online Trading . . . . . . . . . . . 30 5.2.1 Model Deployment . . . . . . . . . . . 30 5.2.2 Trading Execution . . . . . . . . . . . 31 5.2.3 Trading Analysis . . . . . . . . . . . . 31 6 Discussion on 10 Challenges in Quant Technology 31 6.1 Exponentially Growing Demand of Computing Power . . . . . . . . . . . . . . . . . . . . . . 31 6.1.1 Quant 4.0 and Supercomputers . . . . . 31 6.1.2 Solving Computing Power Dilemma . . 33 6.2 Alternative Data Technology . . . . . . . . . . 33 6.2.1 Examples of Alternative Data . . . . . 34 6.2.2 Problems in Data Acquisition . . . . . 34 6.2.3 Problems in Data Aggregation . . . . . 34 6.3 Financial Knowledge Engineering . . . . . . . 35 6.3.1 Di \u000eculties in Knowledge Engineering . 35 6.3.2 Knowledge Engineering vs Large Model 35 6.4 Financial Metaverse & World Model Simulator 35 6.4.1 Financial Metaverse Market Simulator . 35 6.4.2 World Model for Simulation . . . . . . 36 6.5 Cognitive AI & Causality Engineering . . . . . 36 6.5.1 Cognitive AI for Investment . . . . . . 36 6.5.2 Causality Engineering . . . . . . . . . 37 6.6 AI Risk Graph & Systematic Modeling . . . . . 37 6.6.1 Risk Graph for Systematic Modeling . 37 6.6.2 Complex Risk Measure for Investment . 37 6.7 Spatiotemporal Modeling . . . . . . . . . . . . 37 6.7.1 Unifying Cross-section & Time-series . 38 6.7.2 Spatiotemporal Graph for Quant . . . . 38 6.8 Universal Modeling . . . . . . . . . . . . . . . 38 6.8.1 Pretraining-Funetuning Paradigm . . . 38 6.8.2 Challenge in Quant Pretraining . . . . . 38 6.9 Robust Modeling . . . . . . . . . . . . . . . . 38 6.10 End-to-end Modeling . . . . . . . . . . . . . . 39 6.10.1 End-to-end Consistent Optimization . . 40 6.10.2 Learning Unstructured Data . . . . . . 40 7 Conclusion and Perspective 40 Acknowledgement 40 References 40 Author Biographies 53 2\nQuant 4.0: Engineering Quantitative Investment with Automated, Explainable and Knowledge-driven Artiﬁcial Intelligence Jian Guoa,c,1,\u0003, Saizhuo Wanga,b,1,2, Lionel M. Nib,c, Heung-Yeung Shuma,b,\u0003 aIDEA Research, International Digital Economy Academy, 5 Shihua Road, Futian District, Shenzhen, 518045, Guangdong, China bThe Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong, 999077, China cThe Hong Kong University of Science and Technology (Guangzhou), 1st Duxue Road, Nansha District, Guangzhou, 518055, Guangdong, China Abstract Quantitative investment (“quant”) is an interdisciplinary ﬁeld combining ﬁnancial engineering, computer science, mathematics, statistics, etc. Quant has become one of the mainstream investment methodologies over the past decades, and has experienced three generations: Quant 1.0, trading by mathematical modeling to discover mis-priced assets in markets; Quant 2.0, shifting quant research pipeline from small “strategy workshops” to large “alpha factories”; Quant 3.0, applying deep learning techniques to dis- cover complex nonlinear pricing rules. Despite its advantage in prediction, deep learning relies on extremely large data volume and labor-intensive tuning of “black-box” neural network models. To address these limitations, in this paper, we introduce Quant 4.0 and provide an engineering perspective for next-generation quant. Quant 4.0 has three key di erentiating components. First, Automated AI changes quant pipeline from traditional hand-craft modeling to the state-of-the-art automated modeling, practicing the philosophy of “algorithm produces algorithm, model builds model, and eventually AI creates AI”. Second, Explainable AI de- velops new techniques to better understand and interpret investment decisions made by machine learning black-boxes, and explains complicated and hidden risk exposures. Third, Knowledge-driven AI is a supplement to data-driven AI such as deep learning and it incorporates prior knowledge into modeling to improve investment decision, in particular for quantitative value investing. More- over, we discuss how to build a system that practices the Quant 4.0 concept. Finally, we propose ten challenging research problems for quant technology, and discuss potential solutions, research directions, and future trends. 1. Introduction Quantitative investment is an important part of wealth man- agement (a.k.a. asset management) industry. This section con- tains introductory knowledge about quant, including market sit- uation, classiﬁcation and principles of strategy development, historical landmarks, and concepts of Quant1.0–Quant4.0. 1.1. Wealth Management and Quant The wealth management industry is one of the largest sec- tors of the world’s economy. According to a global wealth re- port from Boston Consulting Group (BCG) [1] and the illus- tration in Figure 1, the volume of global ﬁnancial wealth has grown from 188.6 trillion USD in 2016 to 274.4 trillion USD in 2021, almost three times as the global nominal GDP in 2021. Moreover, the company predicts this number will increase to 355 trillion USD in 2026. It is not surprising that North Amer- ica, Asia, and Europe are the three biggest regional markets of wealth management in the world, with approximately 46%, 26%, and 21% of the global market size in 2021, respectively. \u0003Corresponding author. Email addresses: guojian@idea.edu.cn (Jian Guo), swangeh@connect.ust.hk (Saizhuo Wang), ni@ust.hk (Lionel M. Ni), hshum@idea.edu.cn (Heung-Yeung Shum) 1Equal contribution. 2This work was done during the internship at IDEA Research.We also see the stable and sustainable growth of the wealth management market, both globally and regionally. Figure 2 shows an ecosystem of the wealth management industry, where investment funds as well as fund managers (a.k.a. investment managers) play core roles. They raise money from various cap- ital providers, such as endowment foundations, fund of funds (FOF), family o \u000eces, billionaires, insurance companies, pen- sion/sovereign funds and retail clients, and invest this money into ﬁnancial markets to bet return and proﬁt for their cus- tomers. Many types of investment instruments are liked by fund managers, such as stocks, exchange-traded funds (ETFs), bonds, futures, options, and foreign exchange [2]. Some in- vestment funds even borrow money from depository institutions such as banks or peer-to-peer lending companies for investment and proﬁt from the di erence between investment return and loan interest. With the rapid development of digital economy, big data, and artiﬁcial intelligence, more and more new tech- nologies are applied in the wealth management industry, lead- ing to a branch of ﬁnancial technology /engineering, called “in- vestment engineering” [3]. Consequently, the pipeline of in- vestment research, trading execution, and risk management is becoming a systematic, automated, and intelligent process, and this philosophy has been practiced in the recent evolution of quant. As an important family of players in ﬁnancial markets and the wealth management industry, contemporary quant applies 1\n188.6 274.4 355 179.8 255.5 328.2 - 42.3 - 57.2 - 74.3 -100 -50 0 50 100 150 200 250 300 350 400 2016 2021 2026 (est) Global Wealth Market Size and Growth Financial Wealth Real Assets Liabili�es(a) Global market sizes of wealth management. 45.758.171 2016 2021 2026 Europe 1.42.12.8 2016 2021 2026 Africa 4.26.28.7 2016 2021 2026 Latin America 47.570.498.8 2016 2021 2026 Asia 3.856.5 2016 2021 2026 Middle East 81.8126.6159.4 2016 2021 2026 North America 4.267.9 2016 2021 2026 Oceania (b) Regional market sizes of wealth management. Figure 1: Global and regional market sizes of wealth management industry (unit: trillion USD). Panel (a) illustrates the volume of ﬁnancial wealth, real assets and liabilities in 2016, 2021 and 2026 (estimated) in the world. Panel (b) shows the distribution of ﬁnancial wealth in seven regional markets around the world in 2016, 2021 and 20226 (estimated). Data come from the report of BCG [1]. Figure 2: The asset management ecosystem [4] rigorous mathematical and statistical modeling techniques, ma- chine learning techniques, and algorithmic trading techniques to discover asset pricing abnormalities in ﬁnancial markets and make money from the following arbitrage or investment oppor- tunities. Compared with traditional fundamental and techni- cal investment, quantitative investment has a number of advan- tages. Firstly, the performance of quant strategies can be ex- amined and evaluated beforehand using back-test experiments based on historical data before the beginning of real trading. Secondly, quant trading has speed superiority in bidding orders with the best price. Thirdly, it eliminates the negative e ect of human emotion in decision-making. Finally, quant research has signiﬁcant advantages in data analysis with much deeper, broader and diversiﬁed coverage of information about ﬁnancial markets and sectors. In the past 30 years, information infras- tructure and computer technology are widely applied by ﬁnan- cial exchange markets around the world. Nowadays, massive ﬁnancial data are generated and millions of orders are executed every second, leading to the rapid growth of the quant industry. Taking the U.S. stock market as an example, over 60% of over- all trading volumes comes from the orders placed by computer trading algorithms rather than human traders [5].1.2. Quant Strategies A quant strategy is a systematic function or trading method- ology used for trading securities in ﬁnancial markets based on predeﬁned rules or trained models for making trading decisions. Strategies are usually the core intelligent property of a quanti- tative fund. 1.2.1. Components of Quant Strategy A standard quant strategy contains a series of components, such as investment instrument, trading frequency, trading mode, strategy type and data type, and we introduce them one by one (see Figure 3). Investment instrument speciﬁes which ﬁnancial instruments are put in the universe by the strategy. Popular candidate instruments include stocks, ETFs, bonds, foreign exchanges, convertible bonds, and cryptocurrencies, as well as more com- plicated ﬁnancial derivatives such as futures, options, swaps, and forwards [6]. An investment strategy could trade either a single type of instrument (e.g., a strategy for trading ETFs) or multiple types of instruments (e.g., an alpha hedging strategy that longs stocks and shorts index futures to eliminate market risks). Trading frequency speciﬁes how to hold your asset in port- folio and how frequently to trade. Usually, high-frequency trading holds a position in several minutes or seconds, while low-frequency trading may hold an asset over several months or years. Comparing high-frequency trading and low-frequency trading, the dramatic discrepancy of holding periods result in very di erent consideration in strategy design. For exam- ple, asset capacity limitations and trading costs are big issues for high-frequency trading, while how to control the risk of drawdown [7] is what we should carefully think about for low-frequency trading. Model type characterizes how to formally model the trading problem. Examples include cross-sectional trading, time- series trading, and event-driven trading [7]. Cross-sectional trading is used commonly in stock selection, where all stocks 2\nMomentum & Mean-reversion Hedging Arbitrage Market MarkingTrade Type Lead-lagStocks ETFs Bonds Forex Convertible Bond Crypto Futures Options Swaps ForwardsBasics DerivativesInstrument Model Type Time-seriesCross-sectional Event-drivenHigh Frequency Low FrequencyFrequency Medium FrequencyCross- sectional Time- series Event- driven Long-only Short-only Hedging Long-Short Arbitrage�Stock Multifactor Model with Long-only Trading �Future Time-series CTA �Stock Time-series Intraday Trading �High-frequency Future Time-series Trading �Activist Stock Investment �Distressed Cooperation Bond Investment�Stock Multifactor Long- Short Model �Future Cross-sectional CTA �Event-driven Global Macro �Even-driven High- frequency Alpha�Multi-asset Statistical Arbitrage �Triangular Arbitrage �High-frequency Cross- market Arbitrage �High-frequency Calendar Spread Arbitrage�Stock Time-series Trading Hedged by Index Future or Index Option �Merger Arbitrage �Even-driven Convertible ArbitrageExamples of Investment Strategies Data Type Limit Order BookPrice Volume Financial Statement Report News & Social Media Alternative DataFigure 3: Classiﬁcation of common strategies and investment instruments. in a universe are ranked according to their scores of expected future returns predicted by a model, and portfolio managers could long stocks with the highest scores and short those with the lowest scores. Time series trading is relatively simple, where long /short trading operates only on a single instrument such as a certain stock or a certain future contract. Event- driven trading di ers from time-series trading because the time intervals between events are not evenly distributed over time, while investment decisions and trading executions are triggered by the occurrence of events. Trade type is a series of thinking templates for us to design a strategy quickly. Examples include momentum trading [8], mean-reversion trading [9], arbitrage trading [10], hedging [11], market making [12], etc. By leveraging these strategy types, traders can explore proﬁt chances from di erent as- pects of ﬁnancial markets. Speciﬁcally, momentum trading assumes the price trend is sustainable in the following time window and it follows this trend direction to trade. Mean- reversion trading, on the contrary, bets the price trend will move towards the opposite direction in recent future and buy opposite positions. Hedging is the purchase of one asset with the intention of reducing the risk of loss from another asset. Arbitrage is simultaneously longing and shorting the same asset in di erent markets or a pair of highly correlated assets in order to proﬁt from the convergence of price discrepancy. Market making is a liquidity-providing trade that quotes both a buy and a sell price in a tradable asset held in inventory, hoping to make a proﬁt on the bid–ask spread. Data type means what type of data is used in a strategy. Typ- ical data types include quote data, limit order books [13] b, news data, ﬁnancial statements, analysts’ reports, and alter- native data such as sentimental data, location data, satellite images, etc. A strategy researcher must consider what kind of data he has and what kind of data he needs in a strategy development process. For example, limit order book streams are usually used in building high-frequency trading strate-gies, while news data are used more commonly in event- driven strategies. 1.2.2. Examples of Popular Strategies Figure 3 also list a number of popular strategies as exam- ples. For example, stock hedging strategy based on multifactor model [14] is very popular in many main markets around the world. This strategy hedges market risk by longing the most fa- vorable stocks and shorting the other end (in some markets pro- hibiting shorting, short the corresponding index future or index option instead). If we trade stocks with multifactor models in a long-only way without shorting and constrain the risk expo- sure between selected portfolios and certain stock indices, it is an enhanced indexing strategy, which is almost the most popu- lar quantitative strategy in China’s stock market if measured by assets under management (AUM). 1.3. Fundamental Principles of Asset Management Similar to the situation that learning law of energy conser- vation could help avoid the trap of perpetual motion machine, it is beneﬁcial to learn some fundamental principles of asset management so as to get rid of some common traps in strategy development. 1.3.1. Fundamental Law of Active Management The ﬁrst principle is the fundamental law of active man- agement developed by Richard Grinold and Ronald Kahn [15]. This principle states that the performance of an active invest- ment manager (or equivalently quant model) depends on the quality of investment skills and, consequently, the frequency of investment opportunities. This law can be expressed mathemat- ically as follows: IR=IC\u0002p Breadth (1) where ICis the information coe \u000ecient (correlation between the predicted return and true return in a future time window) 3\nevaluating investment quality, Breadth means the number of independent investment decisions in a year, and IRis the ra- tio of portfolio returns above the returns of a benchmark to the volatility of returns, measuring the performance of asset man- agement. Mathematically, the fundamental law of active man- agement can be regarded as an application of the central limit theorem in mathematical statistics [16]. When applying this law in practice, we have to notice that ICandBreadth are usually not independent. For example, given a strategy, we may in- crease its Breadth by relaxing the threshold of trading signals, but in this way, ICmay decrease because more false-positive noise is introduced to our decisions. Therefore, a good strat- egy should ﬁnd an optimal trade-o between these two coupled variables. Figure 4a illustrates the distribution of various pop- ular strategies on ICandBreadth , and their corresponding IR performance. 1.3.2. Impossible Trinity of Investment The second principle is the impossible trinity of asset man- agement. Speciﬁcally, any investment strategy can not meet the following three conditions simultaneously, i.e., high return, low risk (or equivalently high stability), and high capacity. Fig- ure 4b illustrates the impossible trinity using a radar chart with three variables return, stability and capacity. For example, high- frequency market making and calendar arbitrage strategy could reach high return and stability (low portfolio volatility), but the capacity of its AUM is usually small, typically hard to exceed several billions of USD even in global trading. On the contrary, stock fundamental strategy has high capacity up to trillions of USD, but its return and stability are not as good as those of high-frequency trading. 1.4. History of Quantitative Investment The origin of quant can trace back to over a century ago when French mathematician Louis Bachelier published his Ph.D. thesis “The Theory of Speculation” in 1900 [17] and he exhib- ited how to use probability law and mathematical tools to study the movement of stock prices. As a pioneer exploring the ap- plication of advanced mathematics in ﬁnancial markets, Bache- lier’s work inspired academic research of quantitative ﬁnance despite the lack of industry application due to data scarcity at his age. Quantitative investment was ﬁrst practiced by Amer- ican mathematics professor Edward Thorp, who used proba- bility theory and statistical analysis to win blackjack games, and his research was subsequently used to seek systematic and consistent returns in stock markets [18]. In this subsection, we introduce the history and landmarks in the development of quantitative ﬁnance through two routes: research landmarks in academia and evolution of quant in industry practice. 1.4.1. Q-Quant and P-Quant People in academia and investment industry classify quan- titative ﬁnance into two branches, which are usually referred to as “Q-quant” and “P-quant”. These two branches are named af- ter their di erentiation in modeling based on risk-neural mea- sure and probability measure, respectively. Generally speak-ing, Q-quant studies the problem of derivative pricing and ex- trapolate the present , using a model-driven research framework where data is usually used to adjust the parameters of models. On the other hand, P-quant studies quantitative risk and port- folio management to model the future , using a data-driven re- search framework where di erent models are built to improve the ﬁtting of historical data. Usually, Q-quant research is con- ducted in sell-side institutes such as investment banks and secu- rity companies, while P-quant is popular in buy-side institutes such as mutual funds and hedge funds. Table 1 compares the characteristics of these two types of quant. Table 1: Comparisons of P-quant and Q-quant [19]. Q-quant P-quant Goal Extrapolate the present Model the future Scenario Derivatives pricing Portfolio management Measure Risk-neural measure Probability measure Modeling Continuous stochastic process Discrete time series Example Black-Scholes model Multifactor model Algorithm Ito calculus, PDEs Statistics, Machine Learning Challenge Calibration Estimation /Prediction Business Sell-side Buy-side 1.4.2. Landmarks in Q-Quant In 1965, Paul Samuelson, American economist and the win- ner of 1970 Nobel Memorial Prize in Economic Sciences, intro- duced stochastic process and stochastic calculus tools in analyz- ing ﬁnancial markets and modeling the stochastic movement of stock prices [20], and in 1965, he published a paper studying the lifetime portfolio selection problem using a stochastic pro- gramming method [21]. In the same year, another American economist Robert Merton published his work about lifetime portfolio selection as well. Di erent from Samuelson’s work using discrete-time stochastic process, Merton’s work modeled the random uncertainty of portfolio using continuous-time stochas- tic calculus [22]. Almost in the same year, economists Fischer Black and Myron Scholes demonstrated that the expected re- turn and risk of assets under management could be removed by dynamically revising a portfolio, and thus inventing the risk- neutral strategy for derivative investment [23]. They applied the theory to real market trading and published it in 1973. The risk- neutral formula was later named in honor of them and called Black-Scholes Model [24], a partial di erential equation (PDE) tool for pricing a ﬁnancial market containing derivative invest- ment instruments. Speciﬁcally, the Black–Scholes model estab- lishes a partial di erential equation governing the price evolu- tion of a European option call or European option put, as fol- lows: @V @t+1 2\u001b2S2@2V @S2+rS@V @S\u0000rV=0 (2) where Vis the price of the option as a function of stock price S and time t,ris the risk-free interest rate, and \u001bis the volatility of the stock. This PDE has a closed-form solution called Black- Scholes Formula. Since Robert Merton was the ﬁrst to publish a paper expanding the mathematical understanding of the options pricing model, he was usually credited with the contribution of this theory as well. Merton and Scholes received the 1997 4\nHigh -freq Statistical Arbitrage Breadth|IC| Stock Cross -sectional High -freq Alpha Foundamental Value Investment Stock Cross -sectional Foundamental Alpha Cross -Sectional CTA LowHigh |IR|Event -Driven ArbitrageHigh -freq Market Making Global MacroRisk-free Arbitrage Quant CTA(a) Common investment strategies under the fundamental law of active management. The ﬁgure illustrates the relationship between the magnitude of IRwith breadth and the magnitude of ICfor di erent strategies. Return Stability Capacity Stock Fundamental Investment Future High-frequency Arbitrage Fix Income Investment(b) Illustration of the impossible trinity of return, capacity, and stability for ac- tive management using three typical strategies: stock fundamental investment, future high-frequency arbitrage and ﬁxed income investment. Figure 4: Illustration of the principles for active investment management with speciﬁc strategies. Théoriede la SpéculationBachelier1970 Nobel PrizePaul Samuelson 🏅 1900Stochastic Calculus for Asset PricingPaul Samuelson1965Capital Asset Pricing Model (1961-1966)Jack Treynor, William Sharpe, John Linter, Jan Mossi1966Continuous Option PricingRobert Merton1969Black-Scholes ModelBlack, Scholes, Merton1973Arbitrage PricingTheoryStephen Ross1976 Vector AutoregressionChristopher Sims1980Fundamental Theorem of Asset PricingHarrison and Pliska1981 2012 Nobel PrizeLloyd ShapleyContribution: Shapley Value 🏅ARCH/GARCHRobert Engle1982Co-integrationRobert Engle, Clive Granger1987Fama-French Three Factor ModelEugene Fama, Kenneth French1992Local Average Treatment EffectGuido Imbens, Joshua Angrist1994 1990 Nobel PrizeMarkowitz & Sharpe 🏅 2013 Nobel PrizeEugene Fama 🏅 2003 Nobel PrizeRobert Engle & Clive Granger 🏅2011 Nobel PrizeChristopher Sims 🏅2021 Nobel PrizeGuido Imbens, Joshua Angrist 🏅 2011 Turing AwardJudea PearlContribution: Calculus for Probabilistic and Causal Reasoning 🏅2018 Turing AwardG. Hinton, Y. Bengio, Y. LecunContribution: Deep Learning 🏅1997 Nobel PrizeScholes & Merton 🏅Heston ModelSteven Heston 1993Gaussian CopulaDavid X. Li2000SABR ModelHagan et al. 2002P-QuantQ-QuantDeep HedgingBuehler et al.2018Modern Portfolio TheoryHarry Markowitz1952 Figure 5: Main academic contributors and their works that deeply inﬂuence the development of quantitative investment. Photo credit: Wikipedia. Nobel Memorial Prize in Economic Sciences for their discov- ery of the risk-neutral dynamic revision. The original Black- Scholes model was extended later for deterministically variable rates and volatilities, and was extended to characterize the price of European options on instruments paying dividends, as well as American options and binary options. As a pioneering work in risk-neutral theory, Black-Scholes model has many limitations, one of which is the assumption that the underlying volatility is constant over the life of the deriva- tive, and is una ected by the changes in the price level of the underlying security. This assumption usually contradicts the phenomenon of the smile and skew shapes of implied volatility surfaces. A possible solution is to relax the constant volatility assumption. By characterizing the volatility of the underlying price using stochastic process, it is possible to model derivatives more accurately in practice, and this idea leads to a series of works about stochastic volatility, such as the Heston model [25] and the SABR model [26]. As a commonly used stochastic volatility model, the Heston model assumes the variation of the volatility process varies as a square root of the variance itself, and it exhibits a reversion trend towards the long-term mean of variance. Another popular stochastic volatility model is the SABR model, commonly used in interest rate derivative mar- kets. This model uses stochastic di erential equations to de-scribe a single forward (such as a LIBOR forward rate, a for- ward swap rate, or a forward stock price) as well as its volatil- ity, and has the ability to reproduce the e ect of volatility smile. In recent years, deep learning and reinforcement learning tech- niques are applied to integrate with risk neural Q-quant mod- eling and a concept learning to trade was introduced by Hans Buehler [27], who proposed the deep hedging model, a frame- work for hedging a portfolio of derivatives in the presence of market frictions such as transaction costs, market impact, liq- uidity constraints or risk limits and for modeling the volatility stochastic process using deep reinforcement learning and mar- ket simulation. It does not use the Greeks anymore and natu- rally captures co-movements of relevant market parameters. In addition to derivative pricing models, market e \u000eciency theory and risk modeling theory are also very important in Q- quant, both in academia and industry. In 1980s, Harrison and Pliska established the fundamental theorem of asset pricing [28], which provides a series of necessary and su \u000ecient conditions for an e \u000ecient market to be arbitrage free as well as complete. In 2000, David X. Li introduced the statistical model Gaus- sian copula [29] to evaluate the value-at-risk (VaR) of derivative pricing and portfolio optimization, especially the collateralized debt obligations (CDO). Gaussian copula quickly became a tool for ﬁnancial institutions to correlate associations between mul- 5\ntiple ﬁnancial securities since it is relatively simple in modeling even for those assets too complex to price previously, such as mortgages. PFEGMGNCMSFT Modern Portfolio Theory ……0.050.210.030.300.09𝑤!𝑤\"𝑤#𝑤#$!𝑤%Optimized Portfolio PositionsPortfolio Cumulative ReturnTrading and hold the positions in next N daysShift time windowExtract returns -0.200.20.40.60.81 (a) Illustration of Markowitz’ portfolio optimization theory. (b) Illustration of e \u000ecient frontier ﬁrst formulated by Harry Markowitz. Figure cited from [30]. Figure 6: Portfolio optimization 1.4.3. Landmarks in P-Quant Q-quant plays an extremely important role in quantitative ﬁ- nance. In this article, however, we stand on a buy-side point of view and focus on asset prediction and portfolio optimization problems, and thus all discussions about quantitative invest- ment in the following content assume a P-quant statement un- less otherwise speciﬁed. The origin of P-quant started from the establishment of modern portfolio theory introduced by Harry Markowitz. The theory was initialized in his Ph.D. thesis “Port- folio Selection” and later published in Journal of Finance in 1952 [31], and an extension published in his book Portfolio Se- lection: E \u000ecient Diversiﬁcation of Investments [32] in 1959. According to the old adage “Don’t put all your eggs in one bas- ket”, Markowitz came up with the concept of e \u000ecient frontierof asset investment in ﬁnancial market and formalized it math- ematically as a quadratic optimization problem by maximizing the expected return of the portfolio given its risk (usually mea- sured by the variance of the assets in a portfolio) at a certain level. Figure 6 illustrates the application of Markowitz’s the- ory for allocating the best positions for assets in portfolio and illustrates the concept of e \u000ecient frontier. Based on the modern portfolio theory, the Capital Asset Pricing Model (CAPM) was later introduced by Jack Treynor (1961, 1962) [33], William F. Sharpe (1964) [34], John Lintner (1965) [35] and Jan Mossin (1966) [36] independently. CAPM aims to describe the relationship between systematic risk from the market and expected return for assets. E(Rp)\u0000Rf= + \u0001(E(Rm)\u0000Rf) (3) where E(Rp) is the expect return of portfolio, Rfis the risk- free return, E(Rm) is the expected return of market. Speciﬁ- cally, CAPM decomposes asset return and risk into two sepa- rate parts, alpha and beta. Alpha measures the performance of a portfolio compared to a benchmark index (e.g., S&P500 index), while beta measures the variance of the portfolio in relation to a benchmark index, characterizing the risk from market volatility. One of the main contributors to CAPM, William Sharpe, shared the 1990 Nobel Prize with Harry Markowitz. A following im- portant step in quantitative ﬁnance is the establishment of ar- bitrage pricing theory (APT) by MIT economist Stephen Ross in 1976 [37]. APT improved its predecessor CAPM by fur- ther introducing the multifactor model framework to build the relationship between asset price and various macroeconomic risk variables. Under the multifactor model framework, Nobel Prize-winning economist Eugene Fama proposed the famous Fama–French Three-Factor Model with his colleague Kenneth French at the University of Chicago in 1992 [14]. E(Rp)\u0000Rf= 0+ 1\u0001(E(Rm)\u0000Rf)+ 2\u0001S MB + 3\u0001HML (4) This model establishes the relationship between the expected portfolio return (up to subtracting a risk-free return) E(rp)\u0000rf with respect to three systematic risk factors: expected mar- ket return E(Rm)\u0000Rf, size S MB (the spread between small capitalization stocks and large capitalization stocks), book-to- market values HML (the spread between high book-to-market companies and low book-to-market companies). The three- factor model was then extended to Fama and French Five Factor Model in 2015 [38], by adding two more factors: proﬁtability (return spread of the most proﬁtable ﬁrms minus the least prof- itable) and investment aggressiveness (the return spread of ﬁrms that invest conservatively minus aggressively). Parallel with the progress of multifactor models, a num- ber of signiﬁcant research about time-series analysis appears in 1980s. In 1980, Nobel Prize winner Christopher Sims intro- duced the Vector Autoregression (V AR) model into economics and ﬁnance. As an extension of single sequence autoregres- sive (AR) model and autoregressive-moving-average (ARMA) model commonly used in time-series analysis, V AR charac- terizes the autoregressive properties over time across multiple 6\ntimes series and it assumes constant variance of error terms in the regression formula. In 1982, Robert Engle introduced the Autoregressive Conditional Heteroskedasticity (ARCH) model and extend it to Generalized Autoregressive Conditional Het- eroskedasticity (GARCH) model to characterize the pattern of ﬁnancial volatility in the market by specifying stochastic vari- ance in the model. In 1987, he introduced co-integration method with Clive Granger (inventor of Granger Causality for mod- eling lead-lag patterns among multiple time series) for testing the signiﬁcance of mean-reversing patterns in ﬁnancial time se- ries, and co-integration test has been widely used in discovering promising asset pairs for statistical arbitrage strategies. Both Engle and Granger received the 2003 Nobel Prize for their con- tribution to time series analysis which has been widely applied in quantitative ﬁnance for market forecasting and investment research. In 2018, three pioneers in deep learning techniques, Yoshua Bengio, Geo rey Hinton and Yann LeCun, are granted the Turing Award. Nowadays, deep learning has been widely used by academic researchers in ﬁnance and quant researchers in ﬁnancial institutions to build complex nonlinear models in order to learn the relationship between ﬁnancial signals and ex- pected returns and to predict asset prices, and its powerful abil- ity in ﬁtting big data signiﬁcantly improves the performance of market prediction and portfolio management. Although accurately predicting the future trend of asset price is a very important task in P-Quant, how to explain the e ect of model prediction and interpret how a model is really working seems more important for quant researchers since “know how” is more crucial than “know what” in risk management for port- folio managers. Causal e ect analysis [39] and factor impor- tance analysis are two core tasks in quant model interpretation. Clive Granger invented the Granger Causality Test in 1969 [40] for determining whether one time series is useful in forecasting another. The original Granger causality test does not account for latent confounding e ects and does not capture instanta- neous and non-linear causal relationships, though several ex- tensions have been proposed to address these issues. Although there is an argument about whether Granger causality test can evaluate “real” causality in terms of statistics, this method has been widely applied in quant research such as searching and evaluating pairs of stocks with signiﬁcant lead-lag e ect and trading with corresponding strategies. In 1994, Guido Imbens and Joshua Angrist introduced the local average treatment ef- fect (LATE) model to characterize the statistical causal e ect in economics, ﬁnance and social sciences, and they shared the 2021 Nobel Prize in economics. Another important contribu- tor in causal inference is the Turing Award winner Judea Pearl, who invented the causal diagram (Bayesian network) and it can be used to mine the causal e ect among factors and returns in multifactor model. On the other hand, in the area of factor importance analysis, Shapley Value has become an important criterion for measuring the contribution of single feature in a complex nonlinear machine learning model. In fact, it is inter- esting that this criterion was invented originally to measure the contribution of individual player /agent in a cooperative gaming process when it was ﬁrst proposed by Lloyd Shapely, a Nobel Prize winner and a pioneer in game theory research.1.4.4. Development of Quant in Industry The blooming era of quantitative investment funds started from 1990s, along with the emergence of the Internet and the development of electronic trading in exchanges. Here we brieﬂy introduce the evolution of quant operating models and clas- sify them into three generations, denoted as Quant 1.0–3.0, and summarize their characteristics in Figure 7. Quant 1.0 appeared in the early age of quantitative invest- ment but it is still the most popular quant operating model in contemporary market. The features of Quant 1.0 includes: 1) Small but elite team, typically led by an experienced portfo- lio manager and composed of a few genius researchers and traders with strong mathematics, physics or computer sci- ence background; 2) applying or even inventing mathemat- ical and statistical tools to analyze ﬁnancial market analysis and discover mispriced assets for trading; 3) trading signals and trading strategies are usually simple, understandable and interpretable to reduce the risk of in-sample over-ﬁtting in modeling. This operating model has high e \u000eciency in quant trading but low robustness in management. Especially, the success of a Quant 1.0 team relies too much on particular ge- nius researchers or traders, and such a team may decline or even bankrupt rapidly with the departure of genius. In ad- dition, such a small “strategy workshop” limits the research e\u000eciency on complex investment strategies such as quanti- tative stock alpha strategy which depends on diversiﬁed ﬁ- nancial data types, extremely large data volume, and com- plex modeling techniques such as super large deep learning model. Quant 2.0 changes quant operating model from small genius’ workshop to an industrialized and standardized alpha fac- tory. In this model, hundreds or even thousands of invest- ment researchers work on the same pipeline to mine e ec- tive alpha factors [41] out of the plethora of ﬁnancial data, using standardized evaluation criteria, standardized back-test processes and standardized parameter conﬁgurations. These alpha mining researchers are rewarded by submitting quali- ﬁed alpha factors which usually have high back-test returns, high Sharpe ratio, reasonable turnover rate and low correla- tion with existing factors in the alpha database. Tradition- ally, each alpha factor is a mathematical expression char- acterizing some pattern or proﬁle of stocks, or some rela- tionship between stocks, although more and more compli- cated machine learning factors are mined as well. Typical al- pha factors include momentum factors, mean-reversion fac- tors, event-driven factors, volume-price dispersion factors, growth factors, etc. Many alpha factors submitted by alpha researchers are combined into statistical models or machine learning models by portfolio managers to ﬁnd the optimal as- set positions after appropriate risk neutralization, expecting to obtain a stable and promising excess return in the mar- ket. However, large-scale team work results in huge costs for human resources, and the situation gets more and more seri- ous with the team growing larger and larger. Speciﬁcally, we could expect the number of discovered e ective alphas fol- lows an approximately linear trend with the team size (actu- 7\n•Mathematical modeling•Interpretable robust signal•Small genius teamQuant 1.0•Intensive factor mining•Scalable modeling pipeline•Reward mechanismQuant 2.0•Deep learning blackbox•Intensivemodel tuning•Very large data volumeQuant 3.0•End-to-end automated AI•Explainable AI system•Knowledge graph reasoningQuant 4.0 Human-centric TradingSystem-centric TradingFigure 7: The development history of of quantitative investment in industry, from Quant 1.0 to Quant 4.0. ally in practice, discovering new e ective alphas is more and more di \u000ecult when the size of accumulated factors is already large), but the portfolio return grows signiﬁcantly lower than the expand of alpha volume and team size, and this results in the proﬁt margin getting smaller and smaller. This phe- nomenon is caused by a number of reasons such as the lim- itation of strategy market capacity, the growing di \u000eculty in discovering new e ective alphas, and even the limitation of human intelligence in searching all possibilities in strategy space. Quant 3.0 emerges with the rapid development of deep learn- ing techniques which have exhibited success in many domain areas such as computer vision and natural language process- ing. Di erent from Quant 2.0 which puts more research ef- forts and human labor into mining sophisticated alpha fac- tors, Quant 3.0 pays more attention to deep learning model- ing. With relatively simpler factors, deep learning still has the potential to learn a prediction model performing as well as a Quant 2.0 model, by leveraging its powerful end-to-end learning ability and its ﬂexible model ﬁtting ability. In Quant 3.0, the cost of human labor of alpha mining is at least par- tially replaced by the cost of computing power, especially for the expensive GPU servers. But generally speaking, it is a more e \u000ecient way for quant research in the long run. 1.5. Quant 4.0: Why and What 1.5.1. Limitations of Quant 3.0 Although Quant 3.0 has demonstrated its success in some strategy scenarios such as high-frequency stock and future trad- ing, it has three primary limitations. 1. Traditionally, building a “good” deep neural network is time- consuming and labor-intensive, because of the heavy work in network architecture design and model hyperparameter tuning, as well as the tedious work in model deployment and maintenance in trading ends. 2. It is a challenge to read understandable messages from a model encoded by deep learning black box, making it very unfriendly to investors and researchers who care much about the mechanism of ﬁnancial markets and expect to know the source of proﬁt and loss. 3. The good performance of deep learning relies heavily on ex- tremely large volumes of data, and thus only high-frequency trading (or at least medium cross-sectional alpha trading withlarge breadth) belongs to the strategy pool that deep learning favorites. This phenomenon prevents deep learning tech- niques from application in low-frequency investment sce- narios such as value investing, fundamental CTA and global macro. New research and new techniques are needed to address these limitations, and this leads to our proposal for Quant 4.0 in this article. Automated AI ➢AIs create AIs, models build models ➢Make AI economic, efficient & scalable Explainable AI ➢Make AI transparent in investment ➢Interpretability for return and risk Knowledge -driven AI ➢Data -driven to knowledge -driven ➢Financial knowledge logic reasoningManual Deep Learning ➢Labor intensive in hand -craft modeling ➢Heavy workloa dinmodel deploymen t Blackbox Machine Learning ➢Lack oftransparency in trading risk ➢Hard to control model over -fitting Data -driven Modeling ➢Performance depends on data volum e ➢Difficul tin low -frequency investmentLimitations of Quant 3.0 Advantage of Quant 4.0 Figure 8: The three key components of Quant 4.0. 1.5.2. What is Quant 4.0? We believe the limitations of Quant 3.0 are very likely to be solved or at least partially solved in the future with the quick development of the artiﬁcial intelligence (AI) technology fron- tier. Quant 4.0, the next-generation quant technology, is prac- ticing the philosophy of “end-to-end going all-in on AI” and “AI creates AI” by incorporating the state-of-the-art automated AI, explainable AI and knowledge-driven AI and plotting a new picture for quant industry. Automated AI aims to build end-to-end automation for quant research and trading, in order to signiﬁcantly reduce the cost of labor and time for quant research including data prepro- cessing, feature engineering, model construction and model deployment, and to dramatically improve R&D’s e \u000eciency and sustainability. In particular, we introduce state-of-the- art AutoML [42] techniques to automate every module in the whole strategy development pipeline. In this way, we pro- pose to change traditional hand-craft modeling to an auto- mated modeling workﬂow in an “algorithm produces algo- rithm, model builds model” manner, and eventually move to- wards a technical philosophy of “AI creates AI”. Besides AI 8\nautomation, another important task is to make AI more trans- parent, which is essentially important for investment risk man- agement. Explainable AI , usually abbreviated as XAI in machine learn- ing area, attempts to open the black box encapsulating deep learning models. Pure black-box modeling is unsafe for quant research because people can not calibrate the risk accurately. It is di \u000ecult to know, for example, where returns come from and whether they rely on certain market styles, and what the reason for a speciﬁc drawdown is, under black-box model- ing. More and more new techniques in the ﬁeld of XAI could be applied in quant to enhance the transparency of machine learning modeling, and thus we recommend quant researchers to pay more attention to XAI. We have to no- tice that improving model explainability has costs. Figure 9 shows an impossible trinity of versatility, accuracy and ex- plainability, and tells us that we have to sacriﬁce at least one apex in the triangle to obtain the beneﬁt from the other two. For example, physical law E=mc2establishes an explain- able and accurate relationship among energy, mass and speed of light, but this formula can be only applied in speciﬁc do- mains of physics and sacriﬁces versatility. Imagining that we provide more prior knowledge or domain experience in a model, it is equivalent to reducing the versatility to pro- tect the performance of accuracy and explainability simulta- neously. Examples of Versatile & Accurate Modeling •Deep Learning •Kernel Support Vector MachineExamples of Versatile & Explainable Modeling •Linear Model (Feature Explainability ) •Decision Tree (Rule Explainability ) Examples of Accurate & Explainable Modeling •Black -Scholes Model for Option Pricing •Physical Laws Such as 𝐸=𝑚𝑐2 •Know ledge: money oversupply → inflationVersatility Accuracy Explainability Figure 9: Impossible trinity of versatility, accuracy and explainability in mod- eling. Knowledge-driven AI di ers from the data-driven AI which heavily depends on large volumes of data samples and thus is appropriate for investment strategies with large breadth such as high-frequency trading or stock cross-sectional trad- ing. It is an important complement to data-driven AI tech- niques such as deep learning (illustrated in Figure 10 us- ing Bayes’ theorem). In this paper, we introduce knowledge graph which represents knowledge with a network structure composed of entities and relations, and stores knowledge with semantic triples. A knowledge graph of ﬁnancial behaviors and events could be analyzed and inferred for investment de- cisions using symbolic reasoning and neural reasoning tech- niques. This implies potential applications to those invest- ment scenarios with low trading frequency but intensive fun- damental information in collection and analysis, includingvalue investing and global macro investment. 𝑃ȁ𝜃𝑥∝𝑃𝜃×𝑃ȁ𝑥𝜃Posterior Distribution for Decision MakingData -driven Likelihood (e.g., Deep Learning ) Knowledge -driven Prior (e.g., Domain Knowledge Graph Reasoning) Figure 10: Complementary function of data-driven AI and knowledge-driven AI in decision making. 2. Automated AI for Quant 4.0 Automated AI for Quant 4.0 covers the automation of the full quant pipeline. In this section, we will ﬁrst give an overview of the pipeline and then introduce how to upgrade it to an auto- mated AI pipeline. 2.1. Automating Quant Research Pipeline 2.1.1. Traditional Quant Pipeline Over decades of development, quant research has formed a standard workﬂow as shown in Figure 11 (blue part). This workﬂow consists of a number of modules, including data pre- processing, factor mining, modeling, portfolio optimization, or- der execution, and risk analysis. Data preprocessing is usually the ﬁrst step in quant research. Original raw data may have many issues. Firstly, ﬁnancial data usually have missing records, more or less. For exam- ple, in technical analysis, you may not receive price data at some time points due to packet loss during communication, or you may miss the price data on some trading days because of stock suspension. Similarly, in fundamental analysis, you may miss part of ﬁnancial statement data since they are not reported on time. Although conventional statistical data im- putation methods could be used to estimate and ﬁll in miss- ing records, we must avoid using future information in the imputation process. Secondly, ﬁnancial data contain extreme values and outliers which may come from misrecording, data storage issues, data transfer issues, or extreme markets, and these outliers may lead to risky biases in investment deci- sions. Outliers could be eliminated by data winsorization methods [43] which limit extreme values in a certain per- centile range, but we have to notice that some outliers are ac- tually strong signals for quant trading rather than noise, and must di erentiate the two during data preprocessing. Thirdly, many ﬁnancial data, such as news event data, have low data coverage and irregular updating frequency. We must align these types of data with high coverage and regular frequency such as quotes data for the convenience of downstream fac- tor mining and modeling tasks. Fourthly, di erent data fea- tures have quite di erent scales in value range and thus some “large” features may dominate “small” features in modeling. Therefore, data standardization methods are used to normal- ize the range of features. We have to take care of the way to standardize the data in order to reduce information loss. 9\nRaw Data Exchange Traditional Quant Research Pipeline Investment Research ExecutionAdjustments DataPre-processing Cleaning Imputation Aggregation Standardization FactorsFactor Mining Data Analysis Factor Design Evaluation & Selection PredictionsModelling Model Construction Backtest Analysis Stress TestingPortfolio Optimization PositionsMean-Variance Optimization Risk Neuralization Turnover Control OrdersOrder Execution Real-time Risk Control Algorithmic Trading Trading AccelerationMonitor ReturnsRisk Analysis Risk Factor Exposure Return Decomposition Loss Analysis Quant Research Pipeline with Automated AI Investment Research ExecutionAdjustments Meta FactorsPre-processing Cleaning Imputation Aggregation Standardization FactorsFeature Engineering Symbolic Regression Representation Learning Hybrid Models PredictionsAutoML Architecture Search Hyperparameter Optimization Training Objective SelectionAutomated Positioning PositionsReinforcement Learning Position Optimization Risk Calculation OrdersAutomated Trading Order Execution Optimization Reinforcement Learning Trading Latency OptimizationMonitor ReturnsRisk Tracking Real-time Monitor Risk Analysis Risk-Control Intervention One-click DeploymentFigure 11: A prototypical workﬂow of quantitative investment with comparisons between the current quantitative investment system (manual, upper blue part) and AI investment engineering (automated, lower orange part). Factor mining is a task of feature engineering [44], which uses ﬁnancial and economic domain knowledge to design, search, or extract factors (features for downstream modeling) from raw data. Usually, a larger factor value indicates a more signiﬁcant trading signal. The motivation of factor mining is to ﬁnd those signals from raw data for market prediction and improve the quality of downstream modeling tasks. Tradi- tionally, ﬁnancial factors could be represented as either alge- braic formulas or rule-based expressions. Let’s take a simple stock alpha factor as an example. f actor =\u0000tscorr(rank( close );rank( volume );50) (5) where the ts corr() function computes the correlation of daily close price and volume along time using the data from the previous 50 trading days, representing how similar the trend ofclose time series and volume time series are. The rank() function maps the values in a cross-section to their orders and normalizes them to the range of [ \u00001;+1] evenly according to their descending order, in order to remove the e ect of ex- treme values. This factor prefers to select those stocks when their price and volume move in opposite directions, and the idea behind it is based on the assumption that a price trend can not sustain without the support of volume growth. Tra- ditionally, factor mining is a labor-intensive job. Most quant researchers can only discover a limited number of “good” factors in a year. Di erent ﬁnancial institutions have di erent deﬁnitions or criteria for a “good” factor, but most of them consider a few common aspects, such as return, Sharpe ra- tio, maximum drawdown, turnover rate, and correlation with other factors [41], and moreover, some institutions require the factors must be meaningful, understandable and explain- able in economics. Modeling is a task to build statistical or machine learningmodels using factors and to predict market trends, asset price movements, best trading times, or most /least valuable assets. Usually, prediction models are evaluated through back-test experiments which simulate the prediction and trading pro- cess using historical data. Choices of models must consider a number of factors, such as prediction accuracy, model ex- plainability, model robustness, and computational complex- ity, and ﬁnd the best tradeo according to the ultimate goal. In particular, we must notice that most statistical or machine learning models are not speciﬁcally developed for ﬁnancial time series, and we have to adjust the application of these models in quant modeling. Firstly, ﬁnancial time series pre- diction must avoid using future information, and thus we prefer forward-validation [45] (splitting the time series into training, validation, and test blocks over time) rather than cross-validation in model hyperparameter optimization. Sec- ondly, ﬁnancial time series are usually signiﬁcantly nonsta- tionary, far from the independent and identically distributed (i.i.d.) assumption required by many machine learning mod- els. Therefore, data transformation is needed to make the data distribution closer to i.i.d. and if possible, look more like a normal distribution. Thirdly, market style moves over time and it results in the shift of ﬁnancial time-series distri- bution. Therefore, periodic model retraining is necessary for keeping the model adapted to market style variation. Portfolio optimization aims to ﬁnd the optimal asset alloca- tion to expect high return and low risk simultaneously. While prediction models tell us what or when to buy /sell, portfo- lio optimization speciﬁes how much to buy /sell. A typi- cal portfolio optimizer attempts to solve a constrained con- vex quadratic programming problem which is extended from 10\nMarkowitz’s e \u000ecient frontier theory. max wtwT trt subject to wT t\u0006wt\u0014C1 jwt\u0000wt\u00001j\u0014C2 0\u0014wi;t\u0014C3\u00141;fori=1;2;:::; n where rt=(r1;t;r2;t;:::; rn;t)Tis the returns of nassets (e.g., stocks) at time t, and wt=(w1;t;w2;t;:::; wn;t)Tis the cor- responding position weights (percentages of capital alloca- tion). C1;C2;C3are positive constraint bounds. \u0006is the volatility matrix of the nasset returns at time t. The target function tries to maximize the portfolio return and control the upper bound of risk and turnover rate (to reduce transac- tion cost). The key in this optimization problem is how to estimate the volatility matrix \u0006whose estimation is usually unstable if historical data is not long enough, and in this case dimension reduction tricks such as regularization and factor- ization can be helpful to improve estimation robustness. Order execution is a task that buys or sells orders with opti- mal prices and minimal market impact. Usually buying (or selling) a big order at one time will push the price of the tar- get asset in a harmful direction (market impact by this big or- der), and therefore increase the trading cost. A widely used solution is order splitting, which divides a big order into a number of small orders to reduce market impact. Algorith- mic trading provides a series of mathematical tools for or- der splitting, from the simplest time-weighted average price (TWAP) and volume-weighted average price (VWAP) to the complicated reinforcement learning methods [46] in which optimal order ﬂow is modeled as a (partially observable) Markov decision process. Risk analysis is an indispensable task for quant research and quant trading. We must discover and understand every pos- sible risk exposure in order to better control unnecessary and harmful risks in quant research and trading [47]. In the mon- itor module, risks are measured in real-time and these mes- sages and analysis are sent back to help quant researcher im- prove their strategies. The most popular risk model in stock trading is the BARRA model [48] which decomposes portfo- lio volatility into the exposures of a number of predeﬁned risk factors, including style factors (size, growth, liquidity, etc.) and industry factors. However, the BARRA model could ex- plain only about 30% of total volatility, leaving the risk hid- den in the remaining 70% part still unknown. 2.1.2. Automated AI Quant Pipeline The automated pipeline of Quant 4.0 is shown in Figure 11 (orange part), where modules in the pipeline are automated by applying state-of-the-art AI technology. In the following part of this section, we will concentrate on three core modules in the automated pipeline. 1. Automated factor mining (§2.2) applies automated feature engineering techniques to search and evaluate signiﬁcant ﬁ- nancial factors generated from meta factors. We will intro-duce popular search algorithms and demonstrate how to de- sign the algorithmic workﬂow. 2. Automated modeling (§2.3) applies AutoML techniques to discover optimal deep learning models, automatically se- lecting the most appropriate models and the optimal model structures, and tuning the best hyperparameters; 3. One-click deployment (§2.4) builds an automated workﬂow to deploy trained large models on trading servers with lim- ited computing power. It executes model compression, task scheduling, and model parallelization automatically, saving a lot of labor and time for tedious “dirty” work. 2.2. Automating Factor Mining Feature engineering for quant refers to the process of ex- tracting ﬁnancial factors from original data, on which e ective pattern recognition is di \u000ecult due to their intrinsic noisiness [49, 50, 51]. Traditionally, ﬁnancial factors with signiﬁcant “alpha” are explored and developed by quant researchers manu- ally, they rely on professional domain expertise and comprehen- sive knowledge of ﬁnancial markets. Although some ﬁnancial institutions started using random search or generic program- ming algorithms, these techniques are mainly used as small- scale auxiliary tools to help improve the productivity of quant researchers. In Quant 4.0, We propose to automate the factor mining process by formulating feature engineering as a search problem and utilize corresponding algorithms to generate fac- tors with satisfactory backtest performance at scale. In partic- ular, according to their expression form, we classify factors as 1) symbolic [52] factors which are symbolic equations or sym- bolic rules, and 2) machine learning factors which are expressed by neural networks, and we will elaborate on the details in the following part of §2.2. 2.2.1. Symbolic Factors Symbolic factor mining can be regarded as a special case of symbolic regression [56]. Traditional symbolic regression algo- rithms usually generate a large number of symbolic expressions from given operands and operators and select the symbolic ex- pressions that maximize the predeﬁned objective function. Fig- ure 12 shows a framework for automated symbolic factor min- ing, which consists of four core parts: operand space, operator space, search algorithm, and evaluation criterion. Operand Space deﬁnes which meta factors could be used for factor mining. Meta factors are fundamental components for factor construction. Typical meta factors include basic price and volume information, sector categorizations, basic features extracted from limit /order books, common techni- cal indices, basic statistics from ﬁnancial analysts, impor- tant signals from ﬁnancial reports, announcements and other research reports from public companies, sentiment signals from investor emotions [57, 58], etc. Operator space deﬁnes which operators could be used in the factor mining process. For example, in cross-sectional stock selection, the operators could be classiﬁed as main opera- tors for constructing symbolic factors and post-processing 11\nPre- process Search SpaceData Operand Space Volume-price Fundamental Sentiment Grouping Operator Space MainSearch Algorithm Random Search Equation Neural Network Evaluation Criteria Alpha Factor NoiseStyle Factor Risk FactorIC stdevIC magnitudeGenerate Feedback Group group_mean (x, group), group_sum (x, group), group_rank (x, group), ……Time-series ts_corr(x, y, interval ), ts_mean(x, y, interval ), ts_rank(x, interval ), …… Cross-sectional rank(x), quantile (x, quarter), ……Element-wise add(x,y), cos(x), mul(x, y) ……Post-processing Standardization Grouping Neutralization Decay …… Quote Data Limit Order Book Financial Statement Relational Data Text Data ……Genetic Programming Equation Generation Embedding PredictionSymbolic Factor Machine Learning FactorFigure 12: An example factor mining pipeline. The search space is deﬁned by operators and meta-factors, where meta-factors are extracted from raw data in various forms. The search space is explored by search algorithms that are discrete or continuous. The evaluation module provides feedback to search algorithms based on certain criteria that serve as guidance for the next search iteration. Part of this ﬁgure is cited from [53, 54, 55]. operators for standardizing the factors for di erent trading environments. Main operators could be classiﬁed further as element-wise operators such asp() and log(), time-series op- erators such as tsrank() and tsmean () which compute the rank order and mean along each stock respectively, cross- sectional operators such as rank() and quantile () which com- pute the rank and quantile along the cross-section at a spe- ciﬁc trading time, and group operators such as group rank() which compute rank order in each group (e.g., industry or sector) respectively. Post-processing operators are used to “ﬁne-tune” the generated factors. Typical post-processing operators are standardization operators such as winsorization for outlier clipping [43] and normalization for unifying data range, neutralization operators for risk balancing, grouping operators for restricting the universe of stock selection, and decay operators for controlling turnover rate so as to reduce transaction cost. Search algorithms aim to search and ﬁnd e ective or quali- ﬁed factors as e \u000eciently as possible. A simple way to gener- ate new factors is the Monte Carlo (MC) algorithm which randomly picks the elements in the operand and operator spaces and generates a symbolic expression tree recursively. Unfortunately, the search time may grow exponentially with the length and complexity of the generated formula, and push us to consider more e \u000ecient alternatives. The ﬁrst option is Markov-chain Monte Carlo (MCMC) algorithm [59], which generates factors in sampling with importance way from a posterior distribution [60], and thus it is more e \u000ecient than MC. The second option is genetic programming [61], which is a special evolutionary algorithm for sampling and optimiz- ing tree-type data. The third option is about gradient-based methods such as neural networks, which approximate the dis-crete symbolic formulas with continuous nonlinear functions and search along the gradient direction, signiﬁcantly more e\u000ecient than random search. Evaluation criteria measure the quality of factors found by search algorithms. The performance of the generated fac- tors is evaluated using backtest experiments. Typical eval- uation criteria include information coe \u000ecient (IC), informa- tion ratio based on information coe \u000ecient (ICIR), as well as annualized return, maximum drawdown, Sharpe ratio, and turnover rate. In addition, it is very important to keep in- formation diverse among factors by ﬁltering out redundant factors which highly correlated with other factors. Due to their importance in factor mining, we introduce about two types of search algorithms in detail. Genetic programming (GP) [64] (Figure 13), an extension of genetic algorithm [65], is a metaheuristic algorithm for search- ing tree-structured symbolic factor expressions. In an algo- rithmic loop, GP starts from a number of initial factors, and it uses an evolutionary mechanism to produce the next gener- ation of factors, aiming to improve factor performance mea- sured by the ﬁtness function. There are two types of evo- lutionary mechanism in GP: mutation which replace a node randomly with another operand of an operator, and crossover where two trees swap their subtrees randomly. In each iter- ation, all factors are evaluated using IC or alternatives and only the best-performing factors are kept. This process is repeated until convergence. Neural symbolic regression utilizes gradient information to accelerate the search process. Neural networks are used to learn a continuous and nonlinear function to approximate those discrete symbolic expressions and use this function to 12\nMutationCrossoverrankrankopenvolumecorrsign mulhighlowsqrtsubrankopenrankrankopenvolumecorrneg −𝑐𝑜𝑟𝑟(𝑟𝑎𝑛𝑘𝑜𝑝𝑒𝑛,𝑟𝑎𝑛𝑘𝑣𝑜𝑙𝑢𝑚𝑒)s𝑖𝑔𝑛(𝑐𝑜𝑟𝑟𝑟𝑎𝑛𝑘𝑜𝑝𝑒𝑛,𝑟𝑎𝑛𝑘𝑣𝑜𝑙𝑢𝑚𝑒) rankrankopenvolumecorrneg −𝑐𝑜𝑟𝑟(𝑟𝑎𝑛𝑘𝑜𝑝𝑒𝑛,𝑟𝑎𝑛𝑘𝑣𝑜𝑙𝑢𝑚𝑒)mulhighlowsqrtsub 𝑠𝑞𝑟𝑡ℎ𝑖𝑔ℎ ∗𝑙𝑜𝑤−𝑠𝑞𝑟𝑡(ℎ𝑖𝑔ℎ) 𝑠𝑞𝑟𝑡ℎ𝑖𝑔ℎ ∗𝑙𝑜𝑤−𝑐𝑜𝑟𝑟(𝑟𝑎𝑛𝑘𝑜𝑝𝑒𝑛,𝑟𝑎𝑛𝑘𝑣𝑜𝑙𝑢𝑚𝑒)sqrthigh rankvolumecorrneg −𝑐𝑜𝑟𝑟(𝑠𝑞𝑟𝑡(ℎ𝑖𝑔ℎ),𝑟𝑎𝑛𝑘𝑣𝑜𝑙𝑢𝑚𝑒)sqrthighFigure 13: Illustrations of two evolutionary mechanism used in genetic pro- gramming. generate new formulas. We introduce two works about neu- ral symbolic regression. The ﬁrst paper [62] (Figure 14a) builds a transformer generative model from a number of ex- isting symbolic expressions. In the training stage, a special transformer model (called set transformer [66]) encodes the formulas in the training set into embedded vectors, and they are delivered to a transformer decoder to update symbolic expressions in an autoregressive way using beam search, and this process is repeated until convergence. The generative model is trained by minimizing the cross-entropy loss be- tween the generated expression and the original one. In the test stage, the trained model is used to generate new symbolic expressions. The second paper [63] (Figure 14b), designed a new neural network speciﬁcally for expressing symbolic for- mulas. In particular, the activation functions in this network are replaced with symbolic operators such as sin(\u0001) andp(\u0001). This special neural network has the ﬂexibility to generate al- most all formula expressions need to use in factor mining. 2.2.2. Machine Learning Factors Symbolic factors have their advantages in simplicity and understandability, and are thus widely used in practice. How- ever, their representation ability is limited by the richness of operands and operators. Machine learning factors, on the other hand, have more ﬂexibility in representation to ﬁt more com- plicated nonlinear relationships [67], and thus they have the chance to perform better in market prediction. In particular, mining machine learning factors [68, 69, 70, 71] is a process to ﬁt neural networks, where gradients provide the optimal direc- tion for fast search of solutions. As shown in Figure 15, most deep neural networks for stock prediction follows the encoder- (a) Neural symbolic regression in a sequence generation manner [62]. (b) Neural symbolic regression that directly uses the neural network as symbolic expressions [63]. Figure 14: Illustrations of neural symbolic regression algorithms. … EncoderRSI ADX MACDMARaw Data MSFT AAPL… AAPL MSFT … GOOGLStock Embeddings Decoder AAPL MSFT … GOOGL 2.52 -4.63 -1.78Prediction … … GOOGL Figure 15: Illustration of the encoder-decoder architecture used in stock predic- tion. Both embeddings and predictions can be used as factors. decoder architecture [72], where the encoder maps meta factors to a latent vector representation and the decoder transforms this embedding to some outcome such as future return [73]. In fact, not only the ﬁnal outcome, but also the embedding itself could be used as a (high-dimensional) machine learning factor [74], and further applied to various downstream tasks. Machine learning factors have some limitations as well. Firstly, they are usually hard to interpret and understand because of the black-box nature of machine learning. Secondly, gradient search used by neural networks may be stuck at some local op- tima and result in model instability problems. Finally, neural networks may su er more serious overﬁtting due to their ﬂex- ibility, and this situation gets worse in quant because data are extremely noisy. 2.3. Automated Modeling The automation of statistical machine learning such as SVM, decision tree and boosting has been extensively researched. A simple and direct automation method is the brute-force enumer- ation of all possible conﬁgurations, each including the choice of machine learning algorithms and the corresponding hyper- parameters (Figure 17). 13\nSearch Space Cell-based Hierachical Entire-structuredSearch Strategy Recursive DifferentiablePerformance Estimation StrategyEarly-stoppingLower-fidelityWeight InheritanceWeight SharingOne-shot DefineEvaluateFeedbackFigure 16: The automated modeling pipeline for architecture search. The structure of this ﬁgure is adapted from [75]. Illustrations of search spaces and search strategies are cited from [42, 76]. Figure 17: Illustration of early-stage automated machine learning based on brute-force search of algorithms and hyperparameters. Figure is cited from [77]. In this article, we focus on the state-of-the-art deep learning automation problem, which is more complex due to the end-to- end property and network architecture issue in modeling. The conﬁguration of a deep learning model consists of three parts: architecture, hyperparameters, and objectives, and they jointly determine the ﬁnal performance of the models. Traditionally, these conﬁgurations are tuned manually. In Quant 4.0, they are searched and optimized using various AutoML algorithms. A standard AutoML system needs to answer the following three questions: what to search (i.e., search space §2.3.1), how to search (i.e., search algorithm §2.3.2), and why to search (i.e., performance evaluation §2.3.3). 2.3.1. Search Space Search space is designed for the three conﬁguration settings that need to be optimized in an automatic way. Architecture conﬁgures a network structure. For example, the architecture of a multi-layer perceptron is speciﬁed by the number of hidden layers and the number of neurons at each layer. The architecture of a convolution neural net- work needs to consider more conﬁgurations such as the num- ber of convolution kernels as well as their strides and recep- tive ﬁelds. The architecture of large-scale models such as Transformer is composed of a number of predeﬁned blocks (e.g. self-attention blocks, residual blocks) linked together. As discussed above, architecture is complex and may have a hierarchical structure at di erent scales. Accordingly, thesearch space can be deﬁned at various granularities, ranging from low-level operators such as convolutions and attentions to high-level modules such as LSTM cells. Early search al- gorithms run on the ﬁnest granularity and optimize the low- level structure of the neural network [78, 79]. Such a search process is ﬂexible in network structure but ine \u000ecient in in- corporating prior knowledge and abstraction. One solution is to assume a hierarchical structure in network architecture. Speciﬁcally, at a high level, the network is designed to be a graph of cells (a.k.a. blocks /motifs [42, 80]), each of which is a subnetwork. Many cells share the same internal struc- ture at a low level in order to reduce the computational cost. Cell-based search algorithms [81, 82] need to ﬁnd both high- level structures between cells as well as low-level structures within the cells. Hyperparameter controls the overall training process. For example, learning rates determines the step size moving to- wards a minimum of a loss function. A smaller learning rate is more accurate in solution but slower in convergence. The batch size determines the number of samples involved in a batch for gradient estimation, which also has an inﬂuence on training e \u000eciency and stability. The search space for hyper- parameters is simpler than that for architecture since most hyperparameters are continuous (e.g., learning rate) or ap- proximately continuous values (e.g., batch size). Objective speciﬁes the loss functions and labels used for train- ing models. The loss function is the key component of ma- chine learning models since it provides a goal towards which a model should be trained. Besides classic loss functions such as mean square loss and cross-entropy loss, new loss functions speciﬁcally designed for quant tasks can be also selected. Labels deﬁne the “ground-truth” target the model aims to ﬁt. For example, either price raise /fall or future re- turns in di erent holding time windows can be considered in the search space. 14\n2.3.2. Search Algorithm Given the search space, we could use search algorithms to ﬁnd the best model conﬁguration. Table 2 lists various types of search algorithms and their corresponding tasks: network archi- tecture search (NAS) [75], hyperparameter optimization (HPO) [83] and training objective selection (TOS). Table 2: Search algorithms and their applicable search targets. AlgorithmTargetNAS HPO TOS References Grid/Random Search X X [84, 85] Evolutionary Algorithm X X X [86, 87, 88, 89, 90] Reinforcement Learning X X [91, 81, 82, 92] Bayesian Optimization X X [93, 94, 95, 96] Gradient-based Method X X [97, 98, 99, 100, 101] Grid search is a brute-force algorithm that searches on a grid of conﬁgurations and evaluates all of them. It is a good choice when the search space is small due to ease of imple- mentation and parallelization [84]. However, most NAS and HPO problems in deep learning have extremely large search spaces and grid search can not scale well for them. More- over, grid search is used more popular in HPO and TOS than in NAS whose search space is di \u000ecult for grid layout except enumerating all possibilities. Random search generates a number of candidate conﬁgura- tions using some stochastic sampling mechanisms, such as Monte Carlo or MCMC. It is very straightforward to im- plement and parallelize (mainly for independent sampling mechanisms such as Monte Carlo sampling or importance sampling). Random search is very ﬂexible and can be used for NAS, HPO, and TOS. Although random search is usually faster than grid search [84], it is still di \u000ecult to handle high- dimensional search space as the number of potential conﬁgu- rations grows exponentially with the number of hyperparam- eters. Evolutionary algorithm is an extension of random search. It utilizes evolution mechanisms to improve model conﬁgura- tions iteratively. It encodes the architecture of network net- works as a population and performs the evolution steps on them to improve the model iteratively. Speciﬁcally, the mod- els are ﬁrst encoded according to their underlying computa- tion graph. Then, a set of pre-deﬁned evolutionary operators are applied to the encoded models, including architectural modiﬁcations such as inserting or deleting several operations and adding skipping connections, as well as hyperparameter- related operations such as learning rate adjustment. At each iteration, the best-performing models are selected via tourna- ments and combined via mutation and crossover operations to form the next generation. Evolutionary algorithms inher- ently support weight inheriting among generations, which helps accelerate the convergence in training and increase the searching e \u000eciency. Reinforcement learning models the architecture search prob- lem as a Markov decision process. In each step, an RNN con-troller chooses an action to sample a new architecture and the corresponding deep neural network model is trained. Then the performance of the model evaluated on the validation set is used as a reward which is forwarded to compute the policy gradient and update the RNN controller. This loop is iterated until convergence. The reinforcement learning framework is very universal for most optimization problems and it could be used for NAS, HPO, and TOS. Bayesian optimization explores the search space more e \u000e- ciently by leveraging surrogate models to approximate the objective function that couldn’t be expressed explicitly. Specif- ically, for a black-box objective function for HPO, Bayesian optimization initializes a prior distribution using a surrogate function such as Gaussian process or tree-structured Parzen estimator. Then it samples new data points from the prior dis- tribution (with importance) and calculates their values using the underlying objective function. Given these new samples and prior, the posterior function can be calculated and is used as an updated surrogate function to replace the original prior function. This process is repeated until the optimal solution is found. Traditionally, Bayesian optimization is used for continuous search tasks such as HPO, but recent works have extended it to NAS tasks as well. Gradient-based methods is very e \u000ecient when the gradient of the objective function exists. However, for NAS, the search space is discrete and couldn’t deﬁne a gradient directly. One solution is to “soften” the architecture and deﬁne an over- parameterized “super-architecture” which covers all possi- ble candidates and is di erentiable. A typical gradient-based NAS method is DARTS [97] which constructs an over-parameterized network where all types of candidate operations are present on the computation graph. The resulting value is the weighted sum of the results of all the operations, where the weights are the softmax values of a parameterized probability vec- tor. Both model parameters and architectural parameters are trained via a bi-level optimization problem. In the inference process, the architecture and hyperparameters with the high- est probability are selected. DARTS is substantially faster than random research and reinforcement learning in NAS and HPO tasks. 2.3.3. Accelerating Evaluation The computational cost of automated model search comes from two parts: search algorithm and model evaluation, and the latter is usually the bottleneck of the computation because it is very time-consuming to train a deep neural network until convergence under a given conﬁguration. Several methods are introduced in previous research to address this issue. Firstly, the training process of neural networks can be early-stopped before convergence to reduce the computational time for eval- uation [81]. Secondly, the model can select fewer samples to accelerate the training process [94]. Thirdly, warm-start model training can be used to leverage the information from existing selected models [102] or inherit the information from an over- parameterized “parent” model [103, 97, 98] to accelerate the search loop. 15\n2.4. Automated One-click Deployment Model deployment is the task of transferring the developed model from o \u000fine research to online trading. It is not only simply transferring code and data, but also synchronizing data and factor dependency, adapting trading server and system, de- bugging model inference, testing computing latency, etc. In the following part, we focus on one important problem in model de- ployment: how to accelerate deep learning inference for high- frequency trading and algorithmic trading scenarios. We pro- pose an automated one-click deployment solution utilizing tech- niques such as model compilation [104] and model compres- sion [105, 106] to realize inference acceleration [104, 105, 106]. The former makes the inference faster without changing the model itself, and the latter seeks smaller and lighter alternative models to save inference time. 2.4.1. Acceleration by Model Compilation At the development stage, deep learning models’ function- ality is the top priority for the underlying framework which im- plements the computations. Hence, at this stage, the framework strictly maps all the operations to the computation graph. How- ever, such direct mapping introduces large room for optimiza- tion at the deployment stage where the computations are ﬁxed. Therefore, the model’s computation can be greatly simpliﬁed and adapted to hardware features without hurting its original semantics. Such optimization is one of the major topics in deep learning compilers [107, 108, 109], which can be categorized asfront-end optimizations andback-end optimizations , which work on high-level and low-level intermediate representations (IRs) for deep learning models respectively. Following the sum- mary in [104], we will brieﬂy introduce relevant optimization techniques. Front-end optimization, as illustrated in Figure 18a, focuses on simplifying the structure of the computation graphs. For example, algebraic simpliﬁcation techniques such as constant folding [110] and strength reduction [111] convert expensive operations into cheaper ones via transformation or merging. Common subexpression extraction (CSE) [112] techniques iden- tify repeated nodes in the computation graph and merge them into one node to avoid duplicate computations. Back-end optimization, as illustrated in Figure 18b, is per- formed with an emphasis on the features of hardware archi- tectures, such as locality and memory latency. For example, pre-fetching techniques [113] load data from main memory to GPU before they are needed, and speed up fetch operations. Loop-based optimizations [114] reorder, fuse and unroll oper- ations inside loops to enhance locality among neighboring in- structions. Memory latency hiding [115] techniques aim to in- crease instruction throughput so as to mitigate the problem of high latency in accessing memory. Parallelization techniques such as loop splitting [116], automatic vectorization [117] and loop skewing [118, 119], can also be applied to maximize the parallelism provided by modern processors. 2.4.2. Acceleration by Model Compression Model compression aims to reduce model size for inference acceleration while minimizing drops in performance. In thisway, the compressed model can be regarded as an approxima- tion of the original model. Generally speaking, model compres- sion can be performed at both micro- and macro-level, where the former focuses on the precision of individual model param- eters and the latter focuses on simplifying the overall model structure. At the micro-level, pruning and quantization techniques can be applied to reduce both the number of parameters and the bit size of the individual parameters. Model pruning [120], as shown in Figure 19a, removes unimportant connections and neurons in neural networks that have little inﬂuence on the ac- tivation of the neural. The identiﬁcation of candidate parame- ters is usually based on their weights, where those with smaller weights are considered for pruning. Model quantization [121], as shown in Figure 19b, converts the parameters from ﬂoating- point numbers to low-bit representations. Speciﬁcally, a code- book is constructed to store the approximated values of the orig- inal parameters according to the distribution of all parameter values. The parameters are then quantized according to the codebook and thus the bit size for the parameters is reduced. Due to the inevitable performance drop induced by precision re- duction, the compressed model is usually re-trained to approach its original performance. At the macro-level, the model can be signiﬁcantly com- pressed into a smaller model with simpler architectures via knowl- edge distillation [122] and low-rank factorization [123, 124]. Knowledge distillation (Figure 19c) compresses a model by transferring useful knowledge from the original large model (called the teacher model) to a small and simple model (called the student model) with minimal knowledge loss. Low-rank factorization techniques (Figure 19d) assume the sparsity of model parameters and then split the parameter matrix of the original neural networks into products of low-rank matrices [123], and thus reduce the model complexity. 3. Explainable AI for Quant 4.0 XAI [126, 127], as an attractive research direction for decades, is critical to the trustworthiness and robustness of AI models. In the case of quant, improvement in the explainability of AI can make the decision process more transparent and easy to ana- lyze, providing useful insights to researchers and investors and discovering potential risk exposures. In this section, we will discuss how to leverage XAI in Quant 4.0. §3.1 introduces common XAI techniques and §3.2 connects these techniques to real quant scenarios. 3.1. Overview of Explainable AI XAI is an emerging interdisciplinary research area cover- ing machine /deep/reinforcement learning, statistics, game the- ory and visualization. Here we focus on two types of XAI: model-intrinsic explanation [128] and model-agnostic explana- tion [129]. 16\nDGG\u0011\u0014\u0015\u0013\u0013 \u0014 WUDQVSRVH\u0011\u0014\u0015\u0014\u0003 UHVKDSH\u0011\u0014\u0015\u0015 WXSOH\u0011\u0014\u0015\u0017 JHW\u0010WXSOH\u0010 HOHPHQW\u0011\u0014\u0015\u0018\u0003 5227DGG\u0011\u0014\u0015\u0013\u0013 UHVKDSH\u0011\u0019\u0014 5227 FRQYROXWLRQ\u0011\u0014\u0014\u0014\u0003\u00143DUDPHWHU\u0003\u0014\u0019 UHVKDSH\u0011\u0016UHVKDSH\u0011\u0014PD[LPXP\u0011\u0014\u0013 I\u0016\u0015>\u0014\u000f\u0017\u0013 \u0019\u000f\u0014\u000f\u0014@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013`\u0014\u0013 PXOWLSO\\\u0011\u0014\u0014\u0013\u0014 \u0013 DGG\u0011\u0014\u0014\u0016\u0013 PD[LPXP\u0011\u0014\u0014\u0019 I\u0016\u0015>\u0014\u000f\u0017\u0013 \u0019\u000f\u0014\u000f\u0014@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013`\u0014EURDGFDVW\u0011\u0014\u0014\u0018 I\u0016\u0015>\u0014\u000f\u0017\u0013 \u0019\u000f\u0014\u000f\u0014@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013` \u0013\u0013 \u00143DUDPHWHU\u0003\u0014\u0018 FRQYROXWLRQ\u0011\u0014\u0014\u0014\u0003\u00143DUDPHWHU\u0003\u0014\u0019 UHVKDSH\u0011\u0016UHVKDSH\u0011\u0014\u0014 PXOWLSO\\\u0011\u0014\u0014\u0013\u0014 \u0013 DGG\u0011\u0014\u0014\u0016\u0013 \u0014\u0013 \u0013\u0013 \u00143DUDPHWHU\u0003\u0014\u0018 FXVWRP\u0010FDOO\u0011\u0019 BBFXGQQ\u0007FRQY)RUZDUG\u0014 3DUDPHWHU\u0003\u0014\u0019 UHVKDSH\u0011\u0016PXOWLSO\\\u0011\u0014\u0014\u0013\u0014 \u0013 DGG\u0011\u0014\u0014\u0016 PD[LPXP\u0011\u0014\u0014\u0019\u0014\u0013 \u0014JHW\u0010WXSOH\u0010 HOHPHQW\u0011\u0019 \u0013 3DUDPHWHU\u0003\u0014\u0018 FRS\\\u0011\u0014\u00193DUDPHWHU\u0003\u0014\u0019 UHVKDSH\u0011\u0016 PXOWLSO\\\u0011\u0014\u0014\u0013\u0003 FXVWRP\u0010FDOO\u0011\u0014\u0014 BFXGQQ \u0007FRQY%LDV$FWLYDWLRQ)RUZDU\u0013\u0013 \u0016EURDGFDVW\u0011\u0014 \u0015 JHW\u0010WXSOH\u0010 HOHPHQW\u0011\u0014\u0014\u0003\u0014 \u0014)XVHG\u0003H[SUHVVLRQ \u0003ORRS\u0003IXVLRQ3DUDPHWHU\u0003\u0014\u001b 3DUDPHWHU\u0003\u0014FXVWRP\u0010FDOO\u0011\u0014\u0018 BFXGQQ\u0007FRQY)RUZDUG\u0014 JHW\u0010WXSOH\u0010 HOHPHQW\u0011\u0014\u00183DUDPHWHU\u0003\u0013 ELWFDVW\u0011\u001aDGG\u0011\u0013\u0013ELWFDVW\u0011\u001b \u0014\u0013 DOJHEUDLF\u0003VLPSOLILFDWLRQFX'11\u0003 WUDQVIRUPDWLRQ FRQVWDQW\u0003IROGLQJ RSHUDWRU\u0003IXVLRQUHVKDSH\u0011\u0014\u0015\u0016&6(EURDGFDVW\u0011\u0014\u0014\u0018 I\u0016\u0015>\u0014\u000f\u0017\u0013 \u0019\u000f\u0014\u000f\u0014@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013` PD[LPXP\u0011\u0014\u0014\u0019 I\u0016\u0015>\u0014\u000f\u0017\u0013 \u0019\u000f\u0014\u000f\u0014@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013`PD[LPXP\u0011\u0014\u0013 I\u0016\u0015>\u0014\u000f\u0017\u0013 \u0019\u000f\u0014\u000f\u0014@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013` PXOWLSO\\\u0011\u0017\u0018\u0003 I\u0016\u0015>\u0014\u000f\u0014\u000f\u0014\u000f\u0017\u0013 \u0019@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013` PXOWLSO\\\u0011\u0014\u0014\u0013\u0003 I\u0016\u0015>\u0014\u000f\u0017\u0013 \u0019\u000f\u0014\u000f\u0014@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013`ELWFDVW\u0011\u0014 I\u0016\u0015>\u0014\u000f\u0017\u0013 \u0019\u000f\u0014\u000f\u0014@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013`UHVKDSH\u0011\u0014\u0003 I\u0016\u0015>\u0014\u000f\u0017\u0013 \u0019\u000f\u0014\u000f\u0014@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013`PXOWLSO\\ \u0011\u0017\u0018\u0003 I\u0016\u0015>\u0014\u000f\u0014\u000f \u0014\u000f\u0017\u0013 \u0019@ ^\u0016\u000f\u0015\u000f\u0014\u000f\u0013` PXOWLSO\\ \u0011\u0014\u0014\u0013\u0003 I\u0016\u0015>\u0014\u000f\u0017\u0013 \u0019\u000f\u0014\u000f\u0014@ ^\u0016\u000f\u0015\u000f\u0014\u000f\u0013`PXOWLSO\\\u0011\u0017\u0018\u0003 I\u0016\u0015>\u0014\u000f\u0014\u000f\u0014\u000f\u0017\u0013 \u0019@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013` PXOWLSO\\\u0011\u0014\u0014\u0013\u0003 I\u0016\u0015>\u0014\u000f\u0017\u0013 \u0019\u000f\u0014\u000f\u0014@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013`PXOWLSO\\\u0011\u0017\u0018\u0003 I\u0016\u0015>\u0014\u000f\u0014\u000f\u0014\u000f\u0017\u0013 \u0019@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013` PXOWLSO\\\u0011\u0014\u0014\u0013\u0003 I\u0016\u0015>\u0014\u000f\u0017\u0013 \u0019\u000f\u0014\u000f\u0014@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013`ELWFDVW\u0011\u0014 I\u0016\u0015>\u0014\u000f\u0017\u0013 \u0019\u000f\u0014\u000f\u0014@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013`UHVKDSH\u0011\u0014\u0003 I\u0016\u0015>\u0014\u000f\u0017\u0013 \u0019\u000f\u0014\u000f\u0014@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013`PXOWLSO\\\u0011\u0017\u0018\u0003 I\u0016\u0015>\u0014\u000f\u0014\u000f\u0014\u000f\u0017\u0013 \u0019@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013` PXOWLSO\\\u0011\u0014\u0014\u0013\u0003 I\u0016\u0015>\u0014\u000f\u0017\u0013 \u0019\u000f\u0014\u000f\u0014@^\u0016\u000f\u0015\u000f\u0014\u000f\u0013`(a) Front-end optimization Hardware Intrinsic Mapping Memory Allocation & Fetching Memory Latency HidingLoop Oriented Optimization Techniques ParallelizationLoop fusion Slide windows TilingReordering Unrolling Halide Polyhedralfori=1,n Stmt1( i) fori=1,n Stmt2( i) fori=1,n Stmt1( i) Stmt2( i)fori=1,4 Stmt1( i) Stmt1(1) Stmt1(2) Stmt1(3) Stmt1(4)fori=1,n forj=1,m Stmt(i,j) fori=1,m forj=1,n Stmt(i,j) CacheFit inoperatorHardware intrinsic gemm8x8(x, y, z)fori=1,8 forj=1,8 fork=1, 8 z(i,j)=x(k,i)*y(k,j) DRAM GPU Shared Memory GPU Memory ………… …… ……Transfer FetchStore UseWarpDataData Data ldex ldex ld… ldex ldex ld…0 10ld ex ld ex 0 1 1fori=1,n forj=1,n Stmt(i,j)fori=1,n forj=1,n/4 for k=1,4 Stmt(i,j*4+k) fori=1,n forj=1,n/4 vec(Stmt(i,j))fori=min,ma x forj=1,n/4 vec(Stmt(i,j))split vectorize parallelizeAutotuner schedules Nested PolyhedralParallelization Vectorization (b) Back-end optimization Figure 18: Deep learning compiler optimization techniques. Figure from [104] (a) Model pruning. Figure from [120] (b) Model quantization. Figure from [121]. Data Teacher Model Student Model K n o w l e d g eKnowledge Distill Transfer Knowledge Transfer(c) Knowledge distillation. Figure from [125]. 𝑐channelsW′ P 𝑑′channels 𝑑channelsW (a) (b) (d) Low-rank approximation of CNN models. Figure from [123]. Figure 19: Model compression techniques 3.1.1. Model-intrinsic Explanation in XAI Risk control and management is the top priority of ﬁnancial industry. When AI models are deployed in real-world applica- tions, their decision process is usually required to be transparent by regulatory authorities for the safety of transactions. More- over, model-intrinsic explainability is the requirement of many large ﬁnancial institutions such as banks and insurance compa- nies. A machine learning model is intrinsically explainable if its internal structure or mechanisms can be easily explained. Some machine learning algorithms such as linear models and decision trees are inherently explainable, as many other algorithms such as deep neural networks and kernel learning methods (SVM, Gaussian process, etc.) are black boxes with poor explainabili- ties. Figure 20 illustrates many popular machine learning meth- ods arranging along their general performance and explainabil- ity. We can see The increase in model-intrinsic explainability usually leads to a decrease in the model’s prediction perfor- mance, and therefore the selection of machine learning algo- rithms is essentially a trade-o between explainability and per- formance. We brieﬂy introduce a few typical machine learning methods in terms of explainability and predictive performances and discuss their applicable scenarios. Linear Models , such as linear regression, logistic regression, linear discriminant analysis, linear SVM and addition model, is a family of methods where features or transformation of a group of features are in an additive form and thus the perfor- mance of ﬁnal prediction can be easily deposed to the e ects from individual features or feature groups. Therefore, linear models are intrinsically understandable and explainable. Forexample, linear regression explicitly encodes the importance of each feature in their corresponding regression coe \u000ecients (assuming every feature is normalized to eliminate the e ect from scales and units). Although linear models are easy to explain, they are su ering the poor prediction performance since they couldn’t encode complicated nonlinear relation- ships between prediction outputs and features. Rule-based Learning is another type of easy-to-explain meth- ods. Di erent from a linear model which ﬁts a linear deci- sion boundary, a rule-based learning method ﬁts a stepwise decision boundary characterized by decision rules combin- ing a number of logical expressions. Examples of rule-based learning include decision tree [133] and symbolic regression, as well as ensemble models such as random forest [134] and boosting [135, 136, 137]. Rule-based learning models are intrinsically explainable decision rules that are close to the logical thinking process of human beings. However, to bet- ter ﬁt the training data and improve prediction performance, the decision rule is usually complicated, and it reduces the explainability and increases the risk of overﬁtting. Ensemble Learning combines multiple machine learning al- gorithms to achieve better decision performance than single models. Typical examples of ensemble learning include ran- dom forest and boosted trees that combine multiple tree mod- els and make predictions based on the aggregation of individ- ual decisions. Although there is controversy, in this article, we classify mixtures of experts (MoE) [138] as an ensemble method as well. MoE combines multiple expert networks in parallel in a layer and decides which expert (or experts) par- 17\nGaussian Process Decision Tree Residual Networ k Transformer HMM Boosting Random Forest Mixture -of-Expert LSTM Kernel k -NN Generalized Linear ModelKernel SVM Deep Learning Symbolic Regression Bayesian Network CRF Vanilla RNN Model ExplainabilityPrediction Performance Linear SVM Kernel Learning Sequence Learning Generalized Additive Model Linear Model HighHigh LowFigure 20: Comparison of popular machine learning algorithms according to prediction performance and model explainability. Part of this ﬁgure is cited from [130, 131, 132]. ticipates in the decision of a speciﬁc data point via a gating mechanism. Compared with other machine learning meth- ods, ensemble learning provides high-level explainability by demonstrating the relative importance of single models or ex- perts. Kernel Learning , also known as kernel method or kernel ma- chine, is a family of nonparametric learning methods which make predictions by computing the similarity between sam- ples. The similarity is characterized by a kernel function, which is a special inner product deﬁned in a high-dimensional Hilbert space where original data samples are mapped [139]. For example, kernel SVM [140] transforms original posi- tive¬ative samples into another space where they can be easily separated using a linear decision boundary. In prin- ciple, kernel functions can be of arbitrary forms that satisfy the Mercer’s condition [141], and they determine the non- linear relationships between inputs and outputs. Moreover, the idea of kernel functions is extended to the self-attention mechanism [142] used in neural networks such as Trans- former [143]. Traditionally, we think the kernel trick im- proves the performance of models but weakens their explain- ability. From another point of view, however, the deﬁnition of kernel itself encodes the prior insight of users and could help understand the model. Sequence Learning refers to a family of machine learning methods that work with sequential data such as time series or sentences. They are widely used in speech recognition, language understanding, DNA sequence analysis, and stock price prediction. Sequence learning methods characterize the underlying structure hidden in sequential data and dis- cover implicit patterns. For example, hidden Markov model(HMM) [144] assumes that the underlying structure is a ho- mogeneous Markov chain determined by a transition matrix (or transition kernel for continuous state space) and assumes the observed sequence is randomly generated from this chain through emission probabilities. The transition probabilities and emission probabilities are estimated during model train- ing. Although HMM is generally a black-box model, its tran- sition probability matrix provides some insight into the auto- regressive structure in prediction. Conditional random ﬁeld (CRF) [145] extends the ﬁrst-order Markov assumption of HMM and characterizes longer-range time dependency us- ing graphical model for probability modeling, and this extra ﬂexibility usually brings better prediction performance for CRF. Recurrent neural networks (RNN) such as LSTM [146] and GRU [147] exhibit better performance in sequence pre- diction, but it is harder to explain their internal mechanism. Deep Learning usually has superior prediction performance [148] but its shortage in explainability is clear. Some special operators in deep neural networks such as convolution and attention provide partial and local explanations about their mechanisms. For example, the self-attention layer in a Trans- former [143] characterizes the relative importance of each position in a sequence with respect to other positions. The model-intrinsic explainability for machine learning is al- ways a contradiction with its prediction capability. However, before the appearance of a brand-new machine learning model satisfying both high prediction accuracy and high explainabil- ity, we could rebuild and improve current machine learning methods. We could either start from an explainable model such as a linear or rule-based model and improve its prediction per- formance by incorporating more local nonlinear structures with 18\nbetter predictive power. For example, starting from a decision tree model, we could replace the decision rule in each leaf node with a neural network [149], thus improving the model’s ﬂexi- bility. As another example, starting from a deep neural network, we could also improve its partial explainability by incorporat- ing a special layer (e.g. self-attention layer [150]) identifying which important feature interacts more frequently. 3.1.2. Model-agnostic Explanation in XAI To address the contradiction between performance and ex- plainability, a suboptimal solution is to weaken the requirement and shift from model-intrinsic explanation to model-agnostic explanation. Based on the scope of explanations, model-agnostic XAI can be categorized as global methods (explanation applied to all samples) [151] and local methods (explanation applied to part of samples) [154]. Global methods explain the characteristics of features with respect to all samples in the dataset. These characteristics in- clude the importance of features, the importance of feature sets, the interactive e ect of features, and other high-order e ects of features. There are various types of methods for estimating global feature importance. Feature marginalization methods estimate the importance of a speciﬁc feature or a speciﬁc feature set by marginalizing all other features in the model. Speciﬁcally, the importance of the ﬁrst feature is calculated by integrating out all other fea- tures, and similarly, we calculate the second feature, the third feature, etc. For example, partial dependence plot (PDP) [135] computes the marginalized model function w.r.t. the features of interest, and it visualizes the corresponding fea- ture importance. Accumulated local e ect (ALE) plot [155] provides an unbiased estimate of marginal e ects using con- ditional distributions that consider the correlation between features. Feature leave-one-out methods evaluate the importance of con- cerned features by comparing the di erence in the model per- formance before and after leaving these features out of data. Speciﬁcally, the feature of a model can be left out by shuf- ﬂing its values in samples [134, 156]. Leave-one-out meth- ods can be extended to evaluate the interactive e ects of fea- tures as well. For example, based on the partial dependence function, H-statistic [157] is proposed to test for the interac- tion between features. Other alternative techniques for evalu- ating and visualizing feature interactions have also been pro- posed in [158, 159]. Besides, we can also perform functional decomposition [158] on the original model to explore inter- actions between all possible sets of features. Feature surrogate methods interpret the model by learning a globally explainable surrogate model [160] that approximates the original model. The surrogate model is trained under the supervision of the original model using a dataset where the inputs remain the same while the outputs are produced by the original model. Figure 21 summarizes popular global explanation methods, some of which have been introduced above. Moreover, these methods can be categorized as data-driven and model-driven,where the former methods treat the models as black boxes and query the model for explanations, and the latter methods treat models as white boxes and provide explanations using internal information such as gradients. Local methods explain the feature importance at the sam- ple level, i.e., how important a feature is for speciﬁc samples. Similar to the partial dependence plot, we can draw the indi- vidual conditional expectation (ICE) plot [161] for each sam- ple that illustrates the e ect of the concerned features when the values of other features are ﬁxed at speciﬁc values. To inter- pret a black-box machine learning model at a speciﬁc sample, we can learn a surrogate model in the vicinity of a data sample to explain the original model locally. Typical examples include LIME [152] and Anchors [153]. In LIME (Figure 22), LASSO regression [162] is used as the surrogate model to ﬁt the samples randomly generated by perturbing the original samples around the speciﬁed one. In this way, the selected samples by this lo- cal LASSO model contribute most to the ﬁtness of the speciﬁed sample. Analogous to LIME, anchors are explanations on indi- vidual samples in the form of IF-THEN rules [153] that involve features. These anchors are generated by adding features to the rules iteratively using beam search. At each iteration, the can- didate with the highest estimated precision is kept and used as the seed for the next iteration. Furthermore, we can also use feature importance to provide local explanations. For exam- ple, SHAP [163] proposes a uniﬁed framework for computing the importance of each feature in a sample using Shapley values [164]. Since the exact computation of SHAP values is computa- tionally expensive, SHAP also proposes several approximation methods to accelerate the estimation. Gradient information can also be applied [165] in di erentiable models such as deep neu- ral networks to illustrate the importance of input features to the model’s prediction. As shown in Figure 23, local explanations such as LIME and SHAP can also serve as global explanations using aggregations. Such global explanations can be attained by aggregating explanations for all samples to form explana- tions at the dataset level. For example, by computing the aver- age importance of each feature across the whole dataset, we can identify the important features that make great contributions to model prediction for most samples in the dataset. 3.2. Explainable AI for Quant In this part, we take stock alpha investment as an example. The input of deep learning models has three directions: stock, time, and factors (inner circle in Figure 24). Based on these directions, various tasks of interest can be formed to provide practical insights (outer circle in Figure 24). These tasks can be further instantiated as speciﬁc examples to provide explanations for realistic problems in investment. 3.2.1. Explanation on Stock Explanations can be provided for individual stocks to illus- trate their sensitivity to di erent factors at di erent times and their relationships with each other. Some tasks for explanations on stock are provided as follows: 19\nFigure 21: Taxonomy of global explanation methods. Figure cited from [151]. Figure 22: Illustration of LIME [152] and Anchors [153]. Figure cited from [153]. Stock similarity. Stocks are ubiquitously correlated with each other from many aspects (as shown in Figure 25a), and corre- lated stocks are expected to share common properties. In this sense, utilizing the relationships between ﬁnancial instruments can bring advantages to our analysis and prediction over tradi- tional methods that treat stocks individually. We can also better understand what the model has learned by analyzing the simi- larities between stock embeddings. However, the challenge lies in determining an appropriate similarity metric, which is ex- pected to have enough ﬂexibility and e ectiveness. This prob- lem is related to metric learning [168, 169] and graph structure learning [170]. A good similarity metric between stock embed- dings is needed. Based on this metric, a graph structure can be constructed by computing an adjusted adjacency matrix based on pairwise similarity.Lead-lag e ect. In a lead-lag e ect [171], the trend of a stock is followed by some other stocks with a lag in time. Follow- ing lead-lag e ects, investors can observe the trends (i.e. price going up /down) of the leading stocks and take corresponding positions on the lagging stocks before the same trends dupli- cate. In this way, investors can proﬁt by precisely identifying lead-lag e ects on the market [172, 173]. However, it is not a trivial job to identify lead-lag e ects, since duplicated trends appear frequently in ﬁnancial markets but only few of them are actually caused by lead-lag e ects. Strict identiﬁcation of lead- lag e ects needs to be conducted via causal inference that re- quires counterfactual explanations [174]: What will the trends of the lagging stocks be if the lead stock didn’t go in this way? Nevertheless, counterfactual reasoning is usually infeasible in real-world ﬁnancial markets. Sector trends. Sectors are stock categorizations deﬁned accord- ing to certain criteria such as industry and market capitalization. Stocks in the same sector share certain common properties, and the trend of individual stocks can be inﬂuenced by their sec- tors. Therefore, it is important to identify sectors’ contributions to individual stocks. To do this, we can treat stocks’ member- ship to di erent sectors as categorical features and compute the importance of these factors via feature importance algorithms. Besides, investors can also gain insight into the sensitivity of sectors to di erent types of features by evaluating feature in- teractions between sector memberships and other ordinary fea- tures. 3.2.2. Explanation on Time Explanation can be computed on individual time points to illustrate the situation of stocks and factors at that cross-section, 20\nFigure 23: Illustration of how local explanations can be aggregated to form global explanation. Figure cited from [151]. Factor TimeStock ModelSectorSimilarity Lead-lag Concept Calendar EffectEventExtreme Market StyleImportanceEvolution… …… Interaction Figure 24: XAI from data dimensions. and explanations across multiple cross-sections can be further combined to provide insights for market features in a time in- terval. Extreme market. In stock markets, there are extreme conditions where nearly all stocks on the market experience severe price drops (Figure 26a). In extreme markets, it is hard for quant strategies to obtain excess return since the prices of all stocks drop together, and there is little room for arbitrage. There- fore, in extreme markets, it is important to identify the less af- fected stocks and trade them to earn excess returns. To achieve this, we can decompose stock returns from two aspects: those contributed by market trends and those contributed by stock- speciﬁc features The decomposition can be computed by cate- gorizing factors into market factors and stock-speciﬁc factors. Then, the importance of these two types of factors can be com- puted via feature importance algorithms. And we need to select the stocks where the importance of stock-speciﬁc factors out- weighs that of market factors. Calendar e ect. Calendar e ects [177] refer to market anoma- lies that are related to calendars, such as days in a week, months in a year, and event-related periods such as the U.S. presiden- tial cycle. Calendar e ects are caused by market participants’ anticipations toward future trends and have a great inﬂuence onmarket trends. Thus, it is important in quant to identify calen- dar e ects and make use of them to adjust investment strategies. Such identiﬁcation can also be achieved via feature importance algorithms. By computing the importance of calendar factors, such as categorical features representing weekdays and days in the month, we can see whether model predictions heavily rely on these features. Stronger importance on calendar factors usu- ally indicates potential calendar e ects. Style transition. Style factors are used in multiple factor mod- els (as introduced in §1.4) such as BARRA [48] to describe the intrinsic features of stocks such as size, volatility, growth, etc. In such models, the returns of stocks are contributed by their exposures to these style factors, and the return contribution per unit exposure, also called factor return, di ers across style fac- tors. Moreover, the return of each factor also changes over time (Figure 26b) because of the transitions in the market’s prefer- ence for di erent styles. If such transitions can be accurately recognized, investors can adjust their strategies accordingly to focus on stocks with large exposures to the dominating style factors. To detect style transitions, we can regard style expo- sures as factors and compute their contribution to stock returns using feature importance algorithms. We can then observe the distribution of factor contributions across time and detect shifts in this distribution as signals for style transitions. Event inﬂuence. Breaking events (Figure 26c) usually have a great inﬂuence on stock markets. Investors need to have a good understanding of the inﬂuences of breaking events to reduce the negative impacts or proﬁt from the events. Usually, an event is associated with two pieces of information: the timestamp of its occurrence and the speciﬁc contents, which are usually repre- sented in natural languages. Event contents can be encoded as speciﬁc factors using NLP techniques [176], and the e ect of an event can be computed as the importance of the content factors concerning the market trends after its timestamp. Besides, we can also compute causal explanations to show the causal e ect of the event. 3.2.3. Explanation on Factors Explanations can be computed on each factor to illustrate the sensitivity of di erent stocks to it at di erent times. The explanations can be combined to show the interactive e ects among factors for speciﬁc stocks. 21\n(a) Stock similarity graph computed via return correlation. (b) Lead-lag e ect in Chinese stock market. Figure from [166]. (c) Stock correlation matrix. Potential clustering can be visualized from this matrix. Figure from [167]. Figure 25: XAI in stock. (a) Extreme market condition in 2020 of Dow Jones In- dustrial Average. Figure from [175]. (b) Return curves of BARRA risk factors. (c) Inﬂuence of breaking news across time. Figure from [176]. Figure 26: XAI in time. (a) Feature importance for basic volume- price factors (b) Interaction between size and momentum factors. Figure is cited from [178]. (c) Hierarchical clustering. Figure is cited from [179]. Figure 27: XAI in factors. Factor type. Factors can be categorized from various aspects. For example, in terms of data sources, factors can be catego- rized as volume-price factors, sentiment factors, fundamental factors, etc. In terms of ﬁnancial features, factors can be cate- gorized as momentum factors, mean-reversion factors, lead-lag factors, etc. In terms of the time scales, factors can be cat- egorized as tick-level factors, minute-level factors, day-level factors, etc. The contribution of di erent types of factors to portfolio returns can be computed by feature importance algo- rithms, and it provides investors with a better understanding of the investment strategy generated by AI, For example, in Figure 27a, the contribution to the model prediction of each factor at di erent positions in a time window is illustrated as a heatmap. Factor interaction. Deep learning model is good at capturing the complicated associations between factors, and some weak factors can be combined to form strong factors. Such interac-tions reﬂect intriguing patterns among factors and provide new insights into ﬁnding new factors. Factor interactions can be re- vealed using feature crossing techniques. For example, in Fig- ure 27b, the landscape of model prediction with respect to the value of two factors (exposure to size and momentum style fac- tors) is illustrated in a contour map. And it can be seen from the map that model prediction decreases as both factor values drop. Factor hierarchy. We can depict the semantic similarity among factors in a hierarchical way. Leveraging relevant techniques such as hierarchical clustering [180], a factor evolution graph demonstrates factor relations by arranging factors with higher similarities in lower-order neighborhoods (lower-level subtrees in the example of Figure 27c). 22\nBiology A Biomaterial A Biological Product VaccineAnonymous BAnonymous ASuspensionUnder Investigation Price Change MedicinePharmaceutical BCorporation A Litigation Exchange XTech ASecurities A BiomaterialsMedicine C Hospital ANegative Reporting Biotech Inc. A Medical ServicePharmaceutical A Supplies TradedSubsidiary HoldsCausation Related toBelongs to Sub-industryIndustry chain Capital chain Behaviors Supply chain Industry CompanyIndividual ExchangeStatus Misc entity EventSecurityEntities NewsInforma�on Extrac�on Financial StatementsParsing Parsing Sector Catgorization Litigation Documents NLU Potential RiskPotential Risk Potential RiskFigure 28: An example of ﬁnancial knowledge graph that contains behavioral information. All the ﬁnancial entities and events are ﬁctitious and only for illustration purposes. 4. Knowledge-driven AI for Quant 4.0 As discussed in §1.5, knowledge-driven AI is an impor- tant complementary technology to data-driven AI, especially in low-frequency investment scenarios such as value investing and global macro investment. In this section we attempt to answer two technical questions: 1) how to build a practical knowledge- driven AI system, and 2) how to apply knowledge-driven AI to quant research. The ﬁrst question is about the components of a typical knowledge- driven AI system [181]. Generally speaking, such a system con- sists of two parts: a knowledge base which stores all knowledge we need in analysis, reasoning and decision, and a knowledge reasoning engine which analyzes and makes decisions based on the knowledge [182]. These two parts correspond to the problems of knowledge representation and knowledge reason- ing [183][184], respectively, both are hot research directions in artiﬁcial intelligence. In practice, knowledge graph tech- niques [185] are widely used to build a knowledge base due to their simplicity and scalability. The second question consid- ers how knowledge graph techniques can be applied to quant from the perspective of knowledge representation and reason- ing. We introduce how ﬁnancial knowledge is extracted from various sources and incorporated into a ﬁnancial knowledge graph. We will also introduce some potential application sce- narios where knowledge graph reasoning brings extra beneﬁts for investment. Figure 28 illustrates what a ﬁnancial behavior knowledge graph looks like with an example extracted from a large knowl- edge graph. In this example, various entities including public companies, securities, sectors, individuals, events, as well as re- lationships such as supply chain, capital chain, and behavioral relations are characterized in the ﬁnancial behavior knowledge graph. Information constituting the knowledge graph is col- lected from various data such as news reports, litigation docu- ments, ﬁnancial statements, research reports, and sectors, andextracted using NLP techniques such as natural language un- derstanding and information extraction. Knowledge reasoning techniques can be applied to this graph for further analysis and decision. For example, negative new events or lawsuit out- comes may substantially a ect the stock price of the company under study ( Pharmaceutical A ). Moreover, when the potential risk of this company is discovered from the knowledge graph, it may propagate to related entities such as important sharehold- ers of this company and those companies on the same supply chain. In this section, we introduce knowledge representation and reasoning techniques in §4.1 and §4.2, respectively, and intro- duce the application in quant in §4.3. 4.1. Knowledge Representation The goal of knowledge representation is to encode human knowledge into machine-readable forms. It is the foundation of knowledge-driven AI and it determines the modeling method for downstream knowledge reasoning tasks. 4.1.1. Knowledge Base Techniques In the beginning, we brieﬂy review the history of knowl- edge base techniques (Figure 29) [187]. Early knowledge base concepts started from the semantic network which originates from ancient philosophers centuries ago [188] and it was ﬁrst implemented by computer scientists in 1956 [189]. A seman- tic network is a special graph where the nodes represent con- cepts of interest and the edges represent semantic relationships between these concepts. These semantic graphs can be formal- ized equivalently as a set of semantic triples [190] (a base of the popular series of technical standards such as RDF [191]), and it is moreover applied widely in the later knowledge graph techniques. Construction of semantic networks uses many NLP techniques such as semantic parsing [192], which use entity recognition and relation extraction to parse text data and build the semantic network. 23\nGeneral Problem Solver1959 Expert Systems1970sKnowledge Engineering Environment (KEE)1983KL-ONEFrame Languagemid 1980sSemantic Web2001 Frame-based Languagesmid 1980sResource Description Framework (RDF)1999Google’s Knowledge Graph2012OWL 2 Web Ontology Language2009Knowledge Representation Hypothesis1985 Cyc Project1984OWL Web Ontology Language2004Semantic Net1956Figure 29: Knowledge base history. Figure cited from [186]. Later on, logic-based knowledge representation techniques such as propositional logic [193], descriptive logic [194] and ﬁrst-order logic [195] were introduced, and they quickly be- came the mainstream techniques for decades. As an early logic- based knowledge system, general problem solver was proposed in 1959 [196]. It formalizes problems of interest as Horn clauses [197] and applies logic-based methods to solve them. The ap- plication of general problem solver is limited because most real- world problems are too complex, and this shortcoming leads to the development of expert systems [198] which restrict the knowledge to domain-speciﬁc tasks only. In addition to expert systems, frame-based languages [199] is an alternative tech- nique for logic-based knowledge representation. Frame-based languages characterize real-world objects with concepts such as classes and inheritance relationships, similar to the abstrac- tion in object-oriented programming. Successful examples of frame-based expert systems include the Knowledge Engineer- ing Environment [200], KL-ONE framing language [201] and the Cyc system [202]. Traditional knowledge base techniques faced new challenges in the big data era, where data volume and data exchange ex- ploded. The requirement of new protocols results in the emer- gence of various knowledge standards such as resource descrip- tion framework (RDF) [203] and web ontology language (OWL) [204], and these new techniques are summarized and redeﬁned as a new concept Semantic Web [205]. Semantic web tech- niques are further extended to satisfy large-scale applications in practice and it leads to the development of knowledge graph techniques which is the mainstream methods for building knowledge- based systems in data-driven scenarios. 4.1.2. Knowledge Graph Techniques We introduce the structure of a knowledge graph and how to build it in practice. Structure of Knowledge Graph. A knowledge graph typically consists of two parts [206]: ontology and instances. Ontology is the schema of a knowledge graph, which speci- ﬁes the types and semantic meanings of the entities and rela- tionships [207]. In practice, an ontology is usually encoded with formal languages such as OWL and RDF. For example, in the knowledge graph shown in Figure 28, the ontology speciﬁes the eight types of ﬁnancial entities (industry, indi-vidual, status, etc.) and the various types of relationships be- tween them (supplies, shareholding, subsidiary, etc.). In ad- dition to those regular elements, ontology can further deﬁne other attributes for entities and relationships such as times- tamps, hyperlinks, etc. Instances are the main body of a knowledge graph. They can be expressed as semantic triples, which consist of subjects, predicates, and objects. For example, in Figure 28, the se- mantic triple ( Pharmaceutical A ,supplies ,Medicine C ) indi- cates the fact that Pharmaceutical A is a supplier of Medicine C. In a knowledge graph, the subjects and objects are enti- ties and the predicates are relations. In practice, semantic triples are usually implemented in RDF and stored in graph databases. Knowledge Acquisition. In practice, knowledge graphs are usu- ally very large, with millions or even billions of entities. There- fore, knowledge acquisition techniques are required to automat- ically construct the knowledge graph. As summarized in [186], knowledge acquisition can be performed via knowledge extrac- tion and knowledge graph completion. Knowledge Graph Completion aims to ﬁll up the missing links in a knowledge graph based on existing information, and this problem has been extensively studied. For example, [208] proposes a rule learning model that extracts symbolic rules from knowledge bases with large-scale data. The rules can be further used to infer missing facts between entities. In [209], an RNN model is proposed to compose paths in the knowl- edge graph into embedding vectors. The composed embed- ding is then used to infer the missing relationship between the starting and ending entities of the path. Building from scratch extracts structural information from raw data. Most works on knowledge graph construction focus on the task of information extraction from text data [210, 211, 212]. This task consists of two major steps: entity recogni- tion [213] and relationship extraction [214]). Entity recog- nition involves semantic role labeling [215] which identiﬁes entity roles (e.g., subject, object, etc.) in semantic triples, and entity disambiguation [216] which aligns text tokens to entity names in the knowledge graph. Relationship extrac- tion can be formalized as a prediction problem where the in- puts are entity pairs of interest and the output is the relation- ships between entities. Various machine learning methods 24\nsuch as convolutional neural networks [217], attention mech- anism [218], and graph neural network [219] can be applied to this problem. In addition, some works [220, 221, 222] unify the entity recognition task and relationship extraction task in one end-to-end learning framework, where the inputs are token sequences (e.g., sentences) and the outputs are se- mantic triples. 4.2. Knowledge Reasoning Knowledge reasoning refers to the process of analysis, in- ferences, proofs and decisions (e.g., inferring new facts, gen- erating new conclusions, extracting new rules, etc.) based on existing knowledge and data. Reasoning can be conducted in various ways, including symbolic logic methods, neural meth- ods, and neuro-symbolic methods. 4.2.1. Symbolic Reasoning Symbolic reasoning can be conducted in either a determin- istic way or a probabilistic way. Deterministic reasoning is per- formed by applying inference rules on given facts recursively until reaching the desired conclusion. Meanwhile, probabilistic methods are also applied in practice for more ﬂexible reasoning. Probabilistic symbolic reasoning methods model the distri- bution of the existence of fact triples given the knowledge on the graph. Therefore, logic rules are represented in a softer way, which allows more ﬂexibility in the reasoning process. Various methods have been proposed for probabilistic reasoning. For example, probabilistic logic programming [223] models a logic program as a computation graph similar to Bayesian networks. Markov logic programming [224] involves a ﬁrst-order knowl- edge base where each rule is assigned as scalar weight. In this way, the joint distribution of all the observed and hidden facts is modeled as the normalized exponential weighted sum of the grounds of each rule. There are also other probabilistic reason- ing techniques such as stochastic logic programming [225] and TensorLog [226]. Symbolic reasoning has the advantage of transparency and explainability compared with neural methods. Speciﬁcally, it models knowledge in an explicit way by expressing facts and rules with logic chains or formulas. However, symbolic rea- soning depends on curated knowledge bases that require enor- mous human e ort in construction and maintenance. Moreover, it also su ers from high computation costs since most sym- bolic reasoning algorithms have to search in a high-dimensional search space. This fact usually limits their application in big data. 4.2.2. Neural Reasoning Neural reasoning methods learn decision rules with deep neural networks and can represent non-linear associations. The information in entities and relations is embedded into neural networks. Neural methods utilize gradient descent search in the training process, and achieve better e \u000eciency, especially in big data scenarios. Besides, neural methods also achieve better reasoning performance due to stronger expressiveness. Neural methods train a deep learning model using knowl- edge graph structure described by semantic triples as well asthe attributes of entities and relationships. During the infer- ence process, the trained model predicts the fact of the input triple of interest. The following question is how to encode the input including both structures and attributes. Di erent em- bedding spaces can be used to represent entities and relation- ships. Many works represent entities as embedding vectors in Euclidean space but they di er in the modeling of relationships. [227] represents relationships as points in Euclidean space that translate subject embedding towards object embedding. [228] encodes relationships as projection matrices that map entity em- beddings to low-dimensional space. [229] deﬁnes relationships as 3-D tensors that represent the bilinear similarities between entities from multiple dimensions. In addition to Euclidean space, other types of embedding space can also be used, in- cluding complex vector space [230, 231], Gaussian distribution [232, 233], and manifolds [234]. The possibility of semantic triples can be computed directly from the representations of the corresponding entities and relationships. For example, in [227], the possibility is computed as the distance between object em- bedding and subject embedding through relationship embed- ding. The computation of possibility can be extended to capture more complex interactions between entities and relationships using neural networks. For example, [235] concatenates sub- ject embedding, object embedding and relationship embedding and feeds them to a convolutional neural network. Moreover, neural networks can also leverage the structural information of the knowledge graph. For example, in [236], a recurrent neu- ral network encodes paths on the graph (similar to logic chains) involving multiple semantic triples. In [237], a graph neural network leverages the structural information of the knowledge graph and computes graph convolution to improve reasoning. 4.2.3. Neurosymbolic Reasoning Symbolic reasoning and neural reasoning have di erent ad- vantages, which can be combined in a neurosymbolic reasoning framework. Neurosymbolic reasoning can be conducted by ei- ther injecting logic structures into the embedding framework, or vice versa [238]. For the former idea, [239] incorporates conjunction rules into the computation of relation embedding in multi-hop paths. Following the framework of TransE [227], the relationships of semantic triples on a reasoning path are composited as one sin- gle embedding vector, which is used to translate subject embed- ding towards object embedding. [240] put constraints on the representation learning process to enhance the prediction con- ﬁdence of the conclusions in entailments. Besides, other works also propose various ways to inject logical structures [241, 242] or ontological schemas [243, 244, 245] into the knowledge graph embedding framework. For the latter idea, [246] proposes to infer the missing facts using neural networks and then reason over the queries with Markov logic networks. The whole model is optimized via the expectation-maximization algorithm [247], where the E- step corresponds to inferring hidden facts, and the M-step cor- responds to maximizing the likelihood of the given facts. [248] uses knowledge graph embedding techniques to help shrink the size of candidate sets for fact inference in large-scale knowl- 25\nedge bases. Then, inference via ground network sampling is performed on a Markov logic network to compute the ﬁnal re- sults. Moreover, there are also other works utilizing knowledge graph embedding for logic reasoning [249] and rule learning [250, 251]. 4.3. Application in Quant In this part, we will exhibit how knowledge-driven AI is applied in Quant 4.0 from two aspects: construction of ﬁnancial behavioral knowledge graph and knowledge graph reasoning for quant. 4.3.1. Building a Financial Knowledge Graph Ontology Design. The ontology of a ﬁnancial behavioral knowl- edge graph should cover information from the following as- pects: 1) the fundamental information of ﬁnancial entities, 2) ﬁnancial events happening between ﬁnancial entities, and 3) the causal relationships between entities and events. Correspond- ingly, categories for entities include but are not limited to: Financial entities , including stocks, bonds, banks, public com- panies, important individuals, commodities, etc. Concepts , reﬂecting the fundamental information about ﬁ- nancial entities, such as sectors, industries, exchanges, re- gions and countries, currencies, etc. Events , which are economic behaviors such as administra- tive punishment, illegal actions, litigation states, sharehold- ing changes, personnel changes, etc. Similarly, categories for relationships include but are not limited to: Relationship between entities , such as subsidiaries, belongs, shareholding, etc. These relationships are thus associated with timestamps indicating their beginning and ending times. For example, in Figure 28, the industrial chain and capi- tal chain relationships describe the sector categorization and capital relationships between entities. Relationship between events , such as co-occurrence, lead-lag. Relationship between events and entities between events. For example, in Figure 28 the negative reporting leads to the price change and further investigation on “Pharmaceutical A”, leading to a suspension in trading and litigation. Causal relations are usually inferred from existing knowledge and serve as important auxiliary information in downstream rea- soning tasks. For example, in Figure 28, a “related to” rela- tion connects the event of “negative reporting” and the cor- responding ﬁnancial entity “Pharmaceutical A” to indicate a negative reporting happening to the publicly traded company “Pharmaceutical A”. These relations are usually associated with timestamps indicating the speciﬁc time point that the events happen.Knowledge Acquisition. Knowledge that constitutes the ﬁnan- cial behavioral knowledge graph can be acquired from vari- ous sources, and the most challenging part is to extract use- ful structural knowledge from unstructured data (text data as a representative example). Natural language expression has high ﬂexibility and substantial personality, and thus it is a chal- lenge for machines to accurately understand the information in documents and extract the most useful knowledge to build a knowledge graph. There is a lot of contradictory informa- tion in news and documents, leading to the di \u000eculty in fact extraction. Therefore, probabilistic models and machine learn- ing models play important roles in knowledge extraction with conﬁdence evaluation. Moreover, the information is usually in- complete and extrapolation is needed to infer the knowledge that we are actually interested in. Graph completion is thus required to infer the missing knowledge from the given facts. Fourthly, di erent data sources share inconsistent update fre- quencies, which brings new challenges for an appropriate repre- sentation of the knowledge graph. Both snapshot-based repre- sentations and growing ﬂat knowledge graphs can capture tem- poral information. A growing ﬂat graph is friendly for temporal updates, but it is hard to perform temporal analysis directly on it. On the contrary, snapshot-based representations are naturally suitable for temporal analysis, while also bringing extra costs in storage and management. 4.3.2. Knowledge Reasoning for Quant Given a knowledge graph, we can get meaningful represen- tations of knowledge by reasoning on the graph. For quant, the knowledge representations can be incorporated into exist- ing factors as external information and fed to deep learning models for better predictions. Figure 30 demonstrates a typ- ical pipeline of knowledge reasoning in quant. Speciﬁcally, events and relations between stocks are represented as semantic triples, where the entities and relationships are embedded into vectors. Then, neural reasoning is performed on these semantic triples to compute the embedding of events, relations and the whole knowledge graph. After training on historical data, the knowledge representations are used in investment strategies to generate trading decisions. There are also other works studying the application of knowl- edge reasoning for quant. [253] proposes incorporating rela- tional and categorical knowledge for better event embeddings. Given a semantic triple representing an event, external infor- mation about the entities in the semantic triple is retrieved from a knowledge graph and involved in the computation of event embedding. [252] extracts events from news texts and uses en- tity linking techniques [254] to align the extracted information with the knowledge graph. Then event embeddings are gen- erated using TransE. The embeddings are then combined with volume-price data in a temporal convolutional network [255] for stock prediction, as shown in Figure 31. [53] leverages fun- damental information such as sector categorizations and sup- ply chains to build a knowledge graph and uses temporal graph convolution to compute the embedding of each stock. The em- beddings are then used to predict the stock returns, and the whole model is trained by maximizing the stock ranking loss. 26\nFigure 30: Knowledge graph reasoning for stock prediction. Figure is cited from [183]. Figure 31: Knowledge graph event embedding combined with other factors in stock prediction. Figure cited from [252]. [256] uses node2vec [257] to generate stock embeddings based on a knowledge graph and uses these embeddings to compute the similarity between stocks. In this way, the top-K nearest neighbors are computed for each stock, and the factors from neighbors are used to enhance the original factors. Other works [258, 259] also uses knowledge graph to generate better stock embeddings or perform event-driven investment. 5. Building Quant 4.0: Engineering & Architecture Sections 2, 3 and 4 introduce the three components of Quant 4.0 for the algorithmic perspective. In this section, we “retro- spect” Quant 4.0 from a system point of view and study how to put all these components together in one system. Figure 32 illustrates the architecture of a proposed Quant 4.0 systemframework, including the o \u000fine system for quant research and the online system for quant trading. 5.1. System for O \u000fine Research Quant 4.0 o \u000fine quant research system aims to improve the e\u000eciency of quant research. It contains several layers (hard- ware layer, raw data layer, meta factor layer, factor layer, and model layer) and modules (high-performance computing clus- ters, data system, cache system, data preprocessing, automated factor mining, knowledge-based system, large-scale data ana- lytics, AutoML, and risk simulation). 5.1.1. Hardware Platform Architecture The underlying hardware platform for o \u000fine research is a high-performance computing cluster, consisting of many com- 27\nOne-click Deployment of Deep Learning Models Automated Factor MiningPost-hoc XAI AutoMLRisk & SimulationHardware Acceleration HardwareRaw DataMeta FactorFactorModelDeploymentExecutionAnalysis Large-scale Data Analysis Platform Database System Factor Base: Storage,Dependency, Back-test, Tracking,Analysis Pre-processingComputation Workflow SchedulingPost-trade Risk Control High-performance Computing Cluster Distributed File SystemDistributed Computing Quotation data, financial statements, etc.SQL DatabasesGraph DatabasesMoney flow, supply chain, etc.NoSQL DatabasesNews text, satellite images, etc.Time-series DatabasesMeta factor time series, etc.In-memory DatabasesHigh-frequency meta factors, etc.Sample ImputationOutlier CleaningData TransformationInformation EncodingRepresentation and ReasoningKnowledge-based SystemFactor SearchingFactor ComputingEvaluationStatisticsBacktestCommitEvaluate …Query CommitCommitQueryParallel Computing PlatformCommunication BackendDeep Learning FrameworkNAS + HPOMoEComputing OrchestrationTrade SimulationXAI and Risk AnalysisDeep Learning CompilerIntermediate RepresentationFrontend & Backend OptimizationHardware Code GenerationKernel LibraryBasic Linear Algebra SubprogramsDomain-Specific ArchitectureOptimized OperatorMarket Exchange Server Brokerage ServerTrading System (ColocatedServer)Quote Analysis ModuleData Stream AnalysisTrading Protocol ParsingStrategyManagement &CoordinationTrading Database Deployed StrategiesStrategy #1Strategy #2Strategy #N…Order DataMarket QuotesAttributionRisk Analysis Risk VisualizationAnomaly DetectionRisk XAI Risk ExposureFactor XAIMulti-agent RL FrameworkHigh-precision Market SimulationFactor-StockFactor-Sector Large-scale Pretraining Distributed Graph Platform Inference Engine NeuralSymbolic RDD Runtime Resilient Distributed Dataset (RDD) HDFS Architecture GFS Architecture Model CompressionKnowledge DistillationModel PruningModel QuantizationNetwork AccelerationModel Acceleration Order Execution ModuleOrder SchedulingRisk MonitorAlgorithmic Trading Online TradingOffline Research Post-trade Risk Control Real-time Risk Control Pre-trade Risk Control System-level Risk Control Data-level Risk Control Automated AIExplainable AIKnowledge-driven AISystem OptimizationCausal XAIModel XAI Resumption AnalysisReturn DecompositionAbnormal Risk ExposureTrade-Simulation DeviationImportanceSensitivityFactor-StyleEvent Cache SystemData VisualizationTime Decorrelation Meta DataOperatorsSearch Algorithm BaseValidity Verification SubmitFeedbackCommunicationVector ISA & Streaming SIMD GPGPU Computing ParseCalculateFactor SensitivityModel SensitivityConfigurationRisk Sensitivity Figure 32: Architecture of an example Quant 4.0 engineering platform for investment research and trading. Part of this ﬁgure is cited from [260, 261, 262, 263, 264, 265, 89, 266]. 28\nputing nodes that are mounted with shared storage [267] and interconnected by high-bandwidth network [268]. The combi- nation of multiple nodes aggregates the computing power dis- tributed in various single nodes to support large-scale quant computing tasks. However, communication bottleneck usually serves as one of the constraints for scaling up computing power [269]. To address this problem, the network topology of the cluster adopts a hierarchical structure, where neighboring com- puting nodes are connected via lower-level switches to increase the overall throughput of the cluster. 5.1.2. Design of Data System The data layer for Quant 4.0 system is to collect a tremen- dous amount of ﬁnancial data and provide data management and query service for applications in upper layers. Since ﬁnan- cial data are heterogeneous and multimodal, di erent types of database systems are involved in this layer to manage di erent types of data. Examples include SQL database [270], time- series database [271, 272], NoSQL database [273] and graph database [274]. SQL database stores and manages data following the rela- tional model proposed in [270], where data are stored in ta- bles that represent relations among attributes. Most tradi- tional ﬁnancial data such as quote data and ﬁnancial state- ments can be represented in this type and are suitable for SQL databases. Graph database [275] is designed to store and manage graph- structured data composed of nodes and edges. Such graph structures widely exist in ﬁnancial data since ﬁnancial enti- ties are connected with each other through various relation- ships such as money ﬂow and supply chain relations. Such links may indicate some latent patterns that are shared in neighborhoods. Graph database can be used to store and manage economic graphs, ﬁnancial knowledge graphs and ﬁnancial behavior graphs. NoSQL database [273] is used for storing non-tabular data such as key-value pair, document, and wide-column. It is ap- propriate to manage ﬁnancial text data and image data, such as ﬁnancial report text, news text, social media text, satellite and drone images etc. Time series database [272] is a special database type designed for quickly accessing, computing and managing time series data. Such kind of database engines is optimized to acceler- ate time series data processing, e.g., computing stock stream data collected in real-time, and computing time-series factors for high-frequency trading etc. Moreover, the large volume of ﬁnancial data requires a highly e\u000ecient distributed storage system to accelerate data access. To further improve the read /write speed for high-frequency tick data (limit order book etc), in-memory database [276] could be used as data cache to store the most frequently read data. It saves the data transfer time between hard disk and memory. In addition to layer-speciﬁc components, the data layer, meta factor layer, and factor layer share a large-scale data process- ing platform which provides a complete solution and a conve- nient model for various data-driven tasks. The key componentsof this platform include a distributed ﬁle system that provides convenient and reliable data access on distributed storage sys- tems, and a distributed computing engine that provides simple yet e\u000ecient programming interfaces for parallel computing on a large number of computing nodes. Speciﬁcally, distributed ﬁle systems such as HDFS [264] adopts the architecture con- sisting of name nodes and data nodes, where data replication is performed on multiple data nodes for fault tolerance. On the other hand, distributed computing engines such as MapReduce [277] and its open-source implement including Hadoop [278] and Spark [260, 261] provide programming models for parallel computing tasks. By abstracting parallel computing tasks into a set of primitive operations (e.g. map and reduce in MapReduce and transformation in Spark), the programming tool is ease to use by developers and is generic enough to handle many com- mon parallel programming tasks. 5.1.3. Factor Mining System Raw data have di erent formats, but factor mining requires uniﬁed input format. Therefore, corresponding to the workﬂow in Figure 12, the meta factor layer is involved to preprocess raw data with various modalities into meta factors with uniﬁed formats and appropriate values. The factor layer builds automated factor mining engine and automated factor mining pipeline. The factor mining algorithms have been introduced in §2.2.1. Here we introduce how to im- plement factor mining at scale from a system point of view. In particular, we concern how to improve the system e \u000eciency to discover more “good” factors per unit time. Factor mining system needs a parallelization architecture to improve computational e \u000eciency. Syntactic validity of factors should be checked in real time during the factor generation process to reduce the CPU time wasted by invalid factors. Diversity of factors should be controlled in real time during factor generation process in order to reduce the CPU time consumption for redundant factors. The whole system is backed by some key techniques in dis- tributed execution and computing acceleration. Distributed ex- ecution tools such as message queues and distributed caches enable asynchronous parallel execution on multiple nodes, thus guaranteeing system scalability. Computing acceleration tech- niques such as vector and streaming SIMD instructions on CPU and massive parallel computing on general-purpose graphics processing unit (GPGPU) [279] signiﬁcantly improve the com- putation e \u000eciency for data frame operations and thus increas- ing the productivity of the whole system. Meta factor layer, factor layer, and model layer are con- nected to the factor base, which is an integrated platform for the storage, computation, dependency management, backtest, tracking, and analysis of all factors (the output of models are also factors). Factors generated in these layers are commit- ted to the factor base in various forms. Speciﬁcally, meta fac- tors pre-processed from raw data are directly committed to the factor base in data frames, with appropriate descriptions about their data sources and pre-processing methods. Factors gener- 29\nated via automated factor mining are committed to the factor based in the form of symbolic expressions where the operands are other factors in the factor base. The model outputs gener- ated in the model layer can also be regarded as speical machine learning factors, which can be committed to the factor base to- gether with speciﬁcations of input factors, model architectures, hyperparameters and training descriptions, etc. 5.1.4. Knowledge-based System In parallel with the factor mining system, a knowledge- based system is also involved in the overall architecture to pro- vide knowledge-driven AI practice. As discussed in §4, the knowledge-based system consists of two modules: a knowledge base for knowledge representation and an inference engine for knowledge reasoning. Speciﬁcally, a distributed graph comput- ing platform is used as the knowledge base to store large ﬁnan- cial behavior graph and the inference engine is built upon for downstream ﬁnancial knowledge reasoning and decision mak- ing. Compared with traditional small knowledge bases, ﬁnan- cial behavior knowledge graph in Quant 4.0 has the potential to grow up to a scale with billions of nodes and edges, and thus it requires a ﬂexible and scalable architecture [280, 281] to store and manage large-scale dynamic graph data. Therefore, we need a distributed graph computing and management plat- form, which partitions the whole knowledge graph into a num- ber of subgraphs distributed in di erent nodes of a computing cluster. Due to the sparse nature of knowledge graph, this par- tition is speciﬁcally arranged to maximize data locality, where graph nodes located in the same partition are more densely con- nected than those located in di erent partitions [282]. For the inference engine, both knowledge graph embedding and rule- based symbolic reasoning algorithms can be implemented on this platform. 5.1.5. Modeling System The model layer is in charge of automatic generation of ma- chine learning models and the corresponding risk evaluation and backtest simulation procedures before they are deployed into real-world environments. Therefore, this layer involves two major components: an AutoML module and a pre-trade risk analysis and simulation module. The AutoML module implements automated model gener- ation algorithms upon large-scale distributed deep learning sys- tems. The bottom layer of its technology stack consists of par- allel computing platforms such as CUDA [283], and communi- cation backends such as message passing interface (MPI) [284] that provide standard interfaces for communications between computing nodes in a distributed system. In the second layer of the technology stack, deep learning frameworks such as Py- Torch [285] provide basic interfaces for training and inference of deep neural networks via hardware-accelerated linear alge- bra operations and automatic di erentiation engines. In addi- tion, computing orchestration systems such as pathways [286] combine low-level communication primitives with deep learn- ing framework functionalities to implement higher-level par- allelisms such as model parallelism and pipeline parallelism,which is further wrapped as standard interfaces for upper-level programs. The top layer of the technology stack consists of im- plementations of model generation algorithms such as neural architecture search and hyperparameter optimization (NAS +HPO) [287], mixture of experts (MoE) [265] and large-scale pretrain- ing [288, 289]. The risk and simulation module identiﬁes and analyzes the potential risk exposures of the models before they are deployed to real-world trading environments. This module implements explainable AI techniques applied to analyzing factor, model, and causality to reveal nonlinear risk exposures that are more complex than ordinary risk exposures explained by the BARRA model. In addition, market simulator [290] is used to test the performance of trading strategies with higher precision than backtest, whose results may be biased by historical data. Specif- ically, the simulation environment system [290] can be built us- ing multi-agent reinforcement learning [291, 292] where agents imitate the behavior of various market participants in real world [293]. 5.2. System for Online Trading The online trading system focuses on deploying investment strategies for real trading and executing post-trade analysis, and its major goal is to achieve low trading latency and high execu- tion e \u000eciency. The trading system consists of three modules: deployment, trading and analysis, and we introduce their func- tions in the following part. 5.2.1. Model Deployment The deployment layer aims to implement the philosophy of technology “one-click deployment”. It involves a computa- tion scheduling module, an automated deployment module, and an optional hardware acceleration module. The computation scheduling module arranges a reasonable and e \u000ecient compu- tation order for factors based on their intrinsic data dependen- cies which form a directed acyclic graph (DAG) over factors. In the computation scheduling system, each factor starts its com- putation if and only if all its preceding factors on the DAG have ﬁnished their computation. Computation scheduling is a te- dious work and it should be automated by system due to the following reasons. We must synchronize o \u000fine factor dependencies with online factor dependencies and keep their consistency in real time. The number of factors may grow up quickly. Imagine what happens when you accumulate millions of factors? It is a nightmare for any quant researcher to deploy so many factors by hand. Adding, deleting and updating factors is the daily job of fac- tor maintenance, which relies on correctly managing factor dependencies. From the algorithmic point of view, the problem of computation scheduling can be regarded as topological sorting on the depen- dency DAG. In practice, the scheduler is designed to coordinate system components and schedule their executions according to the computation order on DAG. In common implementations [294] of such scheduling systems, asynchronous scheduling is 30\nadopted to improve the overall execution e \u000eciency. In this way, the pending steps can start their execution immediately after the previous steps ﬁnish. The automated deployment module aims to deploy deep learning models trained from o \u000fine research to online trad- ing. It implements the algorithmic techniques discussed in 2.4. In addition to deep learning compilers and model compression engines, this module also involves optimized hardware kernel libraries, which provide implementations of common data pro- cessing and modeling functions that are highly optimized based on hardware features. Popular examples of such libraries in- clude cuDNN [295] and MKL [296]. Functions in kernel li- braries typically include basic linear algebra subprograms (BLAS) [297] that are standard interfaces for scientiﬁc computing, and some optimized operators for deep learning such as 3 convo- lution. The implementations are speciﬁcally optimized based on domain-speciﬁc architectures [298] on devices (e.g. Tensor Core [299]) to maximize the potential of hardware in use. The hardware acceleration module aims to improve the com- putation e \u000eciency of data processing and model inference us- ing special hardware technology such as ﬁeld-programmable gate array (FPGA) acceleration. Strategy components such as network protocol stack or machine learning models implemented on FPGA with customized logic can bypass redundant logics that are inevitable on generic hardware, thus achieving lower latency and winning an advantage for traders over other market participants. However, to deploy strategies on speciﬁc hard- ware, it usually takes a huge amount of development e ort to complete strategy migration with satisfactory speed. Therefore, high-level synthesis techniques [300, 301, 302] are developed to address this problem. They directly generates register trans- fer languages for high-level representations in which strategies are originally implemented. 5.2.2. Trading Execution The execution layer converts trading decisions to actual or- ders that are executed in exchanges, and its goal is to reduce the trading latency as much as possible in order to capture the ﬂeet- ing trading chances in the market. The latency can be decom- posed into two parts: transmission latency is the delay of sig- nal communication between trading servers and market servers (exchange server or brokerage server), and computation latency is the delay between quotes receiving and order sending. To reduce transmission latency, the trading system is usually de- ployed on servers that are colocated with market servers (such as racks and cabinets provided by brokers). To reduce computa- tion latency, a trading system must be optimized in full strategy pipeline from data collection to order execution through various software and hardware acceleration techniques. 5.2.3. Trading Analysis The analysis layer monitors the execution of investment strate- gies and performs analysis for further adjustments. It involves a post-trade risk control module responsible for ordinary per- formance monitor and a post-hoc XAI module for explaining strategy behavior from the perspective of AI. The post-traderisk control module performs return and risk attribution, aim- ing to analyze strategy’s performance and revealing intrinsic risk structure hidden by machine learning blackboxes. Further- more, the post-hoc XAI module provides in-depth analysis and explanation of investment strategies. It provides thorough risk analysis by analyzing all strategy components in terms of im- portance and sensitivity, and visalizes risk results. Remarks: Risk control is one of the core tasks of quantitative invest- ment and is also the primary consideration in system design. In our proposed engineering framework, the idea of risk control runs throughout the whole architecture. Speciﬁcally, system- level risk control is reﬂected in hardware and raw data lay- ers, where the reliability and stability of the underlying hard- ware and raw data are the top priority. Data-level risk con- trol is reﬂected in meta factor and factor layers, where qual- ity control and management of factors and knowledge are em- phasized. Pre-trade risk control is reﬂected in the model layer, where model robustness and explainability are required. Real- time risk control is reﬂected in deployment and execution lay- ers, where the system’s trading behavior is conﬁned within con- strained areas to avoid unexpected situations. Post-trade risk control is reﬂected in the analysis layer, where detailed analysis is conducted to provide comprehensive and reasonable insights into strategy performance. 6. Discussion on 10 Challenges in Quant Technology We have summarized 10 challenges in the development of next-generation quant technology. These challenges range from computing and data infrastructure, investment modeling, risk modeling, market simulation and cognitive AI technology, and they provide new research directions for researchers interested in AI technology and quant. We believe some problems might be solved in the next couple of years with the rapid development of AI technology, while others may remain challenging for a long time. 6.1. Exponentially Growing Demand of Computing Power With the rapid development of GPU technology and par- allel computing technology, AI models have been scaling up year by year. The number of parameters in deep learning mod- els has grown exponentially from 94 million (ELMo in 2018 [304]) to over 500 billion (Megatron in 2022 [303]) (Figure 34). The rapid growth in model size not only improves mod- els’ performances, but also leads to a paradigm shift which is deeply a ecting the technical roadmap of AI research. Accord- ingly, as an important domain going all-in on AI, Quant 4.0 heavily relies on ultra large-scale computing power and related engineering technology. 6.1.1. Quant 4.0 and Supercomputers Investment Research of Quant 4.0 requires super-computing infrastructure as a fundamental support for large-scale factor mining, large-scale modeling, back-test and evaluation. One 31\n17Spatiotemporal Modeling 13 Financial Knowledge Engineering14Financial Metaverse & World Model SimulatorQuant Modeling AGI Research Data & Knowledge Engineering Infrastructure16AI Risk Graph & Systematic Modeling15Cognitive AI & Causality Engineering 12 Alternativ eData Technology18Universal Modeling19Robust Modeling1End-to-end Modeling10 11 Exponentially Growing Demand of Computing PowerFigure 33: The 10 challenges in quantitative investment technology. Figure 34: Growth of model sizes (measured by number of parameters) from 2018 to 2022. Figure is cited from [303]. important idea behind Quant 4.0 is to “replace human power with computing power” in quant research. We have to notice that Quant 4.0 has much more demand on computing power than what we could usually image ever before, because of the following reasons: Large-scale automated factor mining requires large distributed computing clusters with either homogeneous CPU paralleliza- tion or heterogeneous CPU-GPU parallelization. Even on a relatively simple homogeneously structured computing clus- ter, a high-frequency alpha factor mining task needs millions of CPU cores in order to achieve enough e \u000eciency, perfor- mance and diversity in intensive factor search and evaluation. In particular, as more and more factors are discovered and ac- cumulated, it will take longer and longer computing time (or equivalently computing power) to ﬁnd a new qualiﬁed fac- tor which not only performs well but also weakly correlated with existing factors discovered before. The same compu- tation power problem happens similarly for heterogeneous clusters.Large-scale deep learning is necessary in Quant 4.0 research as some billion-parameter-level or even trillion-parameter- level deep learning models with large data volume do exhibit superior performance in many AI scenarios [305]. More and more large models are applied to quant tasks such as mar- ket prediction, portfolio position computing, risk forecasting and real trading by hedge funds and other institutional in- vestors. However, the training process of large deep learning model requires extremely high computing power. For ex- ample, given a scenario of cross-sectional alpha model on 4000+stocks and hundreds of daily factors of 10 years’ his- tory, a Transformer-type deep neural network with about ten billion parameters needs about 1-5 days to ﬁnish a single training /validation cycle on a cluster with 100 Nvidia A-100 GPUs, without counting the additional computational cost due to rolling training, model ensemble and autoML. Rolling training , i.e., iteratively retraining the same model with data sampled from a time window shifting towards the future, is a characteristic of quant modeling since the pat- terns of ﬁnancial market and the patterns of investment in- struments vary over time. In fact, ﬁnancial time series are strongly non-stationary, and thus data features exhibit dif- ferent distributions in di erent market styles and di erent time periods, and the performance of a model usually de- cays over time without retraining on recently incremented data. This phenomenon means a one-time model training on data like what people do in many other AI scenarios such as natural language process and image recognition doesn’t work well in quant research. Therefore, rolling training is widely adopted in quant research for back-test simulation, paper trading and real trading. Imaging an experiment rolling training once a week, a ten years’ back-test process retrain the model 10\u000250=500 times, signiﬁcantly increasing the computational cost. Model ensemble [306] is a common way to improve perfor- mance and robustness of a quant strategy and reduce the risk of portfolio asset loss. Moreover, ensemble of diversiﬁed or uncorrelated models could usually improve strategy perfor- mance and asset return. However, the computational cost in- 32\ncreases linearly with the number of models combined in a strategy, and computer power becomes an important founda- tion for strategy stability and risk management. NAS and HPO are another two modules in quant pipeline heavily consuming computing power. Both of them are for- malized as search-and-optimization problems in the network architecture space and the hyper-parameter space, respec- tively, and the computational cost is nonnegligible even if more and more fast algorithms are developed to improve the search e \u000eciency. Feature selection (or feature importance computation) plays an important role in improving model performance and en- hancing model explainability. Unfortunately, feature selec- tion algorithms for deep learning are usually computation- ally intensive as well due to complicated feature interactive e ects in the model, and they require su \u000ecient computing power to identify signiﬁcant factors within a ordable time. 6.1.2. Solving Computing Power Dilemma Learning large model is very expensive. Figure 35 com- pares a number of deep learning models on code generation tasks in three aspects: parameter size, sample (token) size and estimated cost. We can see that GPT-3 model has about 170 bil- lion parameters and costs about 2 million USD to complete the computation, which is obviously too expensive to be a orded for most ﬁnancial institutions. Given the expensive and limited computing power, what are the solutions to this dilemma? We attempt to provide a few suggestions and research directions. Figure 35: Comparison of large-scale deep learning pretraining models for code generation from model parameter size, data (token) size and estimated training cost. Figure is cited from [307]. Research of faster algorithms is the most direct and most sig- niﬁcant way to reduce computational cost. However, it is a di\u000ecult problem to reduce the computational complexities of deep neural network algorithms, NAS /HPO algorithms and feature selection algorithms without losing their performance and it needs the joint e ort of researchers from multiple rel- evant areas including but not limited to applied mathematics, theoretical computer science and machine learning. Online learning [308] and incremental learning [309] are ma- chine learning techniques where data are acquired sequen- tially along time and the model is updated once new data are received without retraining the model on all data, as opposedto batch learning techniques such as traditional deep learning which train a model using the entire training data set at once. The computation time can be saved by model updating rather than model retraining. Pretraining-ﬁnetuning paradigm could help save overall com- putation time as well. Speciﬁcally, in the pretraining stage, we build a general but computationally intensive deep learn- ing model and in the ﬁnetuning stage, we adapt it to various domains, scenarios and tasks in trading. After the computa- tionally expensive pretraining process has been ﬁnished, the following ﬁnetuning tasks cost signiﬁcantly less computation power and can be repeatedly performed. Model diversiﬁcation reduces model similarity and reduces information redundancy in model representation, thus decreas- ing the number of required models in the ensemble. New al- gorithms are required to improve the diversity of a new model compared with existing ones in model bases. In particular, we need new techniques to enforce the model diversity be- fore or during the training process directly. Utilizing informative low-frequency data is a way to enhance computational e \u000eciency without too much e ort on engi- neering techniques or algorithmic tricks. Many types of low- frequency data such as various alternative data, fundamental data and event data are informative for investment decisions, complementing data with relatively high frequency including quote data and limit order book data. Due to the sparsity and small size nature of these low-frequency data, and moreover, easy to compute for modeling. Sharing computing power and large model is a potential busi- ness/operating model to avoid overlapping investment in com- puting and modeling infrastructure and to dilute the cost across the whole quant investment industry. Although cloud com- puting service is an option for computing power sharing, it is not designed speciﬁcally for quant investment. In partic- ular, almost all architectures and software systems of cloud computing clusters are not optimized towards the computa- tional demand of quant. Therefore, this industry needs its own professional quant computing sharing mechanism, plat- form and service. A similar sharing mechanism could be ap- plied to large models as well. As a business model, ﬁnancial institutes such as hedge funds and mutual funds could buy li- censes for the usage of supercomputers and pretrained large models, just like buying ﬁnancial data. 6.2. Alternative Data Technology In principle, any data about economic activities have the potential to be used for investment and might be considered by quant researchers. Alternative data, a concept opposite to conventional ﬁnancial data such as ﬁnancial statements, stock quotes and limit order books, provides a much broader space for quant research. Although the concept of alternative data has appeared for a decade and is getting more and more popular in quant industry, it is still in the early stages of industry appli- cation because of a few reasons. Firstly, many alternative data 33\nsets such as news and research reports have very narrow cov- erage and low breadth since a news event usually only relates to a limited number of stocks, and you couldn’t expect a public company has news or signiﬁcant events every day. Secondly, some alternative data such as satellite images are too expen- sive for small funds, and relevant satellite image detection and recognition techniques are not well-established due to limited labeled data and expensive labeling costs. Thirdly, processing and cleaning raw alternative data is a time-consuming “dirty work”, and some informative alternative data can not be sup- plied legally and sustainably. In this subsection, we brieﬂy in- troduce some examples of alternative data and the correspond- ing processing techniques, and discuss the di \u000eculties in data acquisition and data aggregation. 6.2.1. Examples of Alternative Data We list a few types of alternative data as examples and sim- ply discuss the technical di \u000eculties and opportunities for ex- tracting informative signals from them. To avoid repetition, those alternative data already introduced in §4, such as supply chain data, are omitted here. Text data from news and social media are one of the most pop- ular alternative data used by investment institutions. The key is to extract traders’ sentiment signals correlated with future market trends. Natural language processing techniques such as opinion mining, sentiment modeling and trend tracking could be used to build useful alpha signals or factors. Satellite image data have been used by hedge funds and other institutional investors for many purposes. For example, in- vestors could use satellite images to estimate how busy the parking lots of retailers are, providing investment signals for longing or shorting the retailers’ stocks. Currently most satel- lite images are read and analyzed by human, but we believe computer vision techniques such as image recognition and object detection have huge application potential in the fu- ture when main technical issues (e.g., cheap labeled data) are solved. Merchandise sales data from e-commerce track sale records of consumer products companies from their online channels and estimate their actual income before an upcoming ﬁnan- cial statement is reported. This kind of data is usually frag- mented and so correct data aggregation is important. In addi- tion, we have to follow relevant data protection laws to make sure the data legitimacy. Logistic and inventory data track transportation and logistic activities of a company to estimate their sales performance which might be helpful for predicting the corresponding stocks. Credit card or e-payment transaction data is another angle to learn the operation and sales performance of a company or an industry, providing a possible ﬁne-grained analysis through the information of micro-economic activities. Data legiti- macy and data sensitivity are the main concerns before using them. Geolocation data collect foot-tra \u000ec information by GPS or cellphone locators. For example, an investor could use foot- track data for pair-trading arbitrage of Adidas and Nike byanalyzing their geolocation activities in di erent retail shops and predicting spreads between the stock prices of these two companies. 6.2.2. Problems in Data Acquisition Traditionally, hedge funds and other institutional investors acquire data from third-party data providers or data vendors. With the rapid accumulation of internet data, more and more hedge funds started data collection from websites using web crawlers. However, collecting data by web crawling is su er- ing unprecedented restrictions because more and more websites are attempting to protect their data using protocol securities and anti-crawler techniques. In addition, many informative data sets such as transaction data are distributed in various banks, e- payment institutes and credit card companies, and few of them allow users to take data out of their own servers considering intellectual property protection, data security and privacy pro- tection. Here we provide a few possible ideas to collect and utilize more protected data legally. Certiﬁcate techniques for data asset ownership are the foun- dation for data owners who are willing to share their data and obtain legal income from data users. Decentralized data management techniques such as blockchain techniques pro- vide a potential solution for this mechanism design problem. Data encryption techniques [310], including data storage en- cryption, data transfer encryption and data computation en- cryption, provide technical solutions for protecting the inter- est of data owners and data users (e.g., investors), and poten- tially encourage the willing of data exchange and data shar- ing. Federated learning [311, 312] is a machine learning tech- nique for modeling data sources distributed across multiple decentralized servers each only holding a part of the whole data samples and it realizes information fusion without ex- changing those local data. Federal learning techniques have the potential to help investors improve their quant models without spending too much time dealing with data security and privacy issues with massive data owners. 6.2.3. Problems in Data Aggregation Data aggregation is important for quant to ﬁnd alpha sig- nals across di erent types of data. However, data structures of alternative data are diverse and heterogeneous, resulting in dif- ﬁculties in data aggregation, especially when integrating with traditional ﬁnancial data. Heterogeneity in frequency is a characteristic of many alter- native data types such as news data and geolocation data, since they are collected irregularly. Therefore, it’s important to align time-series signals occurring at unaligned time points across di erent instruments and di erent data types. A series of problems naturally arise. For example, how to build pre- diction models on these irregular and sparse data samples? How to better estimate and impute missing information in al- ternative data? All these data processing problems need to be further explored. For example, it is worthwhile to consider 34\nwhether data embedding techniques learning “good” repre- sentations in lower-dimensional latent spaces help fuse het- erogeneous data. Di erentiating signals from noise is a di \u000ecult problem for the extraction of alpha signals from alternative data and should be carefully examined during data processing. Due to lim- ited sample sizes and irregular sample timestamps, identify- ing true signals out of false positive patterns is more compli- cated than ever before. Therefore, it is demanding to develop new robustness techniques and signiﬁcance test techniques to help evaluate the truth of signals and the signiﬁcance of patterns. 6.3. Financial Knowledge Engineering As we have introduced in §4, knowledge-driven AI will play an important role in future quantitative ﬁnance. Research- ing new knowledge representation methods, building complete and reliable knowledge bases and developing new knowledge reasoning and decision algorithms are crucial problems in knowl- edge engineering for ﬁnancial investment. 6.3.1. Di \u000eculties in Knowledge Engineering Financial knowledge engineering is an engineering pipeline and research area for constructing e ective AI knowledge sys- tems covering knowledge acquisition, knowledge representa- tion, knowledge management, knowledge reasoning, risk anal- ysis and investment decision-making. Given the limitations of popular knowledge techniques, it is necessary for researchers to explore more e \u000ecient and more sophisticated methods to bet- ter represent and leverage di erent types of knowledge such as declarative knowledge [313], structured knowledge [314], procedural knowledge [315] etc. In particular, exploring and researching particular knowledge representation techniques for quant and other ﬁnancial applications are valuable for contribut- ing to the development of investment industry and for ﬁnding a broader investment ﬁeld for quant. Before the wide applica- tion of knowledge engineering in quant, a number of technical di\u000eculties need to be solved in the future. More sophisticated knowledge representation methods are ex- tremely demanding for building an e ective ﬁnancial knowl- edge engineering in the future. In particular, we need bet- ter data structure and model structure to encode knowledge relevant to all types of economic and ﬁnancial theories and practical activities. More advanced knowledge management system is the require- ment of ﬁnancial knowledge engineering. It requires more research on how to build an automatic and self-updating pipeline of knowledge acquisition, knowledge update, knowledge ag- gregation and knowledge correction, and on how to imple- ment a reliable and scalable knowledge management system. Next-generation knowledge reasoning algorithms should be more explainable, providing more reliable analysis and pre- diction results. We encourage machine learning researchers to pay more attention to this area, which is part of next- generation AI core techniques.6.3.2. Knowledge Engineering vs Large Model Large-scale pretraining models have been widely used in many AI ﬁelds especially in natural language processing and computer vision and in multi-modal pattern recognition tasks. For example, large model GPT-3 [316] has about 175 billion pa- rameters, and it could be imagined as a knowledge “crystal ball” for natural language generation and natural language reasoning. In this respect, large pretraining models play a similar role as knowledge engineering, and both of them could provide deci- sion support and content generation function for downstream tasks. So whether large model is a good substitute for knowl- edge engineering? In our opinion, it is not true. Knowledge engineering has its particular advantage in explicit knowledge representation and knowledge logical reasoning, making the de- cision process transparent and understandable. Moreover, as the memory module in knowledge engineering, knowledge base has its advantage in ﬂexibility, storing and managing all types of popular data structures including entities, facts and even rules. As a matter of fact, these two technology roadmaps are com- plementary to each other and have the potential to be integrated into a uniﬁed framework to improve the ﬁnal performance of investment decisions. 6.4. Financial Metaverse &World Model Simulator In quant research, it is very important to understand the un- derlying logic and micro-structure of ﬁnancial markets. For ex- ample, we would like to know how the market will react to speciﬁc news about a ﬁnancial statement fraud event, or how much a big order a ects the asset price in the market. Unfortu- nately, empirical studies using historical data usually result in biased conclusions because they do not provide experimental access to all relevant information. In particular, even if cer- tain extreme market events have never happened in history, it doesn’t mean they will not happen in the future. What will hap- pen if our trading strategy meets these assumed extreme events? Back-test experiments based on historical data can’t answer this question, but we expect that ﬁnancial metaverse can do it. Fi- nancial metaverse aims to build a simulated ﬁnancial market parallel to real-world ﬁnancial markets and use it as an experi- mental environment to simulate situations other than what have happened in real markets. 6.4.1. Financial Metaverse Market Simulator Daniel Freidman, UCSC Economics Professor, expressed his view that simulation of markets provides a powerful tool to analyze not only individual participant behavior, but also over- all market reactions that emerge from the interaction of individ- ual agents [290]. Financial metaverse should be able to support various kinds of research experiments (about traders’ behavior and market phenomenon) that are di \u000ecult to complete using historical data or trading experiments in real markets. Such a simulator could be used in a number of di erent scenarios. We list a number of examples as follows. Accurately estimating the market impact of a big order at certain market conditions and certain time point 35\nAnalysis of trader behaviors reacting to a particular market event Understand the impact of a certain type of traders in markets High-precision simulation of transaction cost Running treatment-control random experiments to test causal e ect to answer “what if” questions against particular histor- ical dates Besides the above applications, ﬁnancial metaverse can also be applied to accurately justify causal e ects of economic factors. This motivation is achieved by designing and conducting ran- domized experiments in this simulated market. Moreover, ﬁ- nancial metaverse can also provide a fundamental experimental platform for causality engineering discussed in §6.5. Although ﬁnancial metaverse is of great value for quant research and ﬁ- nancial market research, there are many technical di \u000eculties we have to face. There is no way to collect ﬁne-grained data identiﬁable to in- dividual traders, with which market simulation will become much easier. The data we have are lack information about the motivation and intent of every trader. A simulator can not enumerate and involve all possible fac- tors a ecting a market. Current computing power can not support high-precision sim- ulation with a large number of trading agents in ﬁnancial metaverse. 6.4.2. World Model for Simulation A technical problem of ﬁnancial metaverse is the complex- ity of the simulation environment from which agents in the same reinforcement learning system are hard to learn, and it makes traditional reinforcement learning algorithms hard to learn millions of weights of a large model. Usually a ﬁnancial meta- verse requires at least thousands of agents, each of which is modeled by a large neural network and plays a speciﬁc group of traders with some typical style, to participate in simulated market trading and it aims to recurrent the real-market by a re- inforcement learning simulation as precise as possible. This makes regular computing power incapable of completing such accurate simulations. World model [317] provides a potential solution for this di \u000eculty in ﬁnancial metaverse. It is inspired by the function of human brain which develops a mental model of the world by learning an abstract representation of com- plex information ﬂushing into the brain. In a world model, the dimension of outputs from the simulator (environment) is re- duced using unsupervised learning such as variational autoen- coder (V AE). We can use this abstraction to train small network controllers which let the training algorithm search on a small space for credit assignment task, and thus the computation can be accelerated. Figure 36 illustrates the structure of a world model for computer vision problems. The cognition abstrac- tion idea in world model could be used in ﬁnancial metaverse to solve the simulation computation problem for ﬁnancial mar- kets. Figure 36: An example of world model architecture. Figure is cited from [317]. 6.5. Cognitive AI &Causality Engineering Cognitive AI has been regarded as the future direction of artiﬁcial general intelligence (AGI) [318] by many domain ex- perts. In this subsection, we discuss its advantages and techni- cal challenges in investment application, and we propose a new concept causality engineering as a potential solution for causal machine learning in AGI. 6.5.1. Cognitive AI for Investment In 2011, Israeli-American psychologist and economist Daniel Kahneman published his book Thinking, Fast and Slow [319], introducing two modes of thought for the ﬁrst time. Specif- ically, system 1 thinking, driven by instinct and experiences, is a near-instantaneous process and happens automatically, in- tuitively, and with little e ort. On the other hand, system 2 thinking, driven by comprehensive logical reasoning, is slower, more conscious, more deliberative, and requires more e ort. Contemporary mainstream AI technology such as deep learn- ing and reinforcement learning is running on a fast track ap- proaching system 1 thinking, and we refer to it as perceptive AI . On the contrary, cognitive AI techniques aim to simulate sys- tem 2 thinking of human, providing more sophisticated, more logical and more understandable AI solutions. Many pioneer- ing researchers have proposed their opinions and /or solutions for cognitive AI. For example, Gary Marcus proposes “a hy- brid, knowledge-driven, cognitive-model-based approach” to- wards robust artiﬁcial intelligence [320]. Yoshua Bengio identi- ﬁes system 2 deep learning as being able to “understand, reason and generalize beyond training distributions” [321] and pro- poses corresponding techniques such as causal machine learn- ing and generative ﬂow networks [322] to achieve this goal. Figure 37 illustrates the concepts of system 1 and system 2, and explains their appropriate scenarios and tasks for invest- ment decisions. Di erent from perceptive AI mainly applied in high-frequency and high-breadth trading tasks, cognitive AI gives quant opportunities to touch those high-capacity but low- frequency investment strategies, including value investing which buys/sells securities that appear underpriced /priced through multi-dimensional fundamental analysis and usually holds the 36\nSystem 1 for Quant •Strategy Scenarios ➢High -frequency Trading ➢Cross -sectional Trading •Conditions ➢Various Types of Big Data •Modeling Techniques ➢Deep Learning ➢Statistical Machine Learning System 2 for Quant •Strategy Scenarios ➢Value Investment ➢Global Macro •Conditions ➢Financial Knowledge Grap h •Modeling Techniques ➢Knowledge Graph Reasoning ➢Causal Inferenc eFigure 37: Comparison of System 1 and System 2 in AI technology for quant research. position over months or even over years, as well as global macro investment which selects assets and establishes portfolio based on the interpretation and prediction of large-scale events related to national economies (interest rate trends, CPIs growth, GDPs growth, unemployment rates, policies, etc.), and international relations (inter-governmental relations, international trade, cross- border payments, etc.) to decide long /short positions in various equity, ﬁxed income, currency, commodities, and futures mar- kets. 6.5.2. Causality Engineering Causal inference [323] and causal machine learning [324, 325] have been regarded as one potential technical route to- wards AGI [321]. For investment tasks, understanding the true causal relationships among numerous factors is extremely chal- lenging, due to the di \u000eculty in enumerating all possible con- founding variables that a ect the future trend in one single ex- periment and give us a misleading conclusion. Therefore, we propose a potential solution through causality engineering, a proposed concept which aims to build and maintain a large- scale causal diagram database storing and managing all known ∈ferred causal e ect relationships and potential confounding variables [326] and causal variables. Di erent from regular alpha fac- tor bases, a causal diagram database collects as many economic factors and computes their intrinsic conditional probability of causality between each other. Moreover, causality engineering will develop a series of algorithmic tools for di erent purposes, including testing the statistical signiﬁcance of causal pairs, test- ing the e ect of potential confounders, and searching signiﬁ- cant causal pairs or causal clauses. Leveraging causality engi- neering, we expect that most main confounding factors disturb- ing the correct justiﬁcation of true causal e ect factors could be discovered in experiments and could be eliminated correctly using appropriate statistical methods.6.6. AI Risk Graph &Systematic Modeling The rapid growth of various types of ﬁnancial big data brings us opportunities to model and analyze ﬁnancial risk systemat- ically, ranging from macroeconomy to micro-market. We pro- pose the concept of AI risk graph , a special ﬁnancial knowl- edge graph for recording, computing, analyzing and forecast- ing ﬁnancial risks at di erent hierarchical dimensions, includ- ing country, district, industry, sector, etc. 6.6.1. Risk Graph for Systematic Modeling An AI risk graph should be able to represent the causal de- pendency of di erent types of risk among public companies, private companies, banks, important individuals and many other economic entities, and represent risk transfer between various entities. Such a graph could be used to systematically model various types of risks at di erent levels by computing the con- ditional probability of risk for a speciﬁed object (public com- pany, sector, industry, etc) at certain conditions (time, market environment, debt leverage, etc). Various statistical graphical models and machine learning algorithms could be applied to estimate risk conditional probability. Moreover, AI risk graph could help decompose observed risk value in a more scientiﬁc and more intuitive way in order to improve the interpretability of risk analysis and risk management. 6.6.2. Complex Risk Measure for Investment Classic risk management techniques such as BARRA [48] risk factor analysis model are used in measuring the overall risk associated with securities relative to the market risks. Specif- ically, the model decompose the overall risk into a number of exposures from di erent risk factors with a linearly additive in- terpretation. However, the limitation of linear risk modeling is obvious, in particular when the prediction model is built with highly nonlinear machine learning sample ﬁtting. How to char- acterize, measure, and evaluate nonlinear risk is an important research topic in quant. In particular in nonlinear modeling sce- narios. How to deﬁne a reasonable and practically useful nonlinear risk measure? How to make sure the nonlinear risk by deﬁnition exists and is identiﬁable? How to tell the di erence between risk and noise in an ex- treme market? What proportion of overall risk could be explained by linear and nonlinear risk? 6.7. Spatiotemporal Modeling The data structure for stock modeling is typically a tensor with three orthogonal axes: time, stock, and factor. Tradition- ally, stock strategies are developed either along the time axis (called time-series modeling or temporal modeling) or along the stock axis (called cross-sectional modeling or spatial mod- eling). These two modeling types have signiﬁcant di erences in strategy development. Speciﬁcally, cross-sectional modeling only compares relative strengths of investment signals within 37\nthe same cross-section at some time point, and the signal strengths of the same stock at di erent time points are usually not com- parable. Cross-sectional modeling has its advantage in neutral- izing market risk automatically by longing top stocks and short- ing bottom stocks ranked by cross-sectional trading signals. On the other hand, time-series modeling treats each stock individu- ally, and the trading signals of di erent stocks at the same time point are not comparable. 6.7.1. Unifying Cross-section &Time-series A technically di \u000ecult but practically feasible idea is to merge cross-sectional modeling and time-series modeling in a uniﬁed framework, in order to absorb the advantages from both sides. For this aim, we attempt to provide a few tips on how to build a uniﬁed model and what potential di \u000eculties may exist. A uniﬁed model needs to update stock cross-sections in a very high frequency (in seconds, for example) in order to match the prediction paces of time-series modeling. A uniﬁed model should be very selective in longing and short- ing stock positions at each trading point in order to reduce the turnover rate of strategy so as reduce transaction costs. The model should provide a hyperparameter for users to tune the balance between neutralization (alpha-oriented strategy) and absolute return (beta-oriented strategy), according to the design of the portfolio style and the demand of customers. 6.7.2. Spatiotemporal Graph for Quant Although a uniﬁed model aims to combine the advantage of cross-sectional modeling and time-series modeling, the rela- tionships (or interactions) among stocks are hard to be incorpo- rated in it. Spatiotemporal graph modeling could help comple- ment this part of the information. In particular, a spatiotemporal graph [327, 328] can be embedded as a vector of latent factors using graph convolution or graph attention representation learn- ing techniques, the latent vector could be concatenated with the factor vector used in uniﬁed modeling to improve the prediction performance. 6.8. Universal Modeling As data volume and computing power is growing rapidly, large-scale pretrained model has become one of the mainstream AI paradigms in practice. Successful examples such as BERT [329], GPT-3 [316], CLIP [330], Codex [331] and DALL-E [332] have demonstrated the e ectiveness of large-scale pre- trained models on a number of application domains such as ma- chine translation, multi-modal understanding, creative content generation and so on. Many successful pretraining models have exhibited their universal power to help improve many down- stream tasks with di erent tasks and data sources. We expect this phenomenon could be reproduced in quant applications. 6.8.1. Pretraining-Funetuning Paradigm The pretraining-ﬁnetuning paradigm in AI has demonstrated its success in natural language processing [316] and computer vision [333, 334]. By pretraining a large model on as much datain a self-supervised manner, one can obtain a model extract- ing the common information across various tasks and use it to achieve superior performances on a series of downstream tasks, compared to task-speciﬁc training. We think the pretraining- ﬁnetuning paradigm may be transferred to quant scenarios due to a number of reasons. Many ﬁnance prediction tasks have su \u000eciently large volumes of data with diversiﬁed types and this helps train a large- scale neural network pretraining model during the pretrain- ing stage. Many di erent quant prediction tasks may share the same pattern and information in modeling, especially for high-frequency trading where trading decisions mainly depends on market trends and traders’ behaviors in the market. A general pre- training model could learn common patterns across di erent markets and even across di erent types of instruments, and these common patterns may help improve the downstream prediction tasks. Pretraining-ﬁnetuning improves the automation of the mod- eling process and saves computational cost since we only need to maintain and update a pretrained main model which is a large neural network and adapt it to solve di erent down- stream tasks by ﬁnetuninng the main model. 6.8.2. Challenge in Quant Pretraining The pretraining-ﬁnetuning paradigm has to face a number of di\u000eculties if it is applied to quant. Investment data have extremely low signal-to-noise ratio, and make the pretraining process di \u000ecult to converge to a satis- fying solution. The design of the quant pretraining process must be careful to avoid using future information. Therefore, the masks in self-supervised pretraining or labels in supervised pretraining can be allocated only on the right-hand side like what GPT-3 does. Pretraining should retain both ﬂexibility and versatility. Specif- ically, we should ﬁgure out how to deﬁne appropriate labels or how to set appropriate masking structures. 6.9. Robust Modeling Data noise is the biggest issue in quant modeling. It leads to three problems that negatively a ect the robustness and cor- rectness of our model. 1. The signal-to-noise ratio in ﬁnancial data is extremely low. For example, empirically, the information coe \u000ecient of an e ective stock cross-sectional alpha model for daily trading lies around the level of 0.1, indicating a signal-to-noise ratio at 10% level, and thus the low signal-to-noise ratio makes it more di \u000ecult for machine learning algorithms to tell true patterns out of false positive noise when the sample size is not su \u000eciently large. 2. It is di \u000ecult to correctly and robustly model the distribution of data with heavy noise. For example, the performance of many machine learning models is sensitive to model con- ﬁgurations as well as model hyperparameters when they are 38\n[BEG] [MASK] [SEP] [MASK] … … … …Sequence Model[BEG] [MASK] [SEP] [MASK] … … …Label LabelPretrain: Masked Time-series Modeling Cross-market Data United States Hong Kong London China JapanMulti-frequency Data Daily Hour Minute Second TickLarge-scale pretraining dataset Sequence Model [BEG]Finetune: Time-series forecas�ng Downstream-task data[MASK] ……Decoder Stock prediction Cross-sectional Arbitrage Figure 38: Pretrain-ﬁnetune paradigm. Figure is adapted from [329, 335]. trained on very noisy ﬁnancial data. This makes models very sensitive to sample outliers, time-series change points and the option of parameters, and thus increases the risk of over- ﬁtting. 3. The noisy ﬁnancial data do not follow an independent and identical distribution (i.i.d.) condition which is considered as the underlying assumption of most machine learning al- gorithms. Since market style is constantly changing as a consequence of an enormous number of factors, it is hard to ﬁnd a model that persistently generates e ective trading signals forever. Such noisy nature of ﬁnancial data hinders the e ectiveness of transferring existing algorithms directly, requiring both theoretical and practical innovations in the study of AI technology. To address these problems, we suggest researchers considering the following directions in the future. Causal e ect modeling [336] can be applied to explore the causal e ect between factors and ﬁnancial decisions. By removing the interference from confounding variables, we have the chance to approximate the causal e ect by esti- mating an underlying stable relationship between input fac- tors and output decisions, and reduce the uncertainty of the model. Moreover, causal e ect learning techniques could be applied to machine learning modeling in order to improve their out-of-distribution generalization. Continual learning techniques [309] aim to cumulatively ac- quire new information from data without forgetting the knowl- edge obtained from previous tasks. They could be used to improve out-of-distribution generalization [337] and thus in- crease model robustness through iteratively retraining with accumulated data over time and frequently updating the pre- diction model. Model ensemble technique is useful in many scenarios to im- prove prediction stability by combining multiple single mod- els, especially when single models have su \u000eciently di eren- tiable outputs. Classic ensemble methods include bootstrapaggregating (bagging) [338], boosting [339, 340], stacking [341, 342], Bayesian model averaging [343], etc. Model diversiﬁcation for ensemble expands the diversity of single models prepared to be combined, and the robustness of the ensemble model could beneﬁt from diversiﬁcation among single models. Various techniques could be applied to in- crease model diversity and discrepancy, such as model ran- domization (bootstrap, permutation, dropout [344], etc.), model neutralization, model decorrelation, mixture-of-experts [138], etc. Incorporating diversiﬁed data from di erent sources and dif- ferent patterns helps improve model robustness as well. For example, models trained from fundamental data extracted from ﬁnancial statements usually complement well with mod- els trained from stock prices and volumes, and a combination of the two types of models may help improve stability. 6.10. End-to-end Modeling As we have introduced in §2.1, the traditional quant re- search pipeline consists of a number of steps (as shown in the blue blocks in Figure 39), and each step has its own optimiza- tion direction. For example, the factor mining module searches “good” factors aiming to ﬁnd e ective alpha signals with sig- niﬁcant single-factor IC or back-test proﬁt-and-loss ratio. The modeling module trains machine learning models aiming to min- imize some sort of loss functions that measure the di erence between labels and prediction outcomes. The portfolio opti- mization module allocates assets with optimal positions aim- ing to maximize some sort of value-at-risk (VaR) target. And the trading execution module computes the optimal order size in real-time aiming to minimize the market impact and reduce transaction costs. We can see the optimization goals of these steps are somewhat di erent from each other. For example, a “good” factor with high IC doesn’t necessarily contribute pos- itively to the ﬁnal model output in a complicated nonlinear re- lationship. Therefore, it is natural to consider if there exists an end-to-end model taking meta factors as input and trading 39\norders as output, and if its performance has an advantage com- pared with traditional separate modeling. Meta FactorPrediction ModelingPosition Optimization End-to-End Consistent Modeling: Trading Order = f(Meta Factor)Order Optimization Trading ExecutionFactor Mining Raw Data Figure 39: Comparison of traditional quant research pipeline and end-to-end consistent modeling. 6.10.1. End-to-end Consistent Optimization It is a di \u000ecult task to build a consistently optimal model due to technical reasons. Firstly, end-to-end modeling is usually hard to be formulated as a typical supervised learning problem because there is no clear label deﬁnition for the whole pipeline. In particular, the steps of portfolio optimization and algorithmic trading are dynamic optimization problems relevant to Markov time dependency which are di \u000ecult to be incorporated into a supervised learning framework. Secondly, trading decisions in the order execution step are as frequent as a few seconds, while meta factors are usually updated every couple of minutes or days, and therefore it is di \u000ecult to construct “( x;y) pairs” of samples for machine learning models. Thirdly, the noisy ﬁ- nancial data make it di \u000ecult to train machine learning models di\u000ecult, and in particular, it is easy to be stuck at some local op- timum during the training process, making the model sensitive to disturbance, noise and outliers. Finally, the computational cost of an end-to-end model is extremely high, which redirects to the challenge of computing power discussed in §6.1. The opportunity for solutions to consistent modeling relies on the development of new machine learning paradigms satis- fying the following requirements. 1. The machine learning model should have a hierarchical struc- ture supporting data and decisions at various time granulari- ties including millisecond-level, second-level, minute-level, day-level, week-level, or even month-level. 2. The computational complexity should be controlled to ﬁnish the computation in an a ordable time. 3. The optimization direction of the model should depend on predeﬁned labels if available, and should depend on the ef- fect of trading executions measured by typical evaluation criterion for quant. Based upon the above analysis, we would recommend researchers interested in the problem to think from an existing baseline model under the reinforcement learning framework [73, 74], and pay more attention to fusing data, factors, decisions and executions in multiple time granularities. 6.10.2. Learning Unstructured Data Another problem in end-to-end learning is how to model un- structured data automatically. Many raw ﬁnancial data are un- structured in format, says, they are impossible or inappropriateto be formatted as matrices, tensors, or data frames. Examples include order volume distributions on various bid /ask prices from limit order book data, interactions or causal relationships from economic behavior and event data, and investor emotion from news text data. How to e ectively train a deep learning quant model end-to-end with a mixture of structured and un- structured ﬁnancial data is still an open problem to us. New representation learning techniques (especially new information extraction and embedding methods) for unstructured data are needed for future investment research. 7. Conclusion and Perspective In this article, we proposed the concept of Quant 4.0 which describes what the next-generation quant looks like. We think AI technology will become the core of quant research in the future. In particular, we claim that Automated AI, explain- able AI, and knowledge-driven AI are three key components in Quant 4.0, and we think the development of these research areas will not only drive the evolution of quant research but also promote the progress of next-generation AI technology. We emphasize the importance of engineering in Quant 4.0. All three key components can not be implemented at scale without excellent system architecture and powerful computation infras- tructure. Furthermore, we summarize 10 main challenges in quant technology, including one challenge about infrastructure, two challenges about ﬁnancial data, three challenges about AGI technology and quant application, and four challenges about AI quant modeling. We must emphasize that Quant 4.0 is a dynamic concept and it will evolve and improve itself with the emergence of more and more new technology in the future. We encourage all re- searchers from related research areas to pay more attention to this interdisciplinary ﬁeld which may become one of the de- mands driving the development of next-generation AI technol- ogy. To be honest, there is still a long way to go before achiev- ing an ideal Quant 4.0 level since a lot of technical challenges in both artiﬁcial intelligence and quant engineering need to be solved. However, the rapid growth of AI technology in the past decade always moves beyond our expectations and brings us ex- citing progress, brand-new ideas, and more importantly, more conﬁdence to explore this direction. Finally, we hope this per- spective article could provide some insights to both academia and industry and encourage more researchers to study and con- tribute to this interdisciplinary ﬁeld. Acknowledgement This work would not have been possible without the sup- port of International Digital Economy Academy (IDEA). We would like to thank Prof. Qi Liu at Hong Kong University who provides helpful suggestions for the automated AI part of this article, thank Mr. Hang Yuan at IDEA Research for advice on the engineering part and thank Dr. Zhouchi Lin at IDEA Re- search for advice on the introduction part. 40\nReferences [1] A. Zakrzewski, B. Bacchetti, K. Burchardi, D. Frankle, H. Andrew, M. Kahlich, D. Kessler, S. Knobel, S. Kumar, H. Montgomery, E. Palmisani, O. Shipton, A. Soysal, J. Tan, T. Tang, Global Wealth 2022: Standing Still Is Not an Option, Tech. rep., Boston Consulting Group (Jun. 2022). URL https://www :bcg:com/publications/2022/standing- still-not-an-option [2] F. J. Fabozzi, The handbook of ﬁnancial instruments, The Frank J. Fabozzi series., Wiley, Hoboken, N.J, 2002. [3] P. Blakey, An introduction to investment engineering, IEEE Microwave Magazine 6 (2) (2005) 16–26, conference Name: IEEE Microwave Mag- azine. doi:10:1109/MMW:2005:1491247 . [4] From the Margins to the Mainstream: Assessment of the Impact Invest- ment Sector and Opportunities to Engage Mainstream Investors (2013). URL http://wef :ch/1WYVdeG [5] E. Cheng, Just 10% of trading is regular stock picking, JPMorgan estimates, CNBC (Jun. 2017). URL https://www :cnbc:com/2017/06/13/death-of-the- human-investor-just-10-percent-of-trading-is- regular-stock-picking-jpmorgan-estimates :html [6] J. C. Hull, Options, futures, and other derivatives, 6th Edition, Pearson Prentice Hall, Upper Saddle River, NJ [u.a.], 2006. URL http://gso :gbv:de/DB=2:1/CMD?ACT=SRCHA&SRT=YOP&IKT= 1016&TRM=ppn+563580607&sourceid=fbw bibsonomy [7] X. Guo, T. L. Lai, H. Shek, S. Po-Shing Wong, Quantitative Trading: Algorithms, Analytics, Data, Models, Optimization, CRC Press LLC, Boca Raton, 2016. doi:10:1201/9781315371580 . [8] L. K. C. Chan, N. Jegadeesh, J. Lakonishok, Momentum Strategies, The Journal of Finance 51 (5) (1996) 1681–1713, publisher: [American Fi- nance Association, Wiley]. doi:10:2307/2329534 . URL http://www :jstor:org/stable/2329534 [9] J. M. Poterba, L. H. Summers, Mean reversion in stock prices: Evidence and Implications, Journal of Financial Economics 22 (1) (1988) 27–59. doi:10:1016/0304-405X(88)90021-9 . URL https://www :sciencedirect :com/science/articleπi/ 0304405X88900219 [10] A. Pole, Statistical arbitrage : algorithmic trading insights and tech- niques, Wiley ﬁnance., John Wiley & Sons, Hoboken, N.J, 2007, publi- cation Title: Statistical arbitrage : algorithmic trading insights and tech- niques. [11] J. D. Koziol, Hedging : principles, practices, and strategies for the ﬁnan- cial markets, Wiley, New York, 1990. [12] A. J. Baird, Option market making: trading and risk analysis for the ﬁnancial and commodity option markets, Wiley ﬁnance editions., Wiley, New York, 1993. [13] M. D. Gould, M. A. Porter, S. Williams, M. McDonald, D. J. Fenn, S. D. Howison, Limit order books, Quantita- tive Finance 13 (11) (2013) 1709–1742, publisher: Rout- ledge eprint: https: //doi.org /10.1080 /14697688.2013.803148. doi:10:1080/14697688 :2013:803148 . URL https://doi :org/10:1080/14697688 :2013:803148 [14] E. F. Fama, K. R. French, The Cross-Section of Expected Stock Returns, The Journal of Finance 47 (2) (1992) 427–465, eprint: https: //onlinelibrary.wiley.com /doi/pdf/10.1111 /j.1540- 6261.1992.tb04398.x. doi:10:1111/j:1540-6261 :1992:tb04398:x. URL https://onlinelibrary :wiley:com/doi/abs/10 :1111/ j:1540-6261 :1992:tb04398:x [15] R. C. Grinold, The fundamental law of active management, The Journal of Portfolio Management 15 (3) (1989) 30–37, publisher: Institutional Investor Journals Umbrella Section: Primary Article. doi:10:3905/ jpm:1989:409211 . URL https://jpm :pm-research :com/content/15/3/30 [16] Central Limit Theorem, in: The Concise Encyclopedia of Statistics, Springer, New York, NY , 2008, pp. 66–68. doi:10:1007/978-0-387- 32833-1 50. URL https://doi :org/10:1007/978-0-387-32833-1 50 [17] L. Bachelier, Th ´eorie de la Sp ´eculation, Annales Scientiﬁques de L’Ecole Normale Sup ´erieure 17 (1900) 21–88. [18] E. Thorp, S. Kassouf, Beat the Market: A Scientiﬁc Stock Market Sys- tem, Random House, 1967.[19] A. Meucci, ’P’ Versus ’Q’: Di erences and Commonalities between the Two Areas of Quantitative Finance (Jan. 2011). URL https://papers :ssrn:com/abstract=1717163 [20] P. A. Samuelson, Proof That Properly Anticipated Prices Fluctuate Ran- domly, IMR 6 (2) (1965) 41, num Pages: 9 Place: Cambridge, United States Publisher: Massachusetts Institute of Technology, Cambridge, MA. URL http://www :proquest:com/docview/214192447/ citation/821F08A656E463EPQ/3 [21] P. A. Samuelson, Lifetime portfolio selection by dynamic stochastic programming, in: W. ZIEMBA, R. VICKSON (Eds.), Stochastic Op- timization Models in Finance, Academic Press, 1975, pp. 517–524. doi:https://doi :org/10:1016/B978-0-12-780850-5 :50044-7 . URL https://www :sciencedirect :com/science/articleπi/ B9780127808505500447 [22] R. C. Merton, Lifetime Portfolio Selection under Uncertainty: The Continuous-Time Case, The Review of Economics and Statistics 51 (3) (1969) 247–257, publisher: The MIT Press. doi:10:2307/1926560 . URL http://www :jstor:org/stable/1926560 [23] N. Taleb, Dynamic hedging: managing vanilla and exotic options, Wiley series in ﬁnancial engineering, Wiley, New York, 1997. [24] F. Black, M. Scholes, The Pricing of Options and Corporate Liabilities, Journal of Political Economy 81 (3) (1973) 637–654, publisher: Univer- sity of Chicago Press. URL https://www :jstor:org/stable/1831029 [25] S. Heston, A closed-form solution for options with stochastic volatility with applications to bond and currency options, Review of Financial Studies 6 (1993) 327–343. [26] P. S. Hagan, D. Kumar, A. Lesniewski, D. E. Woodward, Managing smile risk, Wilmott Magazine September (2002) 84–108. [27] H. Buehler, L. Gonon, J. Teichmann, B. Wood, Deep hedging, Quantitative Finance 19 (8) (2019) 1271–1291, publisher: Rout- ledge eprint: https: //doi.org /10.1080 /14697688.2019.1571683. doi: 10:1080/14697688 :2019:1571683 . URL https://doi :org/10:1080/14697688 :2019:1571683 [28] J. M. Harrison, S. R. Pliska, Martingales and stochastic integrals in the theory of continuous trading, Stochastic Processes and their Applica- tions 11 (3) (1981) 215–260. doi:10:1016/0304-4149(81)90026-0 . URL https://www :sciencedirect :com/science/articleπi/ 0304414981900260 [29] D. X. Li, On Default Correlation: A Copula Function Approach, The Journal of Fixed Income 9 (4) (2000) 43–54, publisher: Institutional Investor Journals Umbrella Section: Primary Article. doi:10:3905/ jfi:2000:319253 . URL https://jfi :pm-research :com/content/9/4/43 [30] J. Berk, P. DeMarzo, Corporate Finance, 3rd Edition, Pearson, Boston, 2013. [31] H. Markowitz, Portfolio Selection, The Journal of Finance 7 (1) (1952) 77–91, publisher: [American Finance Association, Wiley]. doi: 10:2307/2975974 . URL http://www :jstor:org/stable/2975974 [32] H. M. Markowitz, Portfolio Selection: E \u000ecient Diversiﬁcation of In- vestments, Yale University Press, 1959. URL http://www :jstor:org/stable/j :ctt1bh4c8h [33] J. L. Treynor, Market Value, Time, and Risk (Aug. 1961). doi: 10:2139/ssrn :2600356 . URL https://papers :ssrn:com/abstract=2600356 [34] W. F. Sharpe, Capital Asset Prices: A Theory of Market Equilibrium Under Conditions of Risk*, The Journal of Finance 19 (3) (1964) 425– 442, eprint: https: //onlinelibrary.wiley.com /doi/pdf/10.1111 /j.1540- 6261.1964.tb02865.x. doi:10:1111/j:1540-6261 :1964:tb02865:x. URL http://onlinelibrary :wiley:com/doi/abs/10 :1111/ j:1540-6261 :1964:tb02865:x [35] J. Lintner, The Valuation of Risk Assets and the Selection of Risky In- vestments in Stock Portfolios and Capital Budgets, The Review of Eco- nomics and Statistics 47 (1) (1965) 13–37, publisher: The MIT Press. doi:10:2307/1924119 . URL http://www :jstor:org/stable/1924119 [36] J. Mossin, Equilibrium in a Capital Asset Market, Econometrica 34 (4) (1966) 768–783, publisher: [Wiley, Econometric Society]. doi: 10:2307/1910098 . 41\nURL http://www :jstor:org/stable/1910098 [37] S. A. Ross, The arbitrage theory of capital asset pricing, Journal of Economic Theory 13 (3) (1976) 341–360. doi:10:1016/0022- 0531(76)90046-6 . URL https://www :sciencedirect :com/science/articleπi/ 0022053176900466 [38] E. F. Fama, K. R. French, A ﬁve-factor asset pricing model, Journal of Financial Economics 116 (1) (2015) 1–22. doi:https://doi :org/ 10:1016/j:jfineco:2014:10:010. URL https://www :sciencedirect :com/science/articleπi/ S0304405X14002323 [39] M. Eichler, Causal Inference in Time Series Analysis, in: Causality, John Wiley & Sons, Ltd, 2012, pp. 327–354, section: 22 eprint: https: //onlinelibrary.wiley.com /doi/pdf/10.1002 /9781119945710.ch22. doi:10:1002/9781119945710 :ch22 . URL http://onlinelibrary :wiley:com/doi/abs/10 :1002/ 9781119945710 :ch22 [40] C. W. J. Granger, Investigating Causal Relations by Econometric Mod- els and Cross-spectral Methods, Econometrica 37 (3) (1969) 424–438, publisher: [Wiley, Econometric Society]. doi:10:2307/1912791 . URL http://www :jstor:org/stable/1912791 [41] I. Tulchinsky, Introduction to Alpha Design, in: Finding Al- phas, John Wiley & Sons, Ltd, 2019, pp. 1–6, section: 1 eprint: https: //onlinelibrary.wiley.com /doi/pdf/10.1002 /9781119571278.ch1. doi:https://doi :org/10:1002/9781119571278 :ch1. URL https://onlinelibrary :wiley:com/doi/abs/10 :1002/ 9781119571278 :ch1 [42] X. He, K. Zhao, X. Chu, AutoML: A Survey of the State-of-the-Art, Knowledge-Based Systems 212 (2021) 106622, arXiv: 1908.00709. doi:10:1016/j:knosys:2020:106622 . URL http://arxiv :org/abs/1908 :00709 [43] J. Adams, D. Hayunga, S. Mansi, D. Reeb, V . Ve- rardi, Identifying and treating outliers in ﬁnance, Fi- nancial Management 48 (2) (2019) 345–384, eprint: https: //onlinelibrary.wiley.com /doi/pdf/10.1111 /ﬁma.12269. doi:10:1111/fima :12269 . URL http://onlinelibrary :wiley:com/doi/abs/10 :1111/ fima:12269 [44] A. Zheng, A. Casari, Feature Engineering for Machine Learning: Prin- ciples and Techniques for Data Scientists, 1st Edition, O’Reilly Media, Inc., 2018. [45] M. Schnaubelt, A comparison of machine learning model validation schemes for non-stationary time series data, Working Paper 11 /2019, FAU Discussion Papers in Economics (2019). URL https://www :econstor:eu/handle/10419/209136 [46] Y . Nevmyvaka, Y . Feng, M. Kearns, Reinforcement learning for opti- mized trade execution, in: Proceedings of the 23rd international con- ference on Machine learning, ICML ’06, Association for Computing Machinery, New York, NY , USA, 2006, pp. 673–680. doi:10:1145/ 1143844:1143929 . URL http://doi :org/10:1145/1143844 :1143929 [47] T. Coleman, A Practical Guide to Risk Management (Jul. 2011). URL https://papers :ssrn:com/abstract=2586032 [48] MSCI, Barra’s risk models (1996). URL https://www :msci:com/www/research-paper/barra-s- risk-models/014972229 [49] F. Black, Noise, The Journal of Finance 41 (3) (1986) 528–543, eprint: https: //onlinelibrary.wiley.com /doi/pdf/10.1111 /j.1540- 6261.1986.tb04513.x. doi:10:1111/j:1540-6261 :1986:tb04513:x. URL https://onlinelibrary :wiley:com/doi/abs/10 :1111/ j:1540-6261 :1986:tb04513:x [50] Y . A ¨ıt-Sahalia, J. Yu, High frequency market microstructure noise es- timates and liquidity measures, The Annals of Applied Statistics 3 (1) (2009) 422 – 457, publisher: Institute of Mathematical Statistics. doi: 10:1214/08-AOAS200 . URL https://doi :org/10:1214/08-AOAS200 [51] C. Mancini, Measuring the relevance of the microstructure noise in ﬁ- nancial data, Stochastic Processes and their Applications 123 (7) (2013) 2728–2751. doi:https://doi :org/10:1016/j:spa:2013:04:003. URL https://www :sciencedirect :com/science/articleπi/ S0304414913000951[52] Z. Kakushadze, 101 Formulaic Alphas, arXiv:1601.00991 [q-ﬁn]ArXiv: 1601.00991 (Mar. 2016). URL http://arxiv :org/abs/1601 :00991 [53] F. Feng, X. He, X. Wang, C. Luo, Y . Liu, T.-S. Chua, Temporal Re- lational Ranking for Stock Prediction, ACM Transactions on Informa- tion Systems 37 (2) (2019) 1–30, arXiv: 1809.09441. doi:10:1145/ 3309547 . URL http://arxiv :org/abs/1809 :09441 [54] R. Sawhney, S. Agarwal, A. Wadhwa, R. R. Shah, Spatiotemporal Hy- pergraph Convolution Network for Stock Movement Forecasting, in: 2020 IEEE International Conference on Data Mining (ICDM), 2020, pp. 482–491, iSSN: 2374-8486. doi:10:1109/ICDM50108 :2020:00057 . [55] Y . Wu, D. Magazzeni, M. Veloso, How Robust are Limit Order Book Representations under Data Perturbation?, in: ICML Workshop on Rep- resentation Learning for Finance and E-Commerce Applications, 2021. [56] W. La Cava, P. Orzechowski, B. Burlacu, F. de Franca, M. Virgolin, Y . Jin, M. Kommenda, J. Moore, Contemporary Symbolic Regression Methods and their Relative Performance, in: J. Vanschoren, S. Yeung (Eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, V ol. 1, 2021. URL https://datasets-benchmarks- proceedings :neurips:cc/paper/2021/file/ c0c7c76d30bd3dcaefc96f40275bdc0a-Paper-round1 :pdf [57] A. Rashid, M. Fayyaz, M. Karim, Investor sentiment, momentum, and stock returns: an examination for direct and indirect e ects, Economic Research-Ekonomska Istra ˇzivanja 32 (1) (2019) 2638–2656, publisher: Routledge eprint: https: //doi.org /10.1080 /1331677X.2019.1650652. doi:10:1080/1331677X :2019:1650652 . URL https://doi :org/10:1080/1331677X :2019:1650652 [58] Z. Abdul Karim, F. S. R. Muhamad Fahmi, B. Abdul Karim, M. A. Shokr, Market sentiments and ﬁrm-level equity returns: panel evidence of Malaysia, Economic Research-Ekonomska Istraˇzivanja 35 (1) (2022) 5253–5272, publisher: Rout- ledge eprint: https: //doi.org /10.1080 /1331677X.2021.2025126. doi:10:1080/1331677X :2021:2025126 . URL https://doi :org/10:1080/1331677X :2021:2025126 [59] C. Andrieu, N. de Freitas, A. Doucet, M. I. Jordan, An Introduction to MCMC for Machine Learning, Machine Learning 50 (1) (2003) 5–43. doi:10:1023/A:1020281327116 . URL https://doi :org/10:1023/A:1020281327116 [60] Y . Jin, W. Fu, J. Kang, J. Guo, J. Guo, Bayesian Symbolic Re- gression, arXiv:1910.08892 [stat] (Jan. 2020). doi:10:48550/ arXiv:1910:08892 . URL http://arxiv :org/abs/1910 :08892 [61] T. Chen, W. Chen, L. Du, An Empirical Study of Financial Factor Mining Based on Gene Expression Programming, in: 2021 4th In- ternational Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE), 2021, pp. 1113–1117. doi: 10:1109/AEMCSE51986 :2021:00228 . [62] L. Biggio, T. Bendinelli, A. Neitz, A. Lucchi, G. Parascandolo, Neural Symbolic Regression that scales, in: M. Meila, T. Zhang (Eds.), Pro- ceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, V ol. 139 of Proceedings of Machine Learning Research, PMLR, 2021, pp. 936–945. URL http://proceedings :mlr:press/v139/biggio21a :html [63] G. Martius, C. H. Lampert, Extrapolation and learning equations, in: 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings, Open- Review.net, 2017. URL https://openreview :net/forum?id=BkgRp0FYe [64] M. T. Ahvanooey, Q. Li, M. Wu, S. Wang, A Survey of Genetic Pro- gramming and Its Applications, KSII Transactions on Internet and In- formation Systems (TIIS) 13 (4) (2019) 1765–1794, publisher: Korean Society for Internet Information. doi:10:3837/tiis :2019:04:002. URL https://koreascience :kr/article/ JAKO201919761177651 :page [65] S. Katoch, S. S. Chauhan, V . Kumar, A review on genetic algorithm: past, present, and future, Multimedia Tools and Applications 80 (5) (2021) 8091–8126. doi:10:1007/s11042-020-10139-6 . URL https://doi :org/10:1007/s11042-020-10139-6 [66] J. Lee, Y . Lee, J. Kim, A. Kosiorek, S. Choi, Y . W. Teh, Set Trans- 42\nformer: A Framework for Attention-based Permutation-Invariant Neu- ral Networks, in: Proceedings of the 36th International Conference on Machine Learning, PMLR, 2019, pp. 3744–3753, iSSN: 2640-3498. URL https://proceedings :mlr:press/v97/lee19d :html [67] K. Hornik, M. B. Stinchcombe, H. White, Multilayer feedforward net- works are universal approximators, Neural Networks 2 (5) (1989) 359– 366. [68] A. Thakkar, K. Chaudhari, A comprehensive survey on deep neural networks for stock market: The need, challenges, and future direc- tions, Expert Systems with Applications 177 (2021) 114800. doi: 10:1016/j:eswa:2021:114800 . URL https://www :sciencedirect :com/science/articleπi/ S0957417421002414 [69] Z. Hu, Y . Zhao, M. Khushi, A Survey of Forex and Stock Price Predic- tion Using Deep Learning, Applied System Innovation 4 (1) (2021) 9. doi:10:3390/asi4010009 . URL https://www :mdpi:com/2571-5577/4/1/9 [70] W. Jiang, Applications of deep learning in stock market prediction: re- cent progress, Expert Systems with Applications 184 (2021) 115537, arXiv: 2003.01859. doi:10:1016/j:eswa:2021:115537 . URL http://arxiv :org/abs/2003 :01859 [71] O. B. Sezer, M. U. Gudelek, A. M. Ozbayoglu, Financial time se- ries forecasting with deep learning : A systematic literature review: 2005–2019, Applied Soft Computing 90 (2020) 106181. doi:10:1016/ j:asoc:2020:106181 . URL https://www :sciencedirect :com/science/articleπi/ S1568494620301216 [72] I. Sutskever, O. Vinyals, Q. V . Le, Sequence to Sequence Learning with Neural Networks, in: Advances in Neural Information Processing Systems, V ol. 27, Curran Associates, Inc., 2014. URL https://proceedings :neurips:cc/paper/2014/hash/ a14ac55a4f27472c5d894ec1c3c743d2-Abstract :html [73] J. Wang, Y . Zhang, K. Tang, J. Wu, Z. Xiong, AlphaStock: A Buying-Winners-and-Selling-Losers Investment Strategy using Inter- pretable Deep Reinforcement Attention Networks, in: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Dis- covery & Data Mining, ACM, Anchorage AK USA, 2019, pp. 1900– 1908. doi:10:1145/3292500 :3330647 . URL https://dl :acm:org/doi/10 :1145/3292500 :3330647 [74] Z. Wang, B. Huang, S. Tu, K. Zhang, L. Xu, DeepTrader: A Deep Reinforcement Learning Approach for Risk-Return Balanced Portfolio Management with Market Conditions Embedding, Proceedings of the AAAI Conference on Artiﬁcial Intelligence 35 (1) (2021) 643–650, number: 1. URL https://ojs :aaai:org∈dex :php/AAAI/article/view/ 16144 [75] T. Elsken, J. H. Metzen, F. Hutter, E \u000ecient Multi-Objective Neural Ar- chitecture Search via Lamarckian Evolution, in: 7th International Con- ference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, OpenReview.net, 2019. URL https://openreview :net/forum?id=ByME42AqK7 [76] M. Zhang, H. Li, S. Pan, X. Chang, C. Zhou, Z. Ge, S. Su, One-Shot Neural Architecture Search: Maximising Diversity to Overcome Catas- trophic Forgetting, IEEE Transactions on Pattern Analysis and Machine Intelligence 43 (9) (2021) 2921–2935, conference Name: IEEE Trans- actions on Pattern Analysis and Machine Intelligence. doi:10:1109/ TPAMI:2020:3035351 . [77] M.-A. Z ¨oller, M. F. Huber, Benchmark and Survey of Automated Ma- chine Learning Frameworks, Journal of Artiﬁcial Intelligence Research 70 (2021) 409–472. doi:10:1613/jair :1:11854 . URL http://doi :org/10:1613/jair :1:11854 [78] H. Mendoza, A. Klein, M. Feurer, J. T. Springenberg, F. Hutter, Towards Automatically-Tuned Neural Networks, in: Proceedings of the Workshop on Automatic Machine Learning, PMLR, 2016, pp. 58–65, iSSN: 1938-7228. URL https://proceedings :mlr:press/v64/ mendoza towards 2016:html [79] B. Baker, O. Gupta, R. Raskar, N. Naik, Accelerating Neural Architec- ture Search using Performance Prediction, in: 6th International Confer- ence on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings, OpenReview.net,2018. URL https://openreview :net/forum?id=HJqk3N1vG [80] T. Elsken, J. H. Metzen, F. Hutter, Neural architecture search: a survey, The Journal of Machine Learning Research 20 (1) (2019) 1997–2017. [81] B. Zoph, V . Vasudevan, J. Shlens, Q. V . Le, Learning Transferable Ar- chitectures for Scalable Image Recognition, in: 2018 IEEE /CVF Con- ference on Computer Vision and Pattern Recognition, 2018, pp. 8697– 8710, iSSN: 2575-7075. doi:10:1109/CVPR :2018:00907 . [82] Z. Zhong, Z. Yang, B. Deng, J. Yan, W. Wu, J. Shao, C.-L. Liu, BlockQNN: E \u000ecient Block-Wise Neural Network Architecture Gener- ation, IEEE Trans. Pattern Anal. Mach. Intell. 43 (7) (2021) 2314–2328. doi:10:1109/TPAMI :2020:2969193 . URL https://doi :org/10:1109/TPAMI :2020:2969193 [83] M. Feurer, F. Hutter, Hyperparameter Optimization, in: F. Hutter, L. Kottho , J. Vanschoren (Eds.), Automated Machine Learning: Meth- ods, Systems, Challenges, The Springer Series on Challenges in Ma- chine Learning, Springer International Publishing, Cham, 2019, pp. 3– 33.doi:10:1007/978-3-030-05318-5 1. URL https://doi :org/10:1007/978-3-030-05318-5 1 [84] J. Bergstra, Y . Bengio, Random Search for Hyper-Parameter Optimiza- tion, Journal of Machine Learning Research 13 (10) (2012) 281–305. URL http://jmlr :org/papers/v13/bergstra12a :html [85] L. Li, A. Talwalkar, Random Search and Reproducibility for Neural Ar- chitecture Search, in: Proceedings of The 35th Uncertainty in Artiﬁcial Intelligence Conference, PMLR, 2020, pp. 367–377, iSSN: 2640-3498. URL https://proceedings :mlr:press/v115/li20c :html [86] E. Real, S. Moore, A. Selle, S. Saxena, Y . L. Suematsu, J. Tan, Q. V . Le, A. Kurakin, Large-Scale Evolution of Image Classiﬁers, in: Proceed- ings of the 34th International Conference on Machine Learning, PMLR, 2017, pp. 2902–2911, iSSN: 2640-3498. URL https://proceedings :mlr:press/v70/real17a :html [87] L. Xie, A. Yuille, Genetic CNN, in: 2017 IEEE International Conference on Computer Vision (ICCV), 2017, pp. 1388–1397, iSSN: 2380-7504. doi:10:1109/ICCV :2017:154. [88] M. Suganuma, S. Shirakawa, T. Nagao, A Genetic Programming Ap- proach to Designing Convolutional Neural Network Architectures, in: J. Lang (Ed.), Proceedings of the Twenty-Seventh International Joint Conference on Artiﬁcial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, ijcai.org, 2018, pp. 5369–5373. doi:10:24963/ ijcai:2018/755 . [89] E. Real, A. Aggarwal, Y . Huang, Q. V . Le, Regularized Evolution for Image Classiﬁer Architecture Search, Proceedings of the AAAI Confer- ence on Artiﬁcial Intelligence 33 (01) (2019) 4780–4789, number: 01. doi:10:1609/aaai :v33i01:33014780 . URL https://ojs :aaai:org∈dex :php/AAAI/article/view/ 4405 [90] L. Tani, D. Rand, C. Veelken, M. Kadastik, Evolutionary algorithms for hyperparameter optimization in machine learning for application in high energy physics, The European Physical Journal C 81 (2) (2021) 170. doi:10:1140/epjc/s10052-021-08950-y . URL https://doi :org/10:1140/epjc/s10052-021-08950-y [91] B. Zoph, Q. V . Le, Neural architecture search with reinforcement learn- ing, arXiv preprint arXiv:1611.01578 (2016). [92] H. S. Jomaa, J. Grabocka, L. Schmidt-Thieme, Hyp-RL : Hyperparam- eter Optimization by Reinforcement Learning, arXiv:1906.11527 [cs, stat] (Jun. 2019). doi:10:48550/arXiv :1906:11527 . URL http://arxiv :org/abs/1906 :11527 [93] S. Falkner, A. Klein, F. Hutter, BOHB: Robust and E \u000ecient Hyper- parameter Optimization at Scale, in: J. G. Dy, A. Krause (Eds.), Pro- ceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm ¨assan, Stockholm, Sweden, July 10-15, 2018, V ol. 80 of Proceedings of Machine Learning Research, PMLR, 2018, pp. 1436–1445. URL http://proceedings :mlr:press/v80/falkner18a :html [94] A. Klein, S. Falkner, S. Bartels, P. Hennig, F. Hutter, Fast Bayesian Op- timization of Machine Learning Hyperparameters on Large Datasets, in: Proceedings of the 20th International Conference on Artiﬁcial Intelli- gence and Statistics, PMLR, 2017, pp. 528–536, iSSN: 2640-3498. URL https://proceedings :mlr:press/v54/klein17a :html [95] C. White, W. Neiswanger, Y . Savani, BANANAS: Bayesian Opti- mization with Neural Architectures for Neural Architecture Search, 43\nin: Thirty-Fifth AAAI Conference on Artiﬁcial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artiﬁcial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, AAAI Press, 2021, pp. 10293–10301. URL https://ojs :aaai:org∈dex :php/AAAI/article/view/ 17233 [96] K. Kandasamy, W. Neiswanger, J. Schneider, B. Poczos, E. P. Xing, Neural Architecture Search with Bayesian Optimisation and Optimal Transport, in: Advances in Neural Information Processing Systems, V ol. 31, Curran Associates, Inc., 2018. URL https://proceedings :neurips:cc/paper/2018/hash/ f33ba15effa5c10e873bf3842afb46a6-Abstract :html [97] H. Liu, K. Simonyan, Y . Yang, DARTS: Di erentiable Architecture Search, in: 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, OpenReview.net, 2019. URL https://openreview :net/forum?id=S1eYHoC5FX [98] H. Cai, L. Zhu, S. Han, ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware, in: 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, OpenReview.net, 2019. URL https://openreview :net/forum?id=HylVB3AqYm [99] Y . Bengio, Gradient-Based Optimization of Hyperparameters, Neural Computation 12 (8) (2000) 1889–1900, conference Name: Neural Com- putation. doi:10:1162/089976600300015187 . [100] D. Maclaurin, D. Duvenaud, R. P. Adams, Gradient-based hyperparam- eter optimization through reversible learning, in: Proceedings of the 32nd International Conference on International Conference on Machine Learning - V olume 37, ICML’15, JMLR.org, Lille, France, 2015, pp. 2113–2122. [101] L. Franceschi, M. Donini, P. Frasconi, M. Pontil, Forward and Reverse Gradient-Based Hyperparameter Optimization, in: Proceedings of the 34th International Conference on Machine Learning, PMLR, 2017, pp. 1165–1173, iSSN: 2640-3498. URL https://proceedings :mlr:press/v70/ franceschi17a :html [102] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei- Fei, A. L. Yuille, J. Huang, K. Murphy, Progressive Neural Archi- tecture Search, in: V . Ferrari, M. Hebert, C. Sminchisescu, Y . Weiss (Eds.), Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part I, V ol. 11205 of Lecture Notes in Computer Science, Springer, 2018, pp. 19– 35.doi:10:1007/978-3-030-01246-5 2. URL https://doi :org/10:1007/978-3-030-01246-5 2 [103] T. Wei, C. Wang, Y . Rui, C. W. Chen, Network Morphism, in: Proceed- ings of The 33rd International Conference on Machine Learning, PMLR, 2016, pp. 564–572, iSSN: 1938-7228. URL https://proceedings :mlr:press/v48/wei16 :html [104] M. Li, Y . Liu, X. Liu, Q. Sun, X. You, H. Yang, Z. Luan, L. Gan, G. Yang, D. Qian, The Deep Learning Compiler: A Comprehensive Survey, IEEE Transactions on Parallel and Distributed Systems 32 (3) (2021) 708–727, conference Name: IEEE Transactions on Parallel and Distributed Systems. doi:10:1109/TPDS :2020:3030548 . [105] Y . Cheng, D. Wang, P. Zhou, T. Zhang, Model Compression and Ac- celeration for Deep Neural Networks: The Principles, Progress, and Challenges, IEEE Signal Processing Magazine 35 (1) (2018) 126–136, conference Name: IEEE Signal Processing Magazine. doi:10:1109/ MSP:2017:2765695 . [106] T. Choudhary, V . Mishra, A. Goswami, J. Sarangapani, A comprehen- sive survey on model compression and acceleration, Artiﬁcial Intelli- gence Review 53 (7) (2020) 5113–5155. doi:10:1007/s10462-020- 09816-7 . URL https://doi :org/10:1007/s10462-020-09816-7 [107] T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Q. Yan, H. Shen, M. Cowan, L. Wang, Y . Hu, L. Ceze, C. Guestrin, A. Krishnamurthy, TVM: An Automated End-to-End Optimizing Compiler for Deep Learning, in: A. C. Arpaci-Dusseau, G. V oelker (Eds.), 13th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2018, Carlsbad, CA, USA, October 8-10, 2018, USENIX Association, 2018, pp. 578–594.URL https://www :usenix:org/conference/osdi18/ presentation/chen [108] C. Lattner, M. Amini, U. Bondhugula, A. Cohen, A. Davis, J. Pienaar, R. Riddle, T. Shpeisman, N. Vasilache, O. Zinenko, MLIR: scaling com- piler infrastructure for domain speciﬁc computation, in: Proceedings of the 2021 IEEE /ACM International Symposium on Code Generation and Optimization, CGO ’21, IEEE Press, Virtual Event, Republic of Korea, 2021, pp. 2–14. doi:10:1109/CGO51591 :2021:9370308 . URL http://doi :org/10:1109/CGO51591 :2021:9370308 [109] S. Cyphers, A. K. Bansal, A. Bhiwandiwalla, J. Bobba, M. Brookhart, A. Chakraborty, W. Constable, C. Convey, L. Cook, O. Kanawi, R. Kim- ball, J. Knight, N. Korovaiko, V . Kumar, Y . Lao, C. R. Lishka, J. Menon, J. Myers, S. A. Narayana, A. Procter, T. J. Webb, Intel nGraph: An Intermediate Representation, Compiler, and Executor for Deep Learning, arXiv:1801.08058 [cs] (Jan. 2018). doi:10:48550/ arXiv:1801:08058 . URL http://arxiv :org/abs/1801 :08058 [110] A. V . Aho, R. Sethi, J. D. Ullman, Compilers, Principles, Techniques, and Tools, Addison-Wesley, 1986. [111] K. D. Cooper, L. T. Simpson, C. A. Vick, Operator strength reduc- tion, ACM Transactions on Programming Languages and Systems 23 (5) (2001) 603–625. doi:10:1145/504709 :504710 . URL http://doi :org/10:1145/504709 :504710 [112] G. A. Kildall, A uniﬁed approach to global program optimization, in: Proceedings of the 1st annual ACM SIGACT-SIGPLAN sympo- sium on Principles of programming languages, POPL ’73, Association for Computing Machinery, New York, NY , USA, 1973, pp. 194–206. doi:10:1145/512927 :512945 . URL https://doi :org/10:1145/512927 :512945 [113] S. P. Vanderwiel, D. J. Lilja, Data prefetch mechanisms, ACM Comput- ing Surveys 32 (2) (2000) 174–199. doi:10:1145/358923 :358939 . URL http://doi :org/10:1145/358923 :358939 [114] V . Sarkar, R. Thekkath, A general framework for iteration-reordering loop transformations, ACM SIGPLAN Notices 27 (7) (1992) 175–187. doi:10:1145/143103 :143132 . URL http://doi :org/10:1145/143103 :143132 [115] V . V olkov, Understanding Latency Hiding on GPUs, Ph.D. thesis, UC Berkeley (2016). URL https://escholarship :org/uc/item/1wb7f3h4 [116] R. Allen, Optimizing compilers for modern architectures: a dependence- based approach, Morgan Kaufmann, San Francisco, Calif, 2002. [117] M. J. Wolfe, High performance compilers for parallel computing, Addison-Wesley, Redwood City, Calif, 1996. [118] R. M. Karp, R. E. Miller, S. Winograd, The Organization of Compu- tations for Uniform Recurrence Equations, Journal of the ACM 14 (3) (1967) 563–590, publisher: ACM. URL http://dl :acm:org/citation :cfm?id=321418 [119] L. Lamport, The Parallel Execution of DO Loops, Communications of the ACM 17 (2) (1974) 83–93, publisher: ACM. URL http://research :microsoft :com/en-us/um/people/ lamport/pubs/do-loops :pdf [120] S. Han, J. Pool, J. Tran, W. Dally, Learning both weights and connec- tions for e \u000ecient neural network, Advances in neural information pro- cessing systems 28 (2015). [121] S. Han, H. Mao, W. J. Dally, Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Hu man Cod- ing, in: Y . Bengio, Y . LeCun (Eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv :org/abs/1510 :00149 [122] G. Hinton, O. Vinyals, J. Dean, Distilling the Knowledge in a Neural Network, arXiv:1503.02531 [cs, stat]ArXiv: 1503.02531 (Mar. 2015). URL http://arxiv :org/abs/1503 :02531 [123] X. Zhang, J. Zou, X. Ming, K. He, J. Sun, E \u000ecient and accurate approxi- mations of nonlinear convolutional networks, in: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1984– 1992, iSSN: 1063-6919. doi:10:1109/CVPR :2015:7298809 . [124] X. Yu, T. Liu, X. Wang, D. Tao, On Compressing Deep Models by Low Rank and Sparse Decomposition, in: 2017 IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), 2017, pp. 67–76, iSSN: 1063-6919. doi:10:1109/CVPR :2017:15. 44\n[125] J. Gou, B. Yu, S. J. Maybank, D. Tao, Knowledge Distillation: A Survey, International Journal of Computer Vision 129 (6) (2021) 1789–1819. doi:10:1007/s11263-021-01453-z . URL https://doi :org/10:1007/s11263-021-01453-z [126] V . Belle, I. Papantonis, Principles and Practice of Explainable Machine Learning, Frontiers in Big Data 4 (2021). URL https://www :frontiersin :org/articles/10 :3389/ fdata:2021:688969 [127] W. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, B. Yu, Deﬁni- tions, methods, and applications in interpretable machine learning, Pro- ceedings of the National Academy of Sciences 116 (44) (2019) 22071– 22080, publisher: Proceedings of the National Academy of Sciences. doi:10:1073/pnas :1900654116 . URL https://www :pnas:org/doi/10 :1073/pnas :1900654116 [128] C. Rudin, Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead, Nature Machine Intelligence 1 (5) (2019) 206–215, number: 5 Publisher: Nature Pub- lishing Group. doi:10:1038/s42256-019-0048-x . URL https://www :nature:com/articles/s42256-019-0048-x [129] M. T. Ribeiro, S. Singh, C. Guestrin, Model-Agnostic Interpretability of Machine Learning, CoRR abs /1606.05386, arXiv: 1606.05386 (2016). URL http://arxiv :org/abs/1606 :05386 [130] S. Towﬁghi, pySRURGS - a python package for symbolic regression by uniform random global search, Journal of Open Source Software 4 (2019) 1675. doi:10:21105/joss :01675 . [131] Y . Meshalkin, A. Shakirov, E. Popov, D. Koroteev, I. Gurbatova, Ro- bust well-log based determination of rock thermal conductivity through machine learning, Geophysical Journal International 222 (May 2020). doi:10:1093/gji/ggaa209 . [132] M.-X. Wang, D. Huang, G. Wang, D.-Q. Li, SS-XGBoost: A Machine Learning Framework for Predicting Newmark Sliding Displacements of Slopes, Journal of Geotechnical and Geoenvironmental Engineering 146 (2020) 04020074. doi:10:1061/(ASCE)GT :1943-5606 :0002297 . [133] J. R. Quinlan, Induction of decision trees, Machine Learning 1 (1) (1986) 81–106. doi:10:1007/BF00116251 . URL https://doi :org/10:1007/BF00116251 [134] L. Breiman, Random Forests, Machine Learning 45 (1) (2001) 5–32. doi:10:1023/A:1010933404324 . URL https://doi :org/10:1023/A:1010933404324 [135] J. H. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics 29 (5) (2001) 1189–1232, publisher: Institute of Mathematical Statistics. URL http://www :jstor:org/stable/2699986 [136] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, T.-Y . Liu, LightGBM: A Highly E \u000ecient Gradient Boosting Decision Tree, in: Advances in Neural Information Processing Systems, V ol. 30, Curran Associates, Inc., 2017. URL https://papers :nips:cc/paper/2017/hash/ 6449f44a102fde848669bdd9eb6b76fa-Abstract :html [137] T. Chen, C. Guestrin, XGBoost: A Scalable Tree Boosting System, in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, Association for Computing Machinery, New York, NY , USA, 2016, pp. 785–794. doi:10:1145/2939672 :2939785 . URL http://doi :org/10:1145/2939672 :2939785 [138] S. Masoudnia, R. Ebrahimpour, Mixture of experts: a literature survey, Artiﬁcial Intelligence Review 42 (2) (2014) 275–293. doi:10:1007/ s10462-012-9338-y . URL https://doi :org/10:1007/s10462-012-9338-y [139] T. Hofmann, B. Sch ¨olkopf, A. J. Smola, Kernel methods in ma- chine learning, The Annals of Statistics 36 (3) (2008) 1171 – 1220, publisher: Institute of Mathematical Statistics. doi:10:1214/ 009053607000000677 . URL https://doi :org/10:1214/009053607000000677 [140] C. Cortes, V . Vapnik, Support-vector networks, Machine Learning 20 (3) (1995) 273–297. doi:10:1007/BF00994018 . URL https://doi :org/10:1007/BF00994018 [141] V . N. Vapnik, Statistical learning theory, Adaptive and learning systems for signal processing, communications, and control., Wiley, New York, 1998. [142] Y .-H. H. Tsai, S. Bai, M. Yamada, L.-P. Morency, R. Salakhutdinov,Transformer Dissection: An Uniﬁed Understanding for Transformer’s Attention via the Lens of Kernel, in: Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics, Hong Kong, China, 2019, pp. 4344–4353. doi:10:18653/v1/D19-1443 . URL https://aclanthology :org/D19-1443 [143] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin, Attention is all you need, in: Proceedings of the 31st International Conference on Neural Information Processing Sys- tems, NIPS’17, Curran Associates Inc., Red Hook, NY , USA, 2017, pp. 6000–6010. [144] L. Rabiner, B. Juang, An introduction to hidden Markov models, IEEE ASSP Magazine 3 (1) (1986) 4–16, conference Name: IEEE ASSP Mag- azine. doi:10:1109/MASSP :1986:1165342 . [145] J. D. La erty, A. McCallum, F. C. N. Pereira, Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data, in: Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2001, pp. 282–289. [146] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural com- putation 9 (8) (1997) 1735–1780. [147] K. Cho, B. van Merri ¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, Y . Bengio, Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation, in: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Pro- cessing (EMNLP), Association for Computational Linguistics, Doha, Qatar, 2014, pp. 1724–1734. doi:10:3115/v1/D14-1179 . URL https://aclanthology :org/D14-1179 [148] I. J. Goodfellow, Y . Bengio, A. Courville, Deep Learning, MIT Press, Cambridge, MA, USA, 2016. [149] P. Kontschieder, M. Fiterau, A. Criminisi, S. R. Bul `o, Deep Neural Decision Forests, in: 2015 IEEE International Conference on Com- puter Vision (ICCV), 2015, pp. 1467–1475, iSSN: 2380-7504. doi: 10:1109/ICCV :2015:172. [150] W. Song, C. Shi, Z. Xiao, Z. Duan, Y . Xu, M. Zhang, J. Tang, Au- toInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks, in: Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM ’19, Association for Computing Machinery, New York, NY , USA, 2019, pp. 1161–1170. doi:10:1145/3357384 :3357925 . URL http://doi :org/10:1145/3357384 :3357925 [151] R. Saleem, B. Yuan, F. Kurugollu, A. Anjum, L. Liu, Explain- ing deep neural networks: A survey on the global interpretation methods, Neurocomputing 513 (2022) 165–180. doi:10:1016/ j:neucom:2022:09:129. URL https://www :sciencedirect :com/science/articleπi/ S0925231222012218 [152] M. T. Ribeiro, S. Singh, C. Guestrin, ”Why Should I Trust You?”: Explaining the Predictions of Any Classiﬁer, in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discov- ery and Data Mining, KDD ’16, Association for Computing Machin- ery, New York, NY , USA, 2016, pp. 1135–1144. doi:10:1145/ 2939672:2939778 . URL http://doi :org/10:1145/2939672 :2939778 [153] M. T. Ribeiro, S. Singh, C. Guestrin, Anchors: High-Precision Model-Agnostic Explanations, Proceedings of the AAAI Confer- ence on Artiﬁcial Intelligence 32 (1), number: 1 (Apr. 2018). doi:10:1609/aaai :v32i1:11491 . URL https://ojs :aaai:org∈dex :php/AAAI/article/view/ 11491 [154] C. Molnar, Interpretable Machine Learning: A Guide for Making Black Box Models Explainable, 2nd Edition, 2022. URL https://christophm :github:io∈terpretable-ml-book [155] D. W. Apley, J. Zhu, Visualizing the e ects of predictor variables in black box supervised learning models, Journal of the Royal Statistical Society Series B 82 (4) (2020) 1059–1086, publisher: Royal Statistical Society. URL https://econpapers :repec:org/article/blajorssb/ v3a82 3ay3a2020 3ai3a43ap3a1059-1086 :htm [156] A. Fisher, C. Rudin, F. Dominici, All Models are Wrong, but Many are 45\nUseful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously, Journal of Machine Learning Re- search 20 (177) (2019) 1–81. URL http://jmlr :org/papers/v20/18-760 :html [157] J. H. Friedman, B. E. Popescu, Predictive Learning via Rule Ensembles, The Annals of Applied Statistics 2 (3) (2008) 916–954, publisher: Insti- tute of Mathematical Statistics. URL http://www :jstor:org/stable/30245114 [158] G. Hooker, Discovering additive structure in black box functions, in: Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’04, Association for Com- puting Machinery, New York, NY , USA, 2004, pp. 575–580. doi: 10:1145/1014052 :1014122 . URL http://doi :org/10:1145/1014052 :1014122 [159] B. M. Greenwell, B. C. Boehmke, A. J. McCarthy, A Simple and E ec- tive Model-Based Variable Importance Measure, arXiv:1805.04755 [cs, stat] (May 2018). doi:10:48550/arXiv :1805:04755 . URL http://arxiv :org/abs/1805 :04755 [160] C. Schwartzenberg, T. M. v. Engers, Y . Li, The ﬁdelity of global surrogates in interpretable Machine Learning, in: BNAIC /BeneLearn 2020, 2020. URL https://bnaic :liacs:leidenuniv :nl/ wordpress/wp-content/uploads/papers/ BNAICBENELEARN 2020 Final paper 59:pdf [161] A. Goldstein, A. Kapelner, J. Bleich, E. Pitkin, Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individ- ual Conditional Expectation, Journal of Computational and Graph- ical Statistics 24 (1) (2015) 44–65, publisher: Taylor & Francis eprint: https: //doi.org /10.1080 /10618600.2014.907095. doi:10:1080/ 10618600:2014:907095 . URL https://doi :org/10:1080/10618600 :2014:907095 [162] R. Tibshirani, Regression Shrinkage and Selection via the Lasso, Journal of the Royal Statistical Society. Series B (Methodological) 58 (1) (1996) 267–288, publisher: [Royal Statistical Society, Wiley]. URL http://www :jstor:org/stable/2346178 [163] S. M. Lundberg, S.-I. Lee, A Uniﬁed Approach to Interpreting Model Predictions, in: Advances in Neural Information Processing Systems, V ol. 30, Curran Associates, Inc., 2017. URL https://papers :nips:cc/paper/2017/hash/ 8a20a8621978632d76c43dfd28b67767-Abstract :html [164] L. S. Shapley, 17. A Value for n-Person Games, in: H. W. Kuhn, A. W. Tucker (Eds.), Contributions to the Theory of Games (AM-28), V olume II, Princeton University Press, Princeton, 1953, pp. 307–318. doi:doi: 10:1515/9781400881970-018 . URL https://doi :org/10:1515/9781400881970-018 [165] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Ba- tra, Grad-CAM: Visual Explanations from Deep Networks via Gradient- Based Localization, in: 2017 IEEE International Conference on Com- puter Vision (ICCV), 2017, pp. 618–626, iSSN: 2380-7504. doi: 10:1109/ICCV :2017:74. [166] K. Guo, Y . Sun, X. Qian, Can investor sentiment be used to predict the stock price? Dynamic analysis based on China stock market, Phys- ica A: Statistical Mechanics and its Applications 469 (2017) 390–396. doi:https://doi :org/10:1016/j:physa:2016:11:114. URL https://www :sciencedirect :com/science/articleπi/ S0378437116309384 [167] Systematic Methods for Classifying Equities (Aug. 2018). URL https://www :winton:com/research/systematic- methods-for-classifying-equities [168] B. Kulis, Metric Learning: A Survey, Foundations and Trends ®in Ma- chine Learning 5 (4) (2013) 287–364, publisher: Now Publishers, Inc. doi:10:1561/2200000019 . URL https://www :nowpublishers :com/article/Details/MAL- 019 [169] M. Kaya, H. S. Bilge, Deep Metric Learning: A Survey, Symmetry 11 (9) (2019) 1066, number: 9 Publisher: Multidisciplinary Digital Pub- lishing Institute. doi:10:3390/sym11091066 . URL https://www :mdpi:com/2073-8994/11/9/1066 [170] Y . Zhu, W. Xu, J. Zhang, Q. Liu, S. Wu, L. Wang, Deep Graph Struc- ture Learning for Robust Representations: A Survey, arXiv:2103.03036 [cs]ArXiv: 2103.03036 (Mar. 2021).URL http://arxiv :org/abs/2103 :03036 [171] K. Hou, Industry Information Di usion and the Lead-Lag E ect in Stock Returns (Nov. 2003). doi:10:2139/ssrn :463005 . URL https://papers :ssrn:com/abstract=463005 [172] Y . Li, T. Wang, B. Sun, C. Liu, Detecting the lead–lag e ect in stock markets: deﬁnition, patterns, and investment strategies, Financial Innovation 8 (1) (2022) 1–36, number: 1 Publisher: SpringerOpen. doi:10:1186/s40854-022-00356-3 . URL https://jfin-swufe :springeropen :com/articles/ 10:1186/s40854-022-00356-3 [173] N. Fan, Z.-P. Fan, Y . Li, M. Li, Does the lead-lag ef- fect exist in stock markets?, Applied Economics Letters 29 (10) (2022) 895–900, publisher: Routledge eprint: https: //doi.org /10.1080 /13504851.2021.1897068. doi:10:1080/ 13504851:2021:1897068 . URL https://doi :org/10:1080/13504851 :2021:1897068 [174] L. Bottou, J. Peters, J. Qui ˜nonero-Candela, D. X. Charles, D. M. Chick- ering, E. Portugaly, D. Ray, P. Simard, E. Snelson, Counterfactual Rea- soning and Learning Systems: The Example of Computational Advertis- ing, Journal of Machine Learning Research 14 (101) (2013) 3207–3260. URL http://jmlr :org/papers/v14/bottou13a :html [175] 2020 stock market crash, page Version ID: 1108282892 (Sep. 2022). URL https://en :wikipedia :org/w∈dex :php?title= 2020 stock market crash&oldid=1108282892 [176] Z. Hu, W. Liu, J. Bian, X. Liu, T.-Y . Liu, Listening to Chaotic Whispers: A Deep Learning Framework for News-oriented Stock Trend Prediction, arXiv:1712.02136 [cs, q-ﬁn]ArXiv: 1712.02136 (Feb. 2019). URL http://arxiv :org/abs/1712 :02136 [177] V . Sakalauskas, D. Kriksciuniene, Research of the Calendar E ects in Stock Returns, in: W. Abramowicz, D. Flejter (Eds.), Business Informa- tion Systems Workshops, Springer Berlin Heidelberg, Berlin, Heidel- berg, 2009, pp. 69–78. [178] G. Bonne, J. Wang, H. Zhang, Machine Learning Factors: Capturing Non Linearities in Linear Factor Models, Tech. rep., MSCI (Mar. 2021). URL https://www :msci:com/www/research-report/machine- learning-factors/02410413451 [179] H. Sharma, Hierarchical Clustering (Apr. 2021). URL https://harshsharma1091996 :medium:com/ hierarchical-clustering-996745fe656b [180] D. M ¨ullner, Modern hierarchical, agglomerative clustering algorithms, arXiv:1109.2378 [cs, stat] (Sep. 2011). URL http://arxiv :org/abs/1109 :2378 [181] Knowledge-based systems, page Version ID: 1111001447 (Sep. 2022). URL https://en :wikipedia :org/w∈dex :php?title= Knowledge-based systems&oldid=1111001447 [182] F. Hayes-Roth, D. A. D. A. Waterman, D. B. Lenat, Building expert systems, Reading, Mass. : Addison-Wesley Pub. Co., 1983. URL http://archive :org/details/buildingexpertsy00temd [183] D. Cheng, F. Yang, X. Wang, Y . Zhang, L. Zhang, Knowledge Graph- based Event Embedding Framework for Financial Quantitative Invest- ments, in: Proceedings of the 43rd International ACM SIGIR Confer- ence on Research and Development in Information Retrieval, SIGIR ’20, Association for Computing Machinery, New York, NY , USA, 2020, pp. 2221–2230. doi:10:1145/3397271 :3401427 . URL http://doi :org/10:1145/3397271 :3401427 [184] J. Shinavier, K. Branson, W. Zhang, S. Dastgheib, Y . Gao, B. Arsin- tescu, F. ¨Ozcan, E. Meij, Panel: Knowledge Graph Industry Applica- tions, in: Companion Proceedings of The 2019 World Wide Web Con- ference, WWW ’19, Association for Computing Machinery, New York, NY , USA, 2019, p. 676. doi:10:1145/3308560 :3317711 . URL http://doi :org/10:1145/3308560 :3317711 [185] A. Singhal, Introducing the Knowledge Graph: things, not strings (May 2012). URL https://blog :google∏s/search∈troducing- knowledge-graph-things-not/ [186] S. Ji, S. Pan, E. Cambria, P. Marttinen, P. S. Yu, A Survey on Knowledge Graphs: Representation, Acquisition, and Applications, IEEE Transac- tions on Neural Networks and Learning Systems 33 (2) (2022) 494–514, conference Name: IEEE Transactions on Neural Networks and Learning Systems. doi:10:1109/TNNLS :2021:3070843 . [187] Knowledge representation and reasoning, page Version ID: 1121765398 46\n(Nov. 2022). URL https://en :wikipedia :org/w∈dex :php?title= Knowledge representation andreasoning&oldid= 1121765398 [188] J. Sowa, Semantic Networks (1992). URL http://www :jfsowa:com/pubs/semnet :htm [189] F. Lehmann, Semantic networks, Computers & Mathematics with Appli- cations 23 (2) (1992) 1–50. doi:10:1016/0898-1221(92)90135-5 . URL https://www :sciencedirect :com/science/articleπi/ 0898122192901355 [190] Semantic triple, page Version ID: 1068928735 (Jan. 2022). URL https://en :wikipedia :org/w∈dex :php?title= Semantic triple&oldid=1068928735 [191] Resource Description Framework (RDF) Model and Syntax Speciﬁca- tion. URL https://www :w3:org/TR/PR-rdf-syntax/ [192] A. Kamath, R. Das, A Survey on Semantic Parsing, in: 1st Conference on Automated Knowledge Base Construction, AKBC 2019, Amherst, MA, USA, May 20-22, 2019, 2019. doi:10:24432/C5WC7D . [193] Propositional Logic. URL https://iep :utm:edu/prop-log/ [194] C. J. H. Mann, The Description Logic Handbook - Theory, Imple- mentation and Applications, Kybernetes 32 (9 /10) (2003) 1563–1563, place: London Publisher: Emerald Group Publishing Limited. doi: 10:1108/k:2003:06732iae:006. [195] First-order logic, page Version ID: 1121783215 (Nov. 2022). URL https://en :wikipedia :org/w∈dex :php?title= First-order logic&oldid=1121783215 [196] A. Newell, J. C. Shaw, H. A. Simon, Report on a general problem- solving program, in: IFIP Congress, 1959. [197] A. Horn, On sentences which are true of direct unions of algebras1, The Journal of Symbolic Logic 16 (1) (1951) 14–21, publisher: Cambridge University Press. doi:10:2307/2268661 . URL https://www :cambridge :org/core/journals/ journal-of-symbolic-logic/article/abs/on-sentences- which-are-true-of-direct-unions-of-algebras1/ DF348CB269B06D6702DA3AE4DCF38C39 [198] P. Jackson, Introduction to expert systems, 3rd Edition, International computer science series., Addison-Wesley, Harlow, England ;, 1999. [199] M. Minsky, A framework for representing knowledge, in: The Psychol- ogy of Computer Vision, McGraw-Hill, 1975. URL https://web :media:mit:edu ~minsky/papers/Frames/ frames:html [200] J. Martin, Knowledge Engineering Environment: KEE (1988). URL https://purl :stanford:edu/sv429hd6966 [201] W. A. Woods, J. G. Schmolze, The KL-ONE family, Computers & Mathematics with Applications 23 (2) (1992) 133–177. doi:10:1016/ 0898-1221(92)90139-9 . URL https://www :sciencedirect :com/science/articleπi/ 0898122192901399 [202] D. B. Lenat, R. V . Guha, Building Large Knowledge-Based Systems; Representation and Inference in the Cyc Project, 1st Edition, Addison- Wesley Longman Publishing Co., Inc., USA, 1989. [203] RDF - Semantic Web Standards (Feburary 2014). URL https://www :w3:org/RDF/ [204] OWL - Semantic Web Standards (December 2012). URL https://www :w3:org/OWL/ [205] T. Berners-Lee, J. Hendler, O. Lassila, The Semantic Web, Scientiﬁc American 284 (5) (2001) 34–43. URL http://www :sciam:com/article :cfm?articleID= 00048144-10D2-1C70-84A9809EC588EF21 [206] What is a knowledge graph? URL https://www :jean-delahousse :net/en/graphe-de- connaissance-ontologie-vocabulaires-controles/ [207] E. F. Kendall, Ontology engineering, Synthesis lectures on the semantic web, theory and technology ; #18, Morgan & Claypool Publishers, San Rafael, California, 2019. [208] L. A. Gal ´arraga, C. Teﬂioudi, K. Hose, F. Suchanek, AMIE: association rule mining under incomplete evidence in ontological knowledge bases, in: Proceedings of the 22nd international conference on World Wide Web, WWW ’13, Association for Computing Machinery, New York,NY , USA, 2013, pp. 413–422. doi:10:1145/2488388 :2488425 . URL http://doi :org/10:1145/2488388 :2488425 [209] A. Neelakantan, B. Roth, A. McCallum, Compositional Vector Space Models for Knowledge Base Completion, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (V olume 1: Long Papers), Association for Computational Linguistics, Beijing, China, 2015, pp. 156–166. doi:10:3115/v1/P15-1016 . URL https://aclanthology :org/P15-1016 [210] R. Yangarber, R. Grishman, P. Tapanainen, S. Huttunen, Automatic Ac- quisition of Domain Knowledge for Information Extraction, in: COL- ING 2000 V olume 2: The 18th International Conference on Computa- tional Linguistics, 2000. URL https://aclanthology :org/C00-2136 [211] C. Welty, J. W. Murdock, Towards Knowledge Acquisition from In- formation Extraction, in: I. Cruz, S. Decker, D. Allemang, C. Preist, D. Schwabe, P. Mika, M. Uschold, L. M. Aroyo (Eds.), The Seman- tic Web - ISWC 2006, Lecture Notes in Computer Science, Springer, Berlin, Heidelberg, 2006, pp. 709–722. doi:10:1007/11926078 51. [212] I. Muhammad, A. Kearney, C. Gamble, F. Coenen, P. Williamson, Open Information Extraction for Knowledge Graph Construction, in: G. Kotsis, A. M. Tjoa, I. Khalil, L. Fischer, B. Moser, A. Mashkoor, J. Sametinger, A. Fensel, J. Martinez-Gil (Eds.), Database and Expert Systems Applications, Communications in Computer and Information Science, Springer International Publishing, Cham, 2020, pp. 103–113. doi:10:1007/978-3-030-59028-4 10. [213] V . Yadav, S. Bethard, A Survey on Recent Advances in Named En- tity Recognition from Deep Learning models, in: Proceedings of the 27th International Conference on Computational Linguistics, Associa- tion for Computational Linguistics, Santa Fe, New Mexico, USA, 2018, pp. 2145–2158. URL https://aclanthology :org/C18-1182 [214] S. Pawar, G. K. Palshikar, P. Bhattacharyya, Relation Extraction : A Survey, arXiv:1712.05191 [cs] (Dec. 2017). doi:10:48550/ arXiv:1712:05191 . URL http://arxiv :org/abs/1712 :05191 [215] D. Jurafsky, J. Martin, Semantic Role Labeling, in: Speech and Lan- guage Processing, 3rd Edition, 2021. URL https://web :stanford:edu ~jurafsky/slp3/19 :pdf [216] W. Shen, J. Wang, J. Han, Entity Linking with a Knowledge Base: Issues, Techniques, and Solutions, IEEE Transactions on Knowledge and Data Engineering 27 (2) (2015) 443–460, conference Name: IEEE Transactions on Knowledge and Data Engineering. doi:10:1109/ TKDE:2014:2327028 . [217] D. Zeng, K. Liu, S. Lai, G. Zhou, J. Zhao, Relation Classiﬁcation via Convolutional Deep Neural Network, in: Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tech- nical Papers, Dublin City University and Association for Computational Linguistics, Dublin, Ireland, 2014, pp. 2335–2344. URL https://aclanthology :org/C14-1220 [218] Y . Shen, X. Huang, Attention-Based Convolutional Neural Network for Semantic Relation Extraction, in: Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, The COLING 2016 Organizing Committee, Osaka, Japan, 2016, pp. 2526–2536. URL https://aclanthology :org/C16-1238 [219] Y . Zhang, P. Qi, C. D. Manning, Graph Convolution over Pruned Depen- dency Trees Improves Relation Extraction, in: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, As- sociation for Computational Linguistics, Brussels, Belgium, 2018, pp. 2205–2215. doi:10:18653/v1/D18-1244 . URL https://aclanthology :org/D18-1244 [220] M. Miwa, Y . Sasaki, Modeling Joint Entity and Relation Extraction with Table Representation, in: Proceedings of the 2014 Conference on Em- pirical Methods in Natural Language Processing (EMNLP), Associa- tion for Computational Linguistics, Doha, Qatar, 2014, pp. 1858–1869. doi:10:3115/v1/D14-1200 . URL https://aclanthology :org/D14-1200 [221] M. Miwa, M. Bansal, End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures, in: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (V olume 1: 47\nLong Papers), Association for Computational Linguistics, Berlin, Ger- many, 2016, pp. 1105–1116. doi:10:18653/v1/P16-1105 . URL https://aclanthology :org/P16-1105 [222] A. Katiyar, C. Cardie, Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees, in: Proceedings of the 55th Annual Meeting of the Association for Computational Linguis- tics (V olume 1: Long Papers), Association for Computational Linguis- tics, Vancouver, Canada, 2017, pp. 917–928. doi:10:18653/v1/P17- 1085 . URL https://aclanthology :org/P17-1085 [223] R. Ng, V . S. Subrahmanian, Probabilistic logic programming, Informa- tion and Computation 101 (2) (1992) 150–201. doi:10:1016/0890- 5401(92)90061-J . URL https://www :sciencedirect :com/science/articleπi/ 089054019290061J [224] M. Richardson, P. Domingos, Markov logic networks, Machine Learn- ing 62 (1) (2006) 107–136. doi:10:1007/s10994-006-5833-1 . URL https://doi :org/10:1007/s10994-006-5833-1 [225] J. Cussens, Parameter Estimation in Stochastic Logic Programs, Ma- chine Learning 44 (3) (2001) 245–271, company: Springer Distributor: Springer Institution: Springer Label: Springer Number: 3 Publisher: Kluwer Academic Publishers. doi:10:1023/A:1010924021315 . URL http://link :springer:com/article/10 :1023/A: 1010924021315 [226] W. W. Cohen, TensorLog: A Di erentiable Deductive Database, CoRR abs/1605.06523, arXiv: 1605.06523 (2016). URL http://arxiv :org/abs/1605 :06523 [227] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, O. Yakhnenko, Translating Embeddings for Modeling Multi-relational Data, in: Ad- vances in Neural Information Processing Systems, V ol. 26, Curran Associates, Inc., 2013. URL https://papers :nips:cc/paper/2013/hash/ 1cecc7a77928ca8133fa24680a88d2f9-Abstract :html [228] Y . Lin, Z. Liu, M. Sun, Y . Liu, X. Zhu, Learning Entity and Relation Embeddings for Knowledge Graph Completion, in: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, V ol. 29, 2015, number: 1. doi:10:1609/aaai :v29i1:9491 . URL https://ojs :aaai:org∈dex :php/AAAI/article/view/ 9491 [229] R. Socher, D. Chen, C. D. Manning, A. Y . Ng, Reasoning with neural tensor networks for knowledge base completion, in: Proceedings of the 26th International Conference on Neural Information Processing Sys- tems - V olume 1, NIPS’13, Curran Associates Inc., Red Hook, NY , USA, 2013, pp. 926–934. [230] T. Trouillon, J. Welbl, S. Riedel, E. Gaussier, G. Bouchard, Com- plex Embeddings for Simple Link Prediction, in: M.-F. Balcan, K. Q. Weinberger (Eds.), Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, V ol. 48 of JMLR Workshop and Conference Proceedings, JMLR.org, 2016, pp. 2071–2080. URL http://proceedings :mlr:press/v48/trouillon16 :html [231] Z. Sun, Z.-H. Deng, J.-Y . Nie, J. Tang, RotatE: Knowledge Graph Em- bedding by Relational Rotation in Complex Space, in: 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, OpenReview.net, 2019. URL https://openreview :net/forum?id=HkgEQnRqYQ [232] S. He, K. Liu, G. Ji, J. Zhao, Learning to Represent Knowledge Graphs with Gaussian Embedding, in: Proceedings of the 24th ACM Inter- national on Conference on Information and Knowledge Management, CIKM ’15, Association for Computing Machinery, New York, NY , USA, 2015, pp. 623–632. doi:10:1145/2806416 :2806502 . URL http://doi :org/10:1145/2806416 :2806502 [233] H. Xiao, M. Huang, X. Zhu, TransG : A Generative Model for Knowl- edge Graph Embedding, in: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers), Association for Computational Linguistics, Berlin, Germany, 2016, pp. 2316–2325. doi:10:18653/v1/P16-1219 . URL https://aclanthology :org/P16-1219 [234] H. Xiao, M. Huang, X. Zhu, From one point to a manifold: knowl- edge graph embedding for precise link prediction, in: Proceedings of the Twenty-Fifth International Joint Conference on Artiﬁcial Intelligence,IJCAI’16, AAAI Press, New York, New York, USA, 2016, pp. 1315– 1321. [235] T. Dettmers, P. Minervini, P. Stenetorp, S. Riedel, Convolutional 2D Knowledge Graph Embeddings, in: S. A. McIlraith, K. Q. Wein- berger (Eds.), Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative Applications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, AAAI Press, 2018, pp. 1811–1818. URL https://www :aaai:org/ocs∈dex :php/AAAI/AAAI18/ paper/view/17366 [236] L. Guo, Z. Sun, W. Hu, Learning to Exploit Long-term Relational De- pendencies in Knowledge Graphs, in: K. Chaudhuri, R. Salakhutdinov (Eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, V ol. 97 of Proceedings of Machine Learning Research, PMLR, 2019, pp. 2505–2514. URL http://proceedings :mlr:press/v97/guo19c :html [237] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. van den Berg, I. Titov, M. Welling, Modeling Relational Data with Graph Convolutional Net- works, in: A. Gangemi, R. Navigli, M.-E. Vidal, P. Hitzler, R. Troncy, L. Hollink, A. Tordai, M. Alam (Eds.), The Semantic Web, Lecture Notes in Computer Science, Springer International Publishing, Cham, 2018, pp. 593–607. doi:10:1007/978-3-319-93417-4 38. [238] W. Zhang, J. Chen, J. Li, Z. Xu, J. Z. Pan, H. Chen, Knowledge Graph Reasoning with Logics and Embeddings: Survey and Perspec- tive, arXiv:2202.07412 [cs] (Feb. 2022). URL http://arxiv :org/abs/2202 :07412 [239] Y . Lin, Z. Liu, H. Luan, M. Sun, S. Rao, S. Liu, Modeling Relation Paths for Representation Learning of Knowledge Bases, in: Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process- ing, Association for Computational Linguistics, Lisbon, Portugal, 2015, pp. 705–714. doi:10:18653/v1/D15-1082 . URL https://aclanthology :org/D15-1082 [240] B. Ding, Q. Wang, B. Wang, L. Guo, Improving Knowledge Graph Em- bedding Using Simple Constraints, in: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers), Association for Computational Linguistics, Melbourne, Australia, 2018, pp. 110–121. doi:10:18653/v1/P18-1011 . URL https://aclanthology :org/P18-1011 [241] S. Guo, Q. Wang, L. Wang, B. Wang, L. Guo, Jointly Embedding Knowledge Graphs and Logical Rules, in: Proceedings of the 2016 Con- ference on Empirical Methods in Natural Language Processing, Associ- ation for Computational Linguistics, Austin, Texas, 2016, pp. 192–202. doi:10:18653/v1/D16-1019 . URL https://aclanthology :org/D16-1019 [242] S. Guo, Q. Wang, L. Wang, B. Wang, L. Guo, Knowledge Graph Em- bedding with Iterative Guidance from Soft Rules, in: Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence and Thir- tieth Innovative Applications of Artiﬁcial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artiﬁcial Intel- ligence, AAAI’18 /IAAI’18 /EAAI’18, AAAI Press, 2018, event-place: New Orleans, Louisiana, USA. [243] Y . Wang, H. Wang, J. He, W. Lu, S. Gao, TAGAT: Type-Aware Graph Attention neTworks for reasoning over knowledge graphs, Knowledge-Based Systems 233 (2021) 107500. doi:10:1016/ j:knosys:2021:107500 . URL https://www :sciencedirect :com/science/articleπi/ S0950705121007620 [244] Z. Zhang, F. Zhuang, M. Qu, F. Lin, Q. He, Knowledge Graph Embed- ding with Hierarchical Relation Structure, in: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, As- sociation for Computational Linguistics, Brussels, Belgium, 2018, pp. 3198–3207. doi:10:18653/v1/D18-1358 . URL https://aclanthology :org/D18-1358 [245] K.-W. Chang, W.-t. Yih, B. Yang, C. Meek, Typed Tensor Decomposi- tion of Knowledge Bases for Relation Extraction, in: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), Association for Computational Linguistics, Doha, Qatar, 2014, pp. 1568–1579. doi:10:3115/v1/D14-1165 . 48\nURL https://aclanthology :org/D14-1165 [246] M. Qu, J. Tang, Probabilistic Logic Neural Networks for Reasoning, in: Advances in Neural Information Processing Systems, V ol. 32, Curran Associates, Inc., 2019. URL https://proceedings :neurips:cc/paper/2019/hash/ 13e5ebb0fa112fe1b31a1067962d74a7-Abstract :html [247] A. P. Dempster, N. M. Laird, D. B. Rubin, Maximum Likelihood from Incomplete Data Via the EM Algorithm, Journal of the Royal Statis- tical Society: Series B (Methodological) 39 (1) (1977) 1–22. doi: 10:1111/j:2517-6161 :1977:tb01600:x. URL https://onlinelibrary :wiley:com/doi/10 :1111/j:2517- 6161:1977:tb01600:x [248] Z. Wei, J. Zhao, K. Liu, Z. Qi, Z. Sun, G. Tian, Large-scale Knowl- edge Base Completion: Inferring via Grounding Network Sampling over Selected Instances, in: Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM ’15, Association for Computing Machinery, New York, NY , USA, 2015, pp. 1331–1340. doi:10:1145/2806416 :2806513 . URL http://doi :org/10:1145/2806416 :2806513 [249] T. Rockt ¨aschel, S. Riedel, End-to-end Di erentiable Proving, in: Advances in Neural Information Processing Systems, V ol. 30, Curran Associates, Inc., 2017. URL https://papers :nips:cc/paper/2017/hash/ b2ab001909a8a6f04b51920306046ce5-Abstract :html [250] W. Y . Wang, W. W. Cohen, Learning First-Order Logic Embeddings via Matrix Factorization, in: Proceedings of the Twenty-Fifth Inter- national Joint Conference on Artiﬁcial Intelligence, IJCAI’16, AAAI Press, 2016, pp. 2132–2138, event-place: New York, New York, USA. [251] F. Yang, Z. Yang, W. W. Cohen, Di erentiable Learning of Logical Rules for Knowledge Base Reasoning, in: Advances in Neural Informa- tion Processing Systems, V ol. 30, Curran Associates, Inc., 2017. URL https://papers :nips:cc/paper/2017/hash/ 0e55666a4ad822e0e34299df3591d979-Abstract :html [252] S. Deng, N. Zhang, W. Zhang, J. Chen, J. Z. Pan, H. Chen, Knowledge- Driven Stock Trend Prediction and Explanation via Temporal Con- volutional Network, in: Companion Proceedings of The 2019 World Wide Web Conference, ACM, San Francisco USA, 2019, pp. 678–685. doi:10:1145/3308560 :3317701 . URL https://dl :acm:org/doi/10 :1145/3308560 :3317701 [253] X. Ding, Y . Zhang, T. Liu, J. Duan, Knowledge-Driven Event Embed- ding for Stock Prediction, in: Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Pa- pers, The COLING 2016 Organizing Committee, Osaka, Japan, 2016, pp. 2133–2142. URL https://www :aclweb:org/anthology/C16-1201 [254] A. Sil, A. Yates, Re-ranking for joint named-entity recognition and linking, in: Proceedings of the 22nd ACM international conference on Information & Knowledge Management, CIKM ’13, Association for Computing Machinery, New York, NY , USA, 2013, pp. 2369–2374. doi:10:1145/2505515 :2505601 . URL http://doi :org/10:1145/2505515 :2505601 [255] S. Bai, J. Z. Kolter, V . Koltun, An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling, CoRR abs/1803.01271, arXiv: 1803.01271 (2018). URL http://arxiv :org/abs/1803 :01271 [256] J. Long, Z. Chen, W. He, T. Wu, J. Ren, An integrated framework of deep learning and knowledge graph for prediction of stock price trend: An application in Chinese stock exchange market, Applied Soft Computing 91 (2020) 106205. doi:10:1016/j:asoc:2020:106205 . URL https://linkinghub :elsevier:com/retrieveπi/ S1568494620301459 [257] A. Grover, J. Leskovec, node2vec: Scalable Feature Learning for Net- works, in: Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining, KDD ’16, Associ- ation for Computing Machinery, New York, NY , USA, 2016, pp. 855– 864. doi:10:1145/2939672 :2939754 . URL http://doi :org/10:1145/2939672 :2939754 [258] G. Ang, E.-P. Lim, Learning Knowledge-Enriched Company Embed- dings for Investment Management, in: Proceedings of the Second ACM International Conference on AI in Finance, ICAIF ’21, Association for Computing Machinery, New York, NY , USA, 2021, event-place: VirtualEvent. doi:10:1145/3490354 :3494390 . URL https://doi-org :lib:ezproxy:ust:hk/10:1145/ 3490354:3494390 [259] W. Xu, W. Liu, C. Xu, J. Bian, J. Yin, T.-Y . Liu, REST: Relational Event- driven Stock Trend Forecasting, in: Proceedings of the Web Conference 2021, WWW ’21, Association for Computing Machinery, New York, NY , USA, 2021, pp. 1–10. doi:10:1145/3442381 :3450032 . URL http://doi :org/10:1145/3442381 :3450032 [260] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauly, M. J. Franklin, S. Shenker, I. Stoica, Resilient Distributed Datasets: AfFault-TolerantgAbstraction forfIn-MemorygCluster Computing, in: 9th USENIX Symposium on Networked Systems Design and Implementation (NSDI 12), 2012, pp. 15–28. URL https://www :usenix:org/conference/nsdi12/ technical-sessions/presentation/zaharia [261] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, I. Stoica, Spark: Cluster Computing with Working Sets, in: 2nd USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 10), 2010. URL https://www :usenix:org/conference/hotcloud-10/ spark-cluster-computing-working-sets [262] Q. Wang, Z. Xu, Z. Chen, Y . Wang, S. Liu, H. Qu, Visual analysis of discrimination in machine learning, IEEE Transactions on Visualization and Computer Graphics 27 (2) (2020) 1470–1480, publisher: IEEE. [263] S. Ghemawat, H. Gobio , S.-T. Leung, The Google File System, in: Proceedings of the 19th ACM Symposium on Operating Systems Prin- ciples, Bolton Landing, NY , 2003, pp. 20–43. [264] K. Shvachko, H. Kuang, S. Radia, R. Chansler, The Hadoop Distributed File System, in: 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST), 2010, pp. 1–10, iSSN: 2160-1968. doi: 10:1109/MSST :2010:5496972 . [265] W. Fedus, B. Zoph, N. Shazeer, Switch Transformers: Scaling to Trillion Parameter Models with Simple and E \u000ecient Sparsity, arXiv:2101.03961 [cs]ArXiv: 2101.03961 (Jan. 2021). URL http://arxiv :org/abs/2101 :03961 [266] G. Bender, P.-J. Kindermans, B. Zoph, V . Vasudevan, Q. Le, Understand- ing and Simplifying One-Shot Architecture Search, in: Proceedings of the 35th International Conference on Machine Learning, PMLR, 2018, pp. 550–559, iSSN: 2640-3498. URL https://proceedings :mlr:press/v80/bender18a :html [267] S. A. Weil, S. A. Brandt, E. L. Miller, D. D. E. Long, C. Maltzahn, Ceph: a scalable, high-performance distributed ﬁle system, in: Proceedings of the 7th symposium on Operating systems design and implementation, OSDI ’06, USENIX Association, USA, 2006, pp. 307–320. [268] M. Technologies, Inﬁniband technology overview, InﬁniBand White Paper (2008). URL https://network :nvidia:com/related-docs/ whitepapers/WP InfiniBand Technology Overview:pdf [269] Z. Zhang, C. Chang, H. Lin, Y . Wang, R. Arora, X. Jin, Is Net- work the Bottleneck of Distributed Training?, in: Proceedings of the Workshop on Network Meets AI & ML, NetAI ’20, Association for Computing Machinery, New York, NY , USA, 2020, pp. 8–13. doi: 10:1145/3405671 :3405810 . URL http://doi :org/10:1145/3405671 :3405810 [270] E. F. Codd, A Relational Model of Data for Large Shared Data Banks, Commun. ACM 13 (6) (1970) 377–387. doi:10:1145/ 362384:362685 . URL http://doi :acm:org/10:1145/362384 :362685 [271] D. Namiot, Time Series Databases, in: DAMDID /RCDL, 2015. [272] S. K. Jensen, T. B. Pedersen, C. Thomsen, Time Series Man- agement Systems: A Survey, IEEE Transactions on Knowledge and Data Engineering 29 (11) (2017) 2581–2600. doi:10:1109/ TKDE:2017:2740932 . URL http://ieeexplore :ieee:org/document/8012550/ [273] F. Gessert, W. Wingerath, S. Friedrich, N. Ritter, NoSQL database sys- tems: a survey and decision guidance, Computer Science - Research and Development 32 (3) (2017) 353–365. doi:10:1007/s00450-016- 0334-3 . URL https://doi :org/10:1007/s00450-016-0334-3 [274] R. kumar Kaliyar, Graph databases: A survey, in: Communication & Automation International Conference on Computing, 2015, pp. 785– 790. doi:10:1109/CCAA :2015:7148480 . 49\n[275] R. Angles, C. Gutierrez, Survey of graph database models, ACM Computing Surveys 40 (1) (2008) 1:1–1:39. doi:10:1145/ 1322432:1322433 . URL http://doi :org/10:1145/1322432 :1322433 [276] K.-L. Tan, Q. Cai, B. C. Ooi, W.-F. Wong, C. Yao, H. Zhang, In- memory Databases: Challenges and Opportunities From Software and Hardware Perspectives, ACM SIGMOD Record 44 (2) (2015) 35–40. doi:10:1145/2814710 :2814717 . URL http://doi :org/10:1145/2814710 :2814717 [277] J. Dean, S. Ghemawat, MapReduce: simpliﬁed data processing on large clusters, Communications of the ACM 51 (1) (2008) 107–113. doi: 10:1145/1327452 :1327492 . URL http://doi :org/10:1145/1327452 :1327492 [278] Apache Hadoop. URL https://hadoop :apache:org/ [279] J. L. Hennessy, D. A. Patterson, Computer Architecture, Fifth Edition: A Quantitative Approach, 5th Edition, Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2011. [280] Y . Lu, J. Cheng, D. Yan, H. Wu, Large-scale distributed graph com- puting systems: an experimental evaluation, Proceedings of the VLDB Endowment 8 (3) (2014) 281–292. doi:10:14778/2735508 :2735517 . URL http://doi :org/10:14778/2735508 :2735517 [281] W. Xiao, J. Xue, Y . Miao, Z. Li, C. Chen, M. Wu, W. Li, L. Zhou, fTux²g: Distributed Graph Computation for Machine Learning, 2017, pp. 669–682. URL https://www :usenix:org/conference/nsdi17/ technical-sessions/presentationξao [282] D. Yang, J. Liu, J. Lai, EDGES: An E \u000ecient Distributed Graph Em- bedding System on GPU Clusters, IEEE Transactions on Parallel and Distributed Systems 32 (7) (2021) 1892–1902, conference Name: IEEE Transactions on Parallel and Distributed Systems. doi:10:1109/ TPDS:2020:3041219 . [283] CUDA Toolkit Documentation (2022). URL https://docs :nvidia:com/cuda/ [284] MPI: A message passing interface, in: Supercomputing ’93:Proceedings of the 1993 ACM /IEEE Conference on Supercomputing, 1993, pp. 878– 883, iSSN: 1063-9535. doi:10:1145/169627 :169855 . [285] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K ¨opf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, S. Chintala, PyTorch: An Imperative Style, High- Performance Deep Learning Library, arXiv:1912.01703 [cs, stat]ArXiv: 1912.01703 (Dec. 2019). URL http://arxiv :org/abs/1912 :01703 [286] P. Barham, A. Chowdhery, J. Dean, S. Ghemawat, S. Hand, D. Hurt, M. Isard, H. Lim, R. Pang, S. Roy, B. Saeta, P. Schuh, R. Sepassi, L. Shafey, C. Thekkath, Y . Wu, Pathways: Asynchronous Distributed Dataﬂow for ML, Proceedings of Machine Learning and Systems 4 (2022) 430–449. URL https://proceedings :mlsys:org/paper/2022/hash/ 98dce83da57b0395e163467c9dae521b-Abstract :html [287] H. Jin, Q. Song, X. Hu, Auto-Keras: An E \u000ecient Neural Architec- ture Search System, arXiv:1806.10282 [cs, stat] (Mar. 2019). doi: 10:48550/arXiv :1806:10282 . URL http://arxiv :org/abs/1806 :10282 [288] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catan- zaro, Megatron-LM: Training Multi-Billion Parameter Language Mod- els Using Model Parallelism, arXiv:1909.08053 [cs] (Mar. 2020). doi: 10:48550/arXiv :1909:08053 . URL http://arxiv :org/abs/1909 :08053 [289] S. Rajbhandari, J. Rasley, O. Ruwase, Y . He, ZeRO: Memory Optimiza- tions Toward Training Trillion Parameter Models, arXiv:1910.02054 [cs, stat]ArXiv: 1910.02054 (May 2020). URL http://arxiv :org/abs/1910 :02054 [290] D. Byrd, M. Hybinette, T. H. Balch, Abides: Towards high-ﬁdelity multi-agent market simulation, in: Proceedings of the 2020 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation, SIGSIM-PADS ’20, Association for Computing Machinery, New York, NY , USA, 2020, p. 11–22. doi:10:1145/3384441 :3395986 . URL https://doi :org/10:1145/3384441 :3395986 [291] S. Amrouni, A. Moulin, J. Vann, S. Vyetrenko, T. Balch, M. Veloso,ABIDES-Gym: Gym Environments for Multi-Agent Discrete Event Simulation and Application to Financial Markets, in: Proceedings of the Second ACM International Conference on AI in Finance, ICAIF ’21, Association for Computing Machinery, New York, NY , USA, 2021, event-place: Virtual Event. doi:10:1145/3490354 :3494433 . URL https://doi-org :lib:ezproxy:ust:hk/10:1145/ 3490354:3494433 [292] M. Karpe, J. Fang, Z. Ma, C. Wang, Multi-Agent Reinforcement Learn- ing in a Realistic Limit Order Book Market Simulation, in: Proceedings of the First ACM International Conference on AI in Finance, ICAIF ’20, Association for Computing Machinery, New York, NY , USA, 2020, event-place: New York, New York. doi:10:1145/3383455 :3422570 . URL https://doi-org :lib:ezproxy:ust:hk/10:1145/ 3383455:3422570 [293] A. Coletta, M. Prata, M. Conti, E. Mercanti, N. Bartolini, A. Moulin, S. Vyetrenko, T. Balch, Towards Realistic Market Simulations: A Generative Adversarial Networks Approach, in: Proceedings of the Second ACM International Conference on AI in Finance, ICAIF ’21, Association for Computing Machinery, New York, NY , USA, 2021, event-place: Virtual Event. doi:10:1145/3490354 :3494411 . URL https://doi-org :lib:ezproxy:ust:hk/10:1145/ 3490354:3494411 [294] Apache Airﬂow, original-date: 2015-04-13T18:04:58Z (Nov. 2022). URL https://github :com/apache/airflow [295] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catan- zaro, E. Shelhamer, cuDNN: E \u000ecient Primitives for Deep Learning (Oct. 2014). doi:10:48550/arXiv :1410:0759 . URL https://arxiv :org/abs/1410 :0759v3 [296] E. Wang, Q. Zhang, B. Shen, G. Zhang, X. Lu, Q. Wu, Y . Wang, In- tel Math Kernel Library, in: High-Performance Computing on the Intel Xeon Phi, 2014, pp. 167–188. doi:10:1007/978-3-319-06486-4 7. [297] R. van de Geijn, K. Goto, BLAS (Basic Linear Algebra Subprograms), in: D. Padua (Ed.), Encyclopedia of Parallel Computing, Springer US, Boston, MA, 2011, pp. 157–164. doi:10:1007/978-0-387-09766- 484. URL https://doi :org/10:1007/978-0-387-09766-4 84 [298] N. P. Jouppi, C. Young, N. Patil, D. Patterson, A domain-speciﬁc archi- tecture for deep neural networks, Communications of the ACM 61 (9) (2018) 50–59. doi:10:1145/3154484 . URL https://doi :org/10:1145/3154484 [299] S. Markidis, S. Chien, E. Laure, I. Peng, J. S. Vetter, NVIDIA Tensor Core Programmability, Performance &amp; Precision, in: 2018 IEEE International Parallel and Distributed Processing Symposium Work- shops (IPDPSW), IEEE Computer Society, Los Alamitos, CA, USA, 2018, pp. 522–531. doi:10:1109/IPDPSW :2018:00091 . URL https://doi :ieeecomputersociety :org/10:1109/ IPDPSW:2018:00091 [300] K. Rupnow, Y . Liang, Y . Li, D. Chen, A study of high-level synthe- sis: Promises and challenges, in: 2011 9th IEEE International Con- ference on ASIC, 2011, pp. 1102–1105, iSSN: 2162-755X. doi: 10:1109/ASICON :2011:6157401 . [301] P. Coussy, D. Gajski, M. Meredith, A. Takach, An Introduction to High- Level Synthesis, IEEE Design & Test of Computers 26 (4) (2009) 8–17. doi:10:1109/MDT:2009:69. URL http://ieeexplore :ieee:org/document/5209958/ [302] J. Cong, J. Lau, G. Liu, S. Neuendor er, P. Pan, K. Vissers, Z. Zhang, FPGA HLS Today: Successes, Challenges, and Opportunities, ACM Trans. Reconﬁgurable Technol. Syst.Place: New York, NY , USA Pub- lisher: Association for Computing Machinery (Apr. 2022). doi: 10:1145/3530775 . URL https://doi-org :lib:ezproxy:ust:hk/10:1145/3530775 [303] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V . Korthikanti, E. Zhang, R. Child, R. Y . Aminabadi, J. Bernauer, X. Song, M. Shoeybi, Y . He, M. Houston, S. Tiwary, B. Catanzaro, Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Lan- guage Model, arXiv:2201.11990 [cs] (Feb. 2022). URL http://arxiv :org/abs/2201 :11990 [304] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, L. Zettlemoyer, Deep contextualized word representations, in: Proceed- ings of the 2018 Conference of the North American Chapter of the As- 50\nsociation for Computational Linguistics: Human Language Technolo- gies, V olume 1 (Long Papers), Association for Computational Linguis- tics, New Orleans, Louisiana, 2018, pp. 2227–2237. doi:10:18653/ v1/N18-1202 . URL https://aclanthology :org/N18-1202 [305] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfs- son, S. Buch, D. Card, R. Castellon, N. Chatterji, A. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Dur- mus, S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. Goodman, S. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Koh, M. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Lev- ent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, S. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair, A. Narayan, D. Narayanan, B. New- man, A. Nie, J. C. Niebles, H. Nilforoshan, J. Nyarko, G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech, E. Portelance, C. Potts, A. Raghu- nathan, R. Reich, H. Ren, F. Rong, Y . Roohani, C. Ruiz, J. Ryan, C. R ´e, D. Sadigh, S. Sagawa, K. Santhanam, A. Shih, K. Srinivasan, A. Tamkin, R. Taori, A. W. Thomas, F. Tram `er, R. E. Wang, W. Wang, B. Wu, J. Wu, Y . Wu, S. M. Xie, M. Yasunaga, J. You, M. Zaharia, M. Zhang, T. Zhang, X. Zhang, Y . Zhang, L. Zheng, K. Zhou, P. Liang, On the Opportunities and Risks of Foundation Models, arXiv:2108.07258 [cs] (Aug. 2021). doi:10:48550/arXiv :2108:07258 . URL http://arxiv :org/abs/2108 :07258 [306] Z.-H. Zhou, Ensemble Methods: Foundations and Algorithms, 1st Edi- tion, Chapman & Hall /CRC, 2012. [307] V . J. Hellendoorn, A. A. Sawant, The growing cost of deep learning for source code, Communications of the ACM 65 (1) (2021) 31–33. doi: 10:1145/3501261 . URL http://doi :org/10:1145/3501261 [308] S. C. H. Hoi, D. Sahoo, J. Lu, P. Zhao, Online learning: A compre- hensive survey, Neurocomputing 459 (2021) 249–289. doi:10:1016/ j:neucom:2021:04:112. URL https://www :sciencedirect :com/science/articleπi/ S0925231221006706 [309] M. De Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis, G. Slabaugh, T. Tuytelaars, A Continual Learning Survey: Defying Forgetting in Classiﬁcation Tasks, IEEE Transactions on Pattern Anal- ysis and Machine Intelligence 44 (7) (2022) 3366–3385, conference Name: IEEE Transactions on Pattern Analysis and Machine Intelli- gence. doi:10:1109/TPAMI :2021:3057446 . [310] A. Acar, H. Aksu, A. S. Uluagac, M. Conti, A Survey on Homomor- phic Encryption Schemes: Theory and Implementation, ACM Comput- ing Surveys 51 (4) (2018) 79:1–79:35. doi:10:1145/3214303 . URL http://doi :org/10:1145/3214303 [311] C. Zhang, Y . Xie, H. Bai, B. Yu, W. Li, Y . Gao, A survey on feder- ated learning, Knowledge-Based Systems 216 (2021) 106775. doi: 10:1016/j:knosys:2021:106775 . URL https://www :sciencedirect :com/science/articleπi/ S0950705121000381 [312] Q. Li, Z. Wen, Z. Wu, S. Hu, N. Wang, Y . Li, X. Liu, B. He, A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection, IEEE Transactions on Knowledge and Data Engineering (2021) 1–1Conference Name: IEEE Transactions on Knowledge and Data Engineering. doi:10:1109/TKDE :2021:3124599 . [313] M. S. Burgin, Theory of knowledge: structures and processes, World Scientiﬁc series in information studies ; V ol. 5, World Scientiﬁc Pub- lishing Co. Pte Ltd., Singapore, 2017. [314] D. H. Jonassen, Structural knowledge: techniques for representing, con- veying, and acquiring structural knowledge, L. Erlbaum Associates, Hillsdale, N.J, 1993. [315] C. Pavese, Knowledge How, in: E. N. Zalta, U. Nodelman (Eds.), The Stanford Encyclopedia of Philosophy, fall 2022 Edition, Metaphysics Research Lab, Stanford University, 2022. URL https://plato :stanford:edu/archives/fall2022/ entries/knowledge-how/ [316] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhari- wal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal,A. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, D. Amodei, Language Models are Few-Shot Learners, arXiv:2005.14165 [cs]ArXiv: 2005.14165 (Jul. 2020). URL http://arxiv :org/abs/2005 :14165 [317] D. Ha, J. Schmidhuber, World models (2018). doi:10:5281/ ZENODO:1207631 . URL https://zenodo :org/record/1207631 [318] K. Alattas, A. Alkaabi, A. B. Alsaud, An Overview of Artiﬁcial General Intelligence: Recent Developments and Future Challenges, Journal of Computer Science 17 (4) (2021) 364–370, publisher: Science Publica- tions. doi:10:3844/jcssp :2021:364:370. URL https://thescipub :com/abstract/jcssp :2021:364:370 [319] D. Kahneman, Thinking, fast and slow, Farrar, Straus and Giroux, New York, 2011. URL https://www :amazon:de/Thinking-Fast-Slow-Daniel- Kahneman/dp/0374275637/ref=wl itdpopdT1 nSnC?ie= UTF8&colid=151193SNGKJT9&coliid=I3OCESLZCVDFL7 [320] G. Marcus, The Next Decade in AI: Four Steps Towards Robust Artiﬁ- cial Intelligence, arXiv:2002.06177 [cs]ArXiv: 2002.06177 (Feb. 2020). URL http://arxiv :org/abs/2002 :06177 [321] Y . Bengio, GFlowNets and System 2 Deep Learning (2022). URL https://www :microsoft :com/en-us/research/video/ gflownets-and-system-2-deep-learning/ [322] Y . Bengio, S. Lahlou, T. Deleu, E. J. Hu, M. Tiwari, E. Bengio, GFlowNet Foundations, arXiv:2111.09266 [cs, stat] (Aug. 2022). URL http://arxiv :org/abs/2111 :09266 [323] L. Yao, Z. Chu, S. Li, Y . Li, J. Gao, A. Zhang, A Survey on Causal In- ference, ACM Transactions on Knowledge Discovery from Data 15 (5) (2021) 74:1–74:46. doi:10:1145/3444944 . URL http://doi :org/10:1145/3444944 [324] J. Pearl, The book of why : the new science of cause and e ect, Allen Lane, London, 2018, publication Title: The book of why : the new sci- ence of cause and e ect. [325] B. Sch ¨olkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, Y . Bengio, Toward Causal Representation Learning, Proceed- ings of the IEEE 109 (5) (2021) 612–634, conference Name: Proceed- ings of the IEEE. doi:10:1109/JPROC :2021:3058954 . [326] T. J. VanderWeele, I. Shpitser, On the deﬁnition of a confounder, The Annals of Statistics 41 (1) (2013) 196–220, publisher: Institute of Mathematical Statistics. doi:10:1214/12-AOS1058 . URL https://projecteuclid :org/journals/annals-of- statistics/volume-41/issue-1/On-the-definition-of-a- confounder/10 :1214/12-AOS1058 :full [327] G. Atluri, A. Karpatne, V . Kumar, Spatio-Temporal Data Mining: A Survey of Problems and Methods, ACM Comput. Surv. 51 (4), place: New York, NY , USA Publisher: Association for Computing Machinery (Aug. 2018). doi:10:1145/3161602 . URL https://doi-org :lib:ezproxy:ust:hk/10:1145/3161602 [328] S. Wang, J. Cao, P. Yu, Deep Learning for Spatio-Temporal Data Min- ing: A Survey, IEEE Transactions on Knowledge and Data Engineering (2020) 1–1Conference Name: IEEE Transactions on Knowledge and Data Engineering. doi:10:1109/TKDE :2020:3025580 . [329] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, arXiv:1810.04805 [cs]ArXiv: 1810.04805 (May 2019). URL http://arxiv :org/abs/1810 :04805 [330] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, I. Sutskever, Learning Transferable Visual Models From Natural Language Su- pervision, arXiv:2103.00020 [cs] (Feb. 2021). doi:10:48550/ arXiv:2103:00020 . URL http://arxiv :org/abs/2103 :00020 [331] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y . Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-V oss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, 51\nA. N. Carr, J. Leike, J. Achiam, V . Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. Mc- Grew, D. Amodei, S. McCandlish, I. Sutskever, W. Zaremba, Evaluating Large Language Models Trained on Code, arXiv:2107.03374 [cs] (Jul. 2021). doi:10:48550/arXiv :2107:03374 . URL http://arxiv :org/abs/2107 :03374 [332] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. V oss, A. Radford, M. Chen, I. Sutskever, Zero-Shot Text-to-Image Generation, arXiv:2102.12092 [cs] (Feb. 2021). doi:10:48550/arXiv :2102:12092 . URL http://arxiv :org/abs/2102 :12092 [333] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N. Houlsby, An Image is Worth 16x16 Words: Trans- formers for Image Recognition at Scale, in: International Conference on Learning Representations, 2022. URL https://openreview :net/forum?id=YicbFdNTTy [334] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Ag- garwal, O. K. Mohammed, S. Singhal, S. Som, F. Wei, Image as a Foreign Language: BEiT Pretraining for All Vision and Vision- Language Tasks, arXiv:2208.10442 [cs] (Aug. 2022). doi:10:48550/ arXiv:2208:10442 . URL http://arxiv :org/abs/2208 :10442 [335] X.-C. Zhang, C.-K. Wu, Z.-J. Yang, Z.-X. Wu, J.-C. Yi, C.-Y . Hsieh, T.-J. Hou, D.-S. Cao, MG-BERT: leveraging unsupervised atomic repre- sentation learning for molecular property prediction, Brieﬁngs in Bioin- formatics 22 (6) (2021) bbab152. doi:10:1093/bib/bbab152 . URL https://doi :org/10:1093/bib/bbab152 [336] R. Guo, L. Cheng, J. Li, P. R. Hahn, H. Liu, A Survey of Learning Causality with Data: Problems and Methods, ACM Computing Surveys 53 (4) (2020) 75:1–75:37. doi:10:1145/3397269 . URL http://doi :org/10:1145/3397269 [337] Z. Shen, J. Liu, Y . He, X. Zhang, R. Xu, H. Yu, P. Cui, Towards Out- Of-Distribution Generalization: A Survey, arXiv:2108.13624 [cs] (Aug. 2021). doi:10:48550/arXiv :2108:13624 . URL http://arxiv :org/abs/2108 :13624 [338] L. Breiman, Bagging predictors, Machine Learning 24 (2) (1996) 123– 140. doi:10:1007/BF00058655 . URL https://doi :org/10:1007/BF00058655 [339] R. E. Schapire, The strength of weak learnability, Machine Learning 5 (2) (1990) 197–227. doi:10:1007/BF00116037 . URL https://doi :org/10:1007/BF00116037 [340] L. Breiman, Arcing classiﬁer (with discussion and a rejoinder by the author), The Annals of Statistics 26 (3) (1998) 801–849, publisher: Institute of Mathematical Statistics. doi:10:1214/aos/1024691079 . URL https://projecteuclid :org/journals/annals-of- statistics/volume-26/issue-3/Arcing-classifier-with- discussion-and-a-rejoinder-by-the-author/10 :1214/aos/ 1024691079 :full [341] D. H. Wolpert, Stacked generalization, Neural Networks 5 (2) (1992) 241–259. doi:10:1016/S0893-6080(05)80023-1 . URL https://www :sciencedirect :com/science/articleπi/ S0893608005800231 [342] L. Breiman, Stacked regressions, Machine Learning 24 (1) (1996) 49– 64.doi:10:1007/BF00117832 . URL https://doi :org/10:1007/BF00117832 [343] J. A. Hoeting, D. Madigan, A. E. Raftery, C. T. V olinsky, Bayesian Model Averaging: A Tutorial, Statistical Science 14 (4) (1999) 382– 401, publisher: Institute of Mathematical Statistics. URL http://www :jstor:org/stable/2676803 [344] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting, Journal of Machine Learning Research 15 (56) (2014) 1929–1958. URL http://jmlr :org/papers/v15/srivastava14a :html 52\nAuthor Biographies Jian Guo is currently the Executive Pres- ident and a Chief Scientist at International Digital Economy Academy (IDEA). As a founding member of IDEA, he also serves as the head of IDEA Research Center of AI Finance & Deep Learning and a Professor of Practice at the Hong Kong University of Science and Technol- ogy (Guangzhou). Dr. Guo received his B.S. in mathematics from Tsinghua Uni- versity, and received his Ph.D. in statistics from University of Michigan in 2011. He started his professorship (tenure-track) at Harvard University since 2011. He published a number of re- search papers in deep /reinforcement /statistical learning, includ- ing theory and application. Dr. Guo is one of the pioneering AI ﬁnance researchers, and is an entrepreneur in quantitative in- vestment industry. Saizhuo Wang is currently a Ph.D. candi- date in Department of Computer Science and Engineering at the Hong Kong Uni- versity of Science and Technology, under the supervision of Prof. Harry Heung- Yeung Shum and Prof. Lionel Ming- Shuan Ni and working with Prof. Jian Guo. He received his bachelor of engi- neering degree in computer science from Chu Kochen Honors College at Zhejiang University. His research interest mainly in the interdisciplinary ﬁeld of artiﬁcial intelligence and ﬁnancial technology. Lionel M. Ni is currently the Founding President of the Hong Kong University of Science and Technology (Guangzhou) and Chair Professor in the university’s Data Science and Analytics Thrust, as well as Chair Professor of Computer Sci- ence and Engineering at the Hong Kong University of Science and Technology. He is a Life Fellow of IEEE, and a Fellow of the Hong Kong Academy of Engineer- ing Science. Prof. Ni’s research includes high-performance computing, mobile computing, wireless networking, big data, and intelligent computing. He has published three books and 350+refereed journal and conference articles. He has chaired 30+professional conferences and has received eight awards for authoring outstanding papers. Prof. Ni received his Ph.D. in Electrical Engineering from Purdue University in 1980. Heung-Yeung Shum is the Founding Chairman of International Digital Econ- omy Academy (IDEA), and a Professor- at-Large at the Institute for Advanced Study, Hong Kong University of Science and Technology. He is a Foreign Mem- ber of National Academy of Engineer- ing of the US, International Fellow of Royal Academy of Engineering of the UK, ACM Fellow and IEEE Fellow. Until March 2020, he was the Executive Vice President of Microsoft Corporation, responsible for AI and Research. Dr. Shum re- ceived his Ph.D. in Robotics from School of Computer Science at Carnegie Mellon University. 53\n",
  "metadata": {
    "paper_id": "2301.04020v1",
    "downloaded_at": "2025-08-24T20:07:16.219419+00:00"
  },
  "processed_at": "2025-08-24T20:07:16.219435+00:00"
}