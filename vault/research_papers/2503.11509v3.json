{
  "content": "TikZero : Zero-Shot Text-Guided Graphics Program Synthesis Jonas Belouadi‚àóEddy Ilg‚Ä†Margret Keuper‚àó,‚Ä°Hideki Tanaka¬ßMasao Utiyama¬ß Raj Dabre¬ßSteffen Eger‚Ä†Simone Ponzetto‚àó University of Mannheim, Germany‚àóUniversity of Technology Nuremberg, Germany‚Ä† Max Planck Institute for Informatics, Saarland Informatics Campus, Germany‚Ä° National Institute of Information and Communications Technology, Japan¬ß jonas.belouadi@uni-mannheim.de Abstract Automatically synthesizing figures from text captions is a compelling capability. However, achieving high geomet- ric precision and editability requires representing figures as graphics programs in languages like Ti kZ, and aligned trainingdata(i.e.,graphicsprogramswithcaptions)remains scarce. Meanwhile, large amounts of unaligned graphics programsandcaptionedrasterimagesaremorereadilyavail- able. Wereconcilethesedisparatedatasourcesbypresenting TikZero,whichdecouplesgraphicsprogramgenerationfrom textunderstandingbyusingimagerepresentationsasaninter- mediary bridge. It enables independent training on graphics programs and captioned images and allows for zero-shot text-guidedgraphicsprogramsynthesisduringinference. We show that our method substantially outperforms baselines thatcanonlyoperatewithcaption-alignedgraphicsprograms. Furthermore, when leveraging caption-aligned graphics pro- gramsasacomplementarytrainingsignal, TikZeromatches orexceedstheperformanceofmuchlargermodels,including commercialsystemslike GPT-4o. Ourcode,datasets,and select models are publicly available. 1 1. Introduction Graphicsprogramminglanguagesofferdistinctadvantages overlow-levelvectorformats(PDF,SVG)orrasterimagefor- mats by representing visual concepts as high-level programs that preserve semantics, remain human-interpretable, and allow manual editing. These properties are particularly valu- able in academia, where specialized graphics programming languages like Ti kZ [1] are popular for creating complex fig- ureswithhighexpressivity. However,thiscomeswithasteep learningcurve,asseenontheT EXStackExchange 2(TEX.SE), wherenearly10%ofquestionsconcernTi kZandmakeitthe most frequently discussed topic on the platform [2, 3]. 1https://github.com/potamides/DeTikZify 2https://tex.stackexchange.comAutomaTi kZv2‚úó TikZero +‚úì Re(œâ)Im(œâ) Œì 0 11 0Œ≥ ‚àí505‚àí5 0 5‚àí0.4‚àí0.200.20.4 ‚àí5 0 5‚àí4‚àí2024050100 3D contour plot of a loss function. œÉ1 œÉ2 œÉ3 œÉ4 œÉ5 œÉ6 œÉ7 œÉ8 œÉ9 œÉ10œÉ1 œÉ2 œÉ3 œÉ4 œÉ5 œÉ6 œÉ7 œÉ8 œÉ9 œÉ10œÉ1 œÉ2 œÉ3 œÉ4 œÉ5 œÉ6 œÉ7 œÉ8 œÉ9 œÉ10œÉ1 œÉ2 œÉ3 œÉ4 œÉ5 œÉ6 œÉ7 œÉ8 œÉ9 œÉ10œÉ1 œÉ2 œÉ3 œÉ4 œÉ5 œÉ6 œÉ7 œÉ8 œÉ9 œÉ10 x y z h1 h2 h3 Àúx Àúy‚Ñì1 ‚Ñì2 ‚Ñì3 ‚Ñì4 ‚Ñì5 ‚Ñì6 ‚Ñì7 x y z h1 h2 h3 Àúx Àúy‚Ñì1 ‚Ñì2 ‚Ñì3 ‚Ñì4 ‚Ñì5 ‚Ñì6 ‚Ñì7 x y z h1 h2 h3 Àúx Àúy‚Ñì1 ‚Ñì2 ‚Ñì3 ‚Ñì4 ‚Ñì5 ‚Ñì6 ‚Ñì7 Input layerHidden layer 1Hidden layer 2Output layer Input 1 Input 2 Input 3 Input 4 Input 5Ouput x1 x2 xN1 n1 n y1 yNInput layerHidden layerHidden layerOutput layer A multi-layer perceptron with two hidden layers. 012345678910111213141516171819020406080100120140160180200 Time (s)Distance (m) xt 1‚àö 2œÄe‚àíx2 2xf(x) 1 SD‚àí2 ‚àí1 1 2xy Gaussian probability density function (blue) with markers showing one standard deviation (red). Figure 1. Qualitative comparison of our TikZero +model (last two columns) andthe end-to-end trained baseline AutomaTi kZv2 (LLM;firsttwocolumns)ontext-guidedgraphicsprogramsynthesis with TikZ. Our methodgeneratesoutputs that more closely follow the given captions. Example program listings are in Appendix F. WithrecentadvancesingenerativeAI,simplifyingthecre- ationofgraphicsprogramshasbecomeincreasinglyfeasible. Belouadi et al. [2]introduce DeTi kZify, an inverse graphics model that generates Ti kZ programs from images and hand- drawn sketches. However, creating these visual inputs stays cumbersome,motivatingalternativeinputmodalitiessuchas natural language. While Belouadi et al. [3]propose Auto- maTi kZ,atext-guidedsynthesismodelforTi kZprograms trainedend-to-endonanalignedcaption-programcorpus,its performance remains limited (cf. Fig. 1) [4, 5]. We identify insufficient training data as the primary limi- tation. Unlikeinversegraphicsmodelssuchas DeTi kZify, which are inherently self-supervised (trained by being condi- tionedoncompiledrepresentationsoftheiroutputprograms) and can access sufficient training data (cf. Fig. 2), end-to- 1arXiv:2503.11509v3 [cs.CL] 14 Aug 2025\nImages with CaptionsGraphics ProgramsAutomaTi kZ‚úó Limited to caption-aligned graphics programs only. TikZero ‚úì Leverages captioned images and graphics programs independently.DeTi kZify‚úó Accesses all graphics programs but with- out text guidance. Figure2. Illustrationoftrainingdataavailabilityforgraphicspro- gram synthesis. DeTi kZifycan leverage all graphics programs for training but lacks text guidance, while AutomaTi kZis constrained to the small intersection of captioned graphics programs, resulting in limited performance. Our approach, TikZero, trains indepen- dently onboth graphics programsand captioned images, enabling more effective use of available data and yielding superior results. end text-guided models like AutomaTi kZrequire graphics programs pairedwith captions, substantially reducing the available data pool (cf. Fig. 2). Toaddressthischallenge,wedecouplethegraphicspro- gram generation component from text understanding, en- ablingindependenttrainingongraphicsprogramsandcap- tioned images withoutrequiring paired data (cf. Fig. 2). Our approachfirsttrainsaninversegraphicsmodelconditioned onimagepatchembeddingsfromavisionencoder[ 6]. We then train an adapter network that generates synthetic image patch embeddings from captions. This adapter training re- lies solely on captioned images, effectively circumventing resourcelimitationsandenablingzero-shot(inthesensethat no aligned caption-program examples are involved in the training process) text-guided graphics program synthesis [ 7]. We demonstrate that this approach, to which we refer as TikZero, outperforms previous state-of-the-art methods (cf. Fig. 1). Our key contributions are: (i)Anoveltwo-stagearchitecture, TikZero,whichaddresses the low-resource challenge in text-guided graphics pro- gramsynthesisbyaligningrepresentationspacesrather than relying on aligned data. (ii)TheDaTi kZv3dataset,comprisingover450kTi kZgraph- icsprogramswithroughly170kcaptionedsamples. Using thisdataset, wetrainboth TikZeroandAutomaTi kZv2 (anupdatedversionof AutomaTi kZ)onthesamesource dataandshowthat TikZerooutperforms AutomaTi kZ, AutomaTi kZv2, and other end-to-end trained baselines. (iii)Anenhancedmodel, TikZero +,combining TikZerowith the end-to-end fine-tuning of AutomaTi kZv2, which surpasseslargerbaselinesandmatchestheperformance of commercial models like GPT-4o[8] on key metrics.2. Related Work Inverse Graphics Program Synthesis Inverse graphics, i.e.,synthesizingagraphicsprogramtoreproduceavisual target, represents a specialized instance of neural program synthesis [ 9‚Äì11]. Deep learning models have shown remark- able success in this domain [ 12‚Äì14], with Vision-Language Models ( VLMs) increasingly gaining prominence [ 2,15‚Äì 17]. While controlled experimental studies often rely on synthetic datasets [ 13,15,17‚Äì20], real-world applications typically leverage more complex and diverse human-created data [2,21‚Äì25], highlighting the importance of data avail- ability. In scientific contexts, Ti kZ has emerged as a popular choice due to its versatility, expressiveness, and widespread adoption in academic circles [ 2,21‚Äì23,25]. Although these approaches are not tailored to text-guided generation, we incorporate key elements from them into our approach. Text-Guided Graphics Program Synthesis Current text- guided approaches to graphics program synthesis remain limited,mainlybecauseofthescarcityofcaptionedgraphics programsoutlinedinSec.1,butalsobecauseofthedifficulty of generating synthetic data with human-like captions [ 26, 27]. Researchersinterestedinthiscapabilitycurrentlyrelyon theemergingcapabilitiesoflargecommercialmodelssuchas GPT-4o[8,28‚Äì30],whichraisesconcernsaboutaccessibility, reproducibility, and computational cost [ 31]. In contrast, related domains like vector graphics generation [ 24,32‚Äì35] andNL2Vis[36‚Äì40] have shown more progress. Similar to inverse graphics, these fields increasingly incorporate large language models ( LLMs) [24,36,40]. However, vector graphicsapproachestypicallygenerateonlylow-levelB√©zier curves,limitingoutputcomplexity[ 2,32,33],and NL2Vis focusesexclusivelyondatavisualizationwitharestrictedset ofvisualizationtypes[ 40]. Morecomplexapplications,such asgeneratingarbitraryscientificfiguresfromcaptionswith TikZ, remain underexplored‚Äîa gap we address in this work. Text-to-Image Generation TikZeroshares conceptual and architectural similarities with several text-to-image gen- erationmethods[ 41‚Äì45]. Rodriguezetal. [46,47]explore generative adversarial networks [48, 49] and diffusion mod- els [50,51] for scientific figure generation, but these ap- proaches are tied to raster images, which are not ideal for representing scientific figures. Ramesh et al. [43]propose a two-stagemodelwithindependentlytrainedprioranddecoder componentstogeneraterasterimagesfromtext. Although priornetworksresembleouradaptersandhavebeenusedwith inversegraphicsmodels[ 52],theytarget globalimageembed- dings containing only abstract information, which degrades performance whenused withinversegraphics architectures that workbest with patch-level details [53]. Incontrast, our adapters specifically operate on patch-level embeddings, and we demonstrate that this improves performance compared to end-to-end trained baselines. 2\nTikZero Adapter Network Text Encoder Self-AttnùëÅ... ... Self-Attn 1Cross-AttnùëÅùõæ... Cross-Attn 1ùõæ ProbeInverse Graphics Model Vision Encoder Self-AttnùëÅ... Self-Attn 1Text Decoder Self-Attn 1... Self-AttnùëÅ Caption Image Graphics Program Figure 3. Architecture overview of TikZeroduring inference. Solid lines represent the standard caption-conditioned path, which flows throughthetextencoderintotheadapternetworkof TikZerobeforeconnectingtothevisionencoder. Incertainconfigurations(cf.Sec.5.2), the captionalso feedsinto thetext decoder(depicted by dottedlines and‚Äú ‚Ä¢‚Äù markersrepresenting shortcuts). The self-and cross-attention layers (yellow) are simplified representations, omitting internal feed-forward layers and residual connections [ 54]. An exception is the explicit residual connection between the cross-attention and self-attention layers of the vision encoder, visualizing the gating mechanism ùõæ (purple). Additionally, the dashed path illustrates how the inverse graphics model generates graphics programs when conditioned on images. 3. The TikZero Model & Architecture Asthefoundationof TikZero,wefirstdevelopastate-of-the- art inverse graphics model for graphics program synthesis. Wethenincorporateacross-attentionadapternetwork[ 55] fortextguidance. Fig.3providesanoverviewofourmethod. The Inverse Graphics Model Duetotheirdemonstrated effectiveness(cf.Sec.2),weadopta VLMarchitectureforthe inversegraphicsmodelof TikZero. Figure3illustratesits innerworkings(dashedlines): themodelprocessesrasterized imagesandautoregressivelygeneratestheircorresponding programs without involving captions at this stage. The Adapter Network VLMsconsistoftwoprimarycom- ponents: a vision encoder that produces image embeddings andatextdecoderthat,inourcase,generatesgraphicspro- grams conditioned on these embeddings. The unidirectional andlocalizedflowofinformationbetweenthesecomponents allows us to inject additional information solely into the vision encoder, thereby influencing the output of the text decoder. We exploit this property by introducing a trainable, text-conditioned adapter network that mimics the outputs of theoriginalvisionencoder. Thiseffectivelyenableszero-shot generationofgraphicsprogramsconditionedontextwhenits outputsarefedintothedecoder. Inadditiontocircumventing the resource limitations discussed in Sec. 1, this architecture has other welcome implications: During adapter training, the text decoder, usually the largest component of the model, does not need to be loaded, resulting in efficient and fast training even with large datasets. Our adapter incorporates a lightweighttextencoderforembeddingcaptionsandintro-ducesnewly initializedgatedcross-attentionlayers [ 56,57] beforeeachvisionencoderlayer(cf.,Fig.3). Thekeysand values derive from the final text encoder representations, while the queries originate from a trainable probe used in- stead of image inputs. The gates ùõæallow the model to learn at which layers and to what extent information from the text encoder should flow into the vision encoder. Contrary to existing literature, which often employs tanhgates [58] that initialize to zero (indicating no information flow), we find that using sigmoidgates (0.5 at initialization) accelerates trainingconvergencesince,inourcase,onlylittleinformation originates from the vision encoder inputs (i.e., the probe). We ablate the gates and the probe in Appendix A. Training Objective Given a caption-image dataset for training,wefirstembedthepatches ùëù‚ààùíëofimageùëñusing theunmodifiedvisionencoder Mofour VLM.Subsequently, we incorporate the cross-attention adapter to obtain the modified encoder bM, which we then distill on these image patch embeddings conditioned solely on the caption ùë°and probe ÀÜùö§[59]. This leads to the following objective: Ldist=1 |ùíë|‚àëÔ∏Å ùëù‚ààùíëdist\u0010 MùúÉ(ùëù|ùëñ),bMùúÉ,ÀÜùúÉ(ùëù|ÀÜùö§,ùë°)\u0011 ,(1) where dist(ùíô,ùíö)represents a distance metric. Following common practices in model distillation [ 60,61], we exper- iment with cosine distance and mean squared error. Here, ùúÉdenotes the original model parameters that remain fully frozen,while ÀÜùúÉrepresentstheadapterparametersofwhich the cross-attention layers and the image probe are trainable. 3\nSource DaTi kZ DaTi kZv2DaTi kZv3 curated 981 1 566 3 646 TEX.SE 29 238 30 609 42 654 arXiv 85 656 326 450 407 851 artificial 1 957 1 958 2 256 all 117 832 360 583 456 469 Table 1. Breakdown of the number of unique Ti kZ graphics in DaTi kZv3comparedtoitspredecessors DaTi kZandDaTi kZv2. Qualitative examples can be found in Appendix F. 4. Datasets & Model Training We introduce DaTi kZv3, a novel dataset of Ti kZ graphics programs designed to support the training and evaluation of TikZero. Additionally,wetrain AutomaTi kZv2asadirectly comparable baseline operating on the same data source. TheDaTi kZv3Dataset DaTi kZv3expandsuponitspre- decessors DaTi kZandDaTi kZv2[2,3], incorporating programsfromcuratedrepositories,T EX.SE,arXivpapers, andartificialsamples(cf.Tab.1). Whilepreviousversions focused exclusively on Ti kZ graphics with (v1) or without (v2) captions, DaTi kZv3systematically extracts captions alongside Ti kZ graphics whenever possible to support our claims. Fromover450kinstances,fewerthan170kinclude captions, underscoring the challenges discussed in Sec. 1. Training TikZero TikZero‚ÄôsVLMbuilds upon De- TikZify[2] by conditioning a LLaMA-based text de- coder [62] on patch embeddings from a SigLIPvision en- coder[63]. Specifically,wecombine LLaMA 3.1(8B)[57] with SigLIP SoViT (0.4B). Unlike DeTi kZifyand inspired by the continued ViTpretraining approach of InternVL 1.5[64],weinitializethevisionencoderwithweightsfrom the fine-tuned encoder of PaliGemma [65] and increase the input resolution to 420√ó420pixels. Furthermore, we fully fine-tune the vision encoder alongside the rest of the model instead of freezing it. We train on DaTi kZv3for 5 epochs witha learning rate of 5e‚àí5and abatch size of 128. TikZero‚ÄôsVLMconsistently outperforms DeTi kZify, with detailed evaluation results provided in Appendix B. For the adapter network, we initialize with LLaMA 3.2(1B) as the textencoder[ 57]andleverage ArxivCap [26],adatasetcom- prising6.4millionscientificcaption-imagepairsfortraining. The adapter accounts for 2 billion of TikZero‚Äôs 10 billion total parameters, with only 400 million being trainable. We trainfor3epochswithalearningrateof 1e‚àí4andabatchsize of 512. We emphasize that this two-stage training process does notaccess caption-program pairs. However, we demon- strate that incorporating such aligned data in a subsequent fine-tuning step (Sec. 5.2) further enhances performance.Training AutomaTi kZv2Similartoitspredecessor, Au- tomaTi kZv2isatoken-conditioned LLMthatusestokenized captions as conditioning information for graphics prediction (rather than patch embeddings). We initialize Automa- TikZv2in two different ways: (i) AutomaTi kZv2(LLM), whichstartsfromvanilla LLaMA 3.1(8B)weights,and(ii) AutomaTi kZv2(VLM),which leverages TikZero‚Äôstrained VLM(minus the vision encoder) to benefit from transfer learning [ 66] on its larger training corpus. Both variants employ the same hyperparameters as TikZero‚ÄôsVLMbut canonlyutilizethecaption-annotatedsubsetof DaTi kZv3for training. Despite having access to less caption-aligned data thanTikZero‚Äôs adapter network, AutomaTi kZv2requires a longertrainingperiodprimarilyduetofine-tuningthelarge decoder (8 billion trainable parameters versus the adapter network‚Äôs 400 million). Training requires more than two days for AutomaTi kZv2and 1.5 days for TikZero‚Äôs adapter network when using eight Nvidia A100 40GB GPUs. 5. Experiments Before training models on DaTi kZv3, we extract 1k samples fromitscaptionedsubsettoformourtestset. Tomitigatedata leakagefrompretrainingtotesting,weonlyincludeinstances created after the cut-off date specified by LLaMA 3.2and ArxivCap . Wealsoemployan ùëõ-grammatchingalgorithm to avoid cross-contamination with our training split [ 8]. For all models, the temperature is set to 0.8 and top-p to 0.95. Example outputs are provided in Fig. 1 and Appendix F. Evaluation Metrics The multimodal nature of our task allowsforvariousevaluationmetricsinourautomaticeval- uations. We assess perceptual image similarity between generatedoutputsandreferencesbycomputing DreamSim (DSim)[67,68],whichcorrelateshighlywithhumanjudg- ments for scientific images [ 2]. We also calculate the Kernel Inception Distance ( KID) [69] using SigLIPimage fea- tures,whichevaluatestheoverallqualityofgeneratedfigures by comparing to the distribution of reference figures. We evaluatecaptionsimilarity betweengeneratedoutputsandref- erence captions using CLIPScore (CLIP) [70] with SigLIP features. To measure code similarity between generated and reference Ti kZ programs, we use CrystalBLEU (cBLEU), aBLEUvariant optimized for code evaluation [ 71,72], and TEX Edit Distance ( TED) [2], a variant of the Extended Edit Distance [ 73] utilizing a T EX tokenizer. Since some metrics require that generated programs compile to images, resamplingisnecessaryiftheoutput containsirrecoverable errors. To quantify this, we compute the Mean Token Effi- ciency(MTE),definedasthe10%winsorizedmeanofthe ratiobetweenthenumberoftokensinthefinalTi kZprogram and the total number of tokens generated to produce that program. Fora comprehensiveviewofmodelperformance, we calculate the arithmetic mean ( AVG) ofallprevious 4\nOriginal Text Redacted Text Models DSim ‚ÜëKID ‚ÜìCLIP ‚ÜëcBLEU ‚ÜëTED ‚ÜìMTE ‚ÜëAVG ‚ÜëCLIP ‚ÜëRatio ‚Üë IDEFICS 3 (8B) 45.475 11.426 14.327 0.656 63.175 69.558 66.628 4.851 33.858 AutomaTi kZ(13B) 46.033 1.294 3.955 0.386 62.24 85.866 63.093 2.965 74.975 AutomaTi kZv2(VLM)38.313 33.203 0.775 0.328 76.985 21.595 0.0 0.284 36.597 AutomaTi kZv2(LLM)50.548 3.491 15.766 0.658 62.307 81.775 82.375 8.002 50.753 TikZero(MSE) 52.024 5.664 10.583 1.723 66.07 79.318 85.004 8.237 77.831 TikZero(Cos) 52.829 5.103 10.051 1.603 65.51 82.291 85.599 7.226 71.893 Table 2. System-level scores√ó100forTikZeroand baselines of comparable size and training setup. Bold and underlined values denote the best and second-best scores for each metric column, respectively. Cell shading illustrates relative score magnitudes. Arrows indicate metric directionality. Overall, TikZeroachieves the strongest average performance across metrics. metrics. As these metrics operate on different scales, we apply min-max normalization before computing the average. Additionally, some metrics are recomputed with redacted text in the outputs as part of our analysis, cf. Sec. 6.1. 5.1. Comparison against End-to-End Fine-Tuning In our initial experiment, we evaluate the zero-shot perfor- manceof TikZero,trainedasdescribedinSecs.3&4using eithercosinedistance( Cos)ormeansquarederror( MSE), and compare it against end-to-end trained baselines. Baselines Besides AutomaTi kZv2(LLM&VLM), which we designed as directly comparable baselines, we assess othertoken-conditionedmodelsofsimilarandslightlylarger sizestrainedonTi kZ.Specifically,weevaluate AutomaTi kZ (13B),3the strongest original AutomaTi kZbaseline [ 3], and thegeneral-purposechatbot IDEFICS 3 (8B)[22]. Additional models and details are available in Appendices A & C. Results Wepresentthesystem-levelmetricscoresinTab.2 (Original Text). On average, TikZero, trained with cosine distance,achievesthebestperformancewithanAVGscore of 85.599, closely followed by the MSEvariant at 85.004. The next best model, AutomaTi kZv2(LLM), scores 82.375, whichis3percentagepoints(pp)lower. Theremainingmod- elsexhibitasubstantialperformancegap,with IDEFICS 3 (8B)and AutomaTi kZ(13B)fallingbehindbyapproximately 20ppand AutomaTi kZv2(VLM)showingtheweakest per- formance across all metrics, resulting in an AVG score of 0. Thesurprisinglypoorresultsof AutomaTi kZv2(VLM) are likely due to catastrophic forgetting [ 74], as the removal of the vision encoder from TikZero‚ÄôsVLMnecessitates reacquisition of conditioning based solely on text. Asforindividualmetrics,ouradapter-basedmodelsper- form particularly well in perceptual image similarity, with TikZero(Cos) outperforming the best baseline, Automa- TikZv2(LLM),by3ppon DreamSim . Although Automa- TikZv2(LLM) outperforms TikZeroby 1.5ppon KID, this 3Belouadi et al. [3] refer to this model as CLiMA(13B).indicates in this context that such token-conditioned models (compared to those using patch embeddings) capture the general appearance of scientific figures well but fall short in inferringvisualspecificsfromcaptions. Theydo,however, have an edge in reproducing text from captions, which we identify as the primary reason for up to 5pp higher CLIP- Score, as noted in Sec. 6.1. Regarding code similarity, both TikZeromodels considerably outperform others on cBLEU. Interestingly, we observe a mild inverse correlation between cBLEUandTED. Models conditioned solely on tokenized captions tend to generate shorter, often simplified programs [ 3], potentially resulting in a reduced edit distance to the reference. In terms of efficiency, all models achieve anMTEof80‚Äì85,indicatingthatonly2outof10inferences require resampling. TikZero(Cos) is 3pp more efficient thanMSE,while AutomaTi kZ(13B),likelybenefitingfrom its larger model size, exceeds it by another 3pp. In summary, training AutomaTi kZv2on top of a VLM yields worse performance than training based on vanilla LLaMA 3.1, indicating that effective end-to-end training can onlyleveragethesmallintersectionofgraphicsprogramsand images with captions, as illustrated in Fig. 2. However, even withoutaccesstothisintersection, TikZerosurpassesboth AutomaTi kZ(13B) and AutomaTi kZv2(LLM) on average bybeingabletotrainonimageswithcaptionsindependently ofgraphicsprograms. Moreover,usingalossfunctionbased on cosine distance proves more effective than using MSE. 5.2. Combining Adapters with Fine-Tuning Inthissection,weinvestigatewhetherexplicitlyincorporat- ing the subset of DaTi kZv3that includes captions into the training process of TikZeroenhances performance. Our approachinvolvesthreeincrementalstages: (i)Weperform a light fine-tuning of TikZero(Cos) end-to-end on caption- programpairsforoneepochwithalowlearningrateof 1e‚àí5. Extending the training duration or increasing the learning rate does not yield further performance gains, likely due to the decoder having already reached its saturation point; 5\nOriginal Text Redacted Text Models DSim ‚ÜëKID ‚ÜìCLIP ‚ÜëcBLEU ‚ÜëTED ‚ÜìMTE ‚ÜëAVG ‚ÜëCLIP ‚ÜëRatio ‚Üë Qwen 2.5Coder(32B) 54.473 5.493 24.87 0.285 59.856 97.269 48.593 12.164 48.911 GPT-4o 56.464 2.844 31.787 0.327 58.511 97.675 79.019 13.32 41.905 TikZero(Cos) 52.829 5.103 10.051 1.603 65.51 82.291 14.658 7.226 71.893 +Fine-tuning (i) 53.203 1.794 10.687 0.759 61.572 94.851 46.497 6.512 60.931 +Separate Captions (ii) 52.983 2.905 15.72 0.804 61.32 95.722 46.326 8.741 55.608 +Weight Resetting (iii) 56.295 1.831 24.177 1.988 59.008 93.058 87.043 11.479 47.478 Table 3. System-level scores√ó100for additional baselines and TikZerocombined with fine-tuning and token-conditioning. The scores for TikZero(Cos)arereplicatedfromTab.2forconvenience. Boldandunderlinedvaluesdenotethebestandsecond-bestscoresforeachmetric column, respectively. Cell shading illustrates relative score magnitudes. Arrows indicate metric directionality. Overall, TikZero(Cos) with Weight Resetting (iii) demonstrates the strongest average performance across metrics. (ii) Alongside feeding captions into the adapter, we provide them separately to the text decoder in tokenized form (cf., Fig. 3); (iii) Prior to fine-tuning, we reset the decoder to its initial weights to overcome saturation, enabling us to fine-tune using the setup described in Sec. 4, which involves 5 epochs and a learning rate of 5e‚àí5. Baselines In addition to the baselines in Tab. 2, which remain comparable, we also evaluate larger and commercial models that serve as stronger baselines (cf. Appendix C). Specifically, we assess GPT-4o[8], which has demonstrated strong performance in generating Ti kZ [3,28,30] and Qwen 2.5Coder(32B) [75] as an open-weights model. Results In Tab. 3 (Original Text), all fine-tuning setups ofTikZeroshowconsiderableimprovementoverthebase version. Approaches(i)and(ii)eachenhanceperformance byover30pponAVG,whileapproach(iii)surpassesthem with an improvement of over 70pp, positioning it as the best-performing model on average, even when compared to our new baselines, with GPT-4obeing 8pp lower and Qwen 2.5Coder(32B)approximately40pplower. Approach (i)demonstratesthatdirectfine-tuningyieldspositiveeffects acrossnearlyallmetrics,notablyimproving MTEby12pp, TEDby4pp,and KIDby3.5pp. Approach(ii)showssimilar trends but, by also incorporating tokenized captions, further improves CLIPScore by 5pp, closing the gap to Automa- TikZv2(LLM).Interestingly,both(i)and(ii)slightlydecrease performanceon cBLEU,potentiallyduetosimilarreasons discussed in Sec. 5.1. However, the same cannot be said for (iii), which not only achieves the highest score on cBLEU butalsoranksasthesecond-beston TED,trailingonly0.5pp behind GPT-4oandshowcasingthatitispossibletoperform wellonbothmetrics. Additionally,itincreases DreamSim byanother3ppand CLIPScore by8.5pp,competingwiththe muchstrongerbaselines Qwen 2.5Coder(32B)and GPT-4o. InKID,itevensurpassesthemby3.5ppand1pp,respectively. In summary, fine-tuning TikZero, especially when com-bined with a separate caption input and weight resetting, greatly improves performance. This illustrates that the in- tersection of graphics programs and images with captions, though small, provides a valuable training signal, and best performancecanbeachievedbymakingfulluseofbothsets. The best-performing TikZeromodel even competes with andoftensurpasses Qwen 2.5Coder(32B)and GPT-4oon several key metrics. Notably, the former model is more than three times larger, and the latter is often estimated at around 1.8 trillion parameters [76], making it 180 times larger. 5.3. Human Evaluation Tocorroborateourfindingsfromautomaticevaluation,we conduct ahuman annotationcampaign focusingon twokey properties: captionandimagesimilarity. Weemploy Best- Worst Scaling (BWS) [77], a comparative annotation method that yields high-quality results even with few annotators [ 78, 79]. Wesample100instancesfromourtestsetandpresent annotators with ùëõ-tuples of generated figures, asking them to identify the most and least similar figure to either the reference caption or reference image. This data is then transformed into scores from -1 (poor) to 1 (excellent) by subtracting the proportion of times a figure is selected as the best from the proportion of times it is chosen as the worst[80]. Foramanageableworkload,wefocuson ùëõ=4 key models: TikZero(Cos), our best-performing model from Sec. 5.1; AutomaTi kZv2(LLM), its direct end-to-end trained competitor; GPT-4o, our strongest baseline; and TikZero(Cos) fine-tuned using approach (iii) from Sec. 5.2, ourbestmodeloverall,henceforthreferredtoas TikZero +for convenience. Weengagethirteenannotatorsandobtainsix fullyannotatedsetspertask(cf.AppendixEformoredetails). Toassessannotatorconsistency,wecalculatethe split-half reliability (SHR) [79]. This method randomly divides all annotations into two sets, calculates scores independently, and then determines their correlation using Spearman‚Äôs ùúå. 6\n\u00001¬ï0\u00000¬ï5 0¬ï0 0¬ï5 1¬ï0 Caption Similarity\u00001¬ï00\u00000¬ï75\u00000¬ï50\u00000¬ï250¬ï000¬ï250¬ï500¬ï751¬ï00Image SimilarityT/i.pc/k.pcZ/e.pc/r.pc/o.pc(Cos) T/i.pc/k.pcZ/e.pc/r.pc/o.pc+ GPT-4/o.pc A/u.pc/t.pc/o.pc/m.pc/a.pcT/i.pc /k.pcZ/v.pc2(LLM)Figure4. Bivariatedistributionsof BWSscores(higherisbetter) using kernel density estimation for caption and image similarity. Along the diagonal, TikZero(Cos) achieves higher scores than AutomaTi kZv2(LLM), while TikZero +andGPT-4odemonstrate superior performance compared to both. Results Fig. 4 presents kernel density estimates for the BWSscores, showing generally consistent rankings with automaticevaluationsbutrevealingnotabledifferencesinthe magnitudeofgaps. Forcaptionsimilarity,therankingaligns withCLIPScore evaluations ( ùúå=1.0), with TikZero(Cos), AutomaTi kZv2(LLM),TikZero +,and GPT-4oachieving mean scores ùúáof -0.25, -0.18, 0.03, and 0.4, respectively. Interestingly,humansperceivea40%smallergapbetween AutomaTi kZv2(LLM)and TikZero(Cos)thansuggested byCLIPScore values,where AutomaTi kZv2(LLM)outper- forms TikZero(Cos)by50%. Thisindicateshumansmay evaluatecaptionsimilaritydifferentlythan CLIPScore (cf. Sec. 6.1). For image similarity, the system order remains consistent with our DreamSim metric (ùúå=1.0), with Auto- maTi kZv2(LLM),TikZero(Cos), TikZero +,and GPT-4o achievingùúáof -0.26, -0.02, 0.01, and 0.27, respectively. However,therelativegapsbetweenmodelsdiffer: thesepa- rationbetween AutomaTi kZv2(LLM)and TikZero(Cos), as well as between TikZero +andGPT-4o, appear more pronounced than observed with DreamSim . This discrep- ancylikelystemsfrom BWScapturingrelativepreferences rather than absolute performance differences. GPT-4ois selected20%moreoftenasthebestmodelthan TikZero +, andAutomaTi kZv2(LLM) 15% more often as the worst model than TikZero(Cos), creating larger perceived gaps even when qualitative differences may be subtle. TheSHRvaluesof0.68forcaptionsimilarityand0.76for image similarity indicate moderate to strong inter-annotatoragreement. We also observe a correlation between these two tasks,withsegment-level ùúå=0.62andsystem-level ùúå=0.8, suggestingthatbothevaluationdimensionscapturerelated aspects of model performance. GPT-4oemerges as the best- performingmodel,aligningwithitssuperiorperformanceon the corresponding automatic metrics, CLIPScore andDSim. Among open-source models, TikZero +performs best, while AutomaTi kZv2(LLM) ranks lowest overall. 6. Analysis We present a comprehensive analysis, investigating the influ- enceoftypographicattackson CLIPScore andexamining the effectiveness of our architecture in low-resource settings, both in terms of training data and trainable parameters. 6.1.CLIPScore Limitations & Typographic Attacks Aknownlimitationof CLIPScore withtext-richimagesisits susceptibilitytotypographicattacks,wherescoresaredispro- portionately influencedby stringsimilarity between images and captions [ 3,43]. We suspect that token-conditioned models like AutomaTi kZv2(LLM) achieve higher CLIP- Scorevalues than models such as TikZero(Cos&MSE) primarily because they tend to visibly copy more substrings from the caption in the output image. To test this hypothesis, we apply the ROT13 substitution cipher [ 81] to all visible stringsinthegeneratedfiguresandrecompute CLIPScore . Thisbasiccipherreplaceseachletterwiththe13thletterafter it in the Latin alphabet. While not cryptographically secure, theratiobetweentheoriginalandrecomputed CLIPScore values shouldindicate theinfluence ofstring matching,i.e., higher ratios suggest less copied text and vice versa. Tabs.2&3(RedactedText)presenttherecomputed CLIP- Scorevaluesandratiosforallevaluatedmodels. Theresults revealthatmost TikZeromodels,exceptforfine-tuningap- proaches(ii)and(iii),whichalsoconditionontokenizedcap- tions, have considerably higher ratios (61%‚Äì78%) compared tostrictlytoken-conditionedmodels(34%‚Äì51%),supporting ourhypothesis. AutomaTi kZ(13B)isanexception,possibly due to its initially low score. Further analysis shows that withredactedtext, AutomaTi kZv2(LLM)‚ÄôsCLIPScore per- formance drops to the same level as TikZero(Cos&MSE), suggesting that string matching is the primary factor in its superiorperformance ratherthanproducingbetter visuals‚Äî arguably a more difficult task. Nevertheless, reproducing strings is still somewhat desirable. The human oracle of ourtestsetachievesaratioof50.8%,closetothe47.5%of TikZero +,ourbest-performingmodel. Incontrast,models likeGPT-4o, with a lower ratio of 41.9%, may overfit to caption copying, artificially inflating the CLIPScore values. 6.2. Low-Resource Training Whileouradapterstrainefficientlyonlarge-scaledatasets,we investigate whether such extensive data is necessary for opti- 7\nTraining Data Intv.100% 50% 25% 12.5% 1 92.411 77.478 49.967 56.055 2 87.557 85.249 54.942 33.817 4 82.254 47.381 32.12 37.914 8 76.545 40.816 29.774 16.25 Table4. AVGscoresfor TikZero(Cos)trainedonvaryingfractions ofdataandintervalsofcross-attentionlayers. Higherscoresindicate betterperformance. Boldandunderlinedvaluesdenotethebestand second-bestscoresforthewholetable,respectively. Cellshading illustrates score magnitudes. mal performance. Along the same vein, we examine the im- pactofreducingtheamountofcross-attentionlayersinserted intothevisionencoder. Weretrain TikZero(Cos)usingvary- ingfractionsofthetrainingdata( ùëë‚àà{1,1 2,1 4,1 8})andinsert cross-attentionlayersatdifferentintervals( ùëñ‚àà{1,2,4,8}). Table 4 presents the AVG scores from this parameter grid, with detailed scores in Appendix D. Our findings reveal that utilizingthefulldatasetandinsertingcross-attentionatevery layer yields the highest average performance, highlighting the benefits of maximizing both variables. Interestingly, the model‚Äôs performance appears more robust to a reduction in the number of layers compared to a decrease in training data. For instance, training on only1 8th of the data leads to a substantial performance drop of 36pp, whereas insert- ingcross-attentionlayersevery8layers(resultinginonly3 cross-attention layers in total) causes a more modest decline of 16pp. Minimizing both variables leads to the most severe dropofover75pp. Theseresultsvalidateourtrainingsetup while suggesting that incorporating additional data might furtherenhanceperformance. Giventhat ArxivCap extracts figures from only 572k papers, whereas some corpora index over 200 million papers [ 82], there remains a lot of potential for leveraging larger datasets in future work. 7. Conclusion Inthiswork,wedemonstratethepotentialof TikZeroandits variantsforgeneratingTi kZgraphicsprogramsfromcaptions. Notably, TikZerodoes not require aligned caption-program pairsinitsoriginalformulationbutinsteadalignsrepresenta- tion spaces of unaligned graphics programs and captioned images. This enables our model to leverage substantially moretrainingdatacomparedtoend-to-endtrainedmodels thatoperatesolelyoncaption-imagepairs(cf.Fig.2)while maintainingtrainingefficiency. TikZerooutperformsstrong end-to-end trained baselines, including our independently trained AutomaTi kZv2models, which use the same data pool,excludinginstancestheycannotprocess,illustratingthe strengths of our approach. When extending the TikZeroap-proach with additional end-to-end training, it also compares favorablytomuchlargerbaselinesandcommercialsystems likeGPT-4o. Whilethisenhancedapproach, TikZero +,is nolongerzero-shotbyourdefinition,itremainsa TikZero modelinthesensethatitoperatesonbothsetsofgraphics programsandcaptionedimages,withtheaddedadvantage of explicitly utilizing their intersection (cf. Fig. 2). These results demonstrate the benefits of designing archi- tecturesaroundavailabledataandvalidatetheapproachofde- coupling graphics program generation from text understand- ing (with optional later reconciliation through TikZero +). AlthoughwedemonstrateourmethodspecificallyonTi kZ, we believe its general principleswill inspire future work on related graphics program synthesis tasks. Future Work Beyond scaling up our training data to ex- ploreconvergencelimits(cf.Sec.6.2),weplantoinvestigate automatic methods for improving the quality and alignment of caption-image or caption-program pairs. This includes rewriting potentially noisy captions with LLMs and en- hancingthemwiththevisualunderstandingcapabilitiesof VLMs [83‚Äì85]. We believe our approach to aligning textual andimagemodalitiesenablesotherpromisingapplicationsfor graphics program synthesis, such as editing images in latent space viatextual instructionsto generatemodified graphics programs. Additionally, we intend to explore alternative alignment strategies beyond model distillation, including contrastive learning [ 86], which has successfully aligned modalities in discriminative models [6, 87, 88]. Limitations Our evaluations include proprietary systems that operate as black boxes; their training data is unknown, and they offer no guarantees of consistent performance over time. This (i)makesaddressingdataleakageandcross-contamination impossible and (ii) limits the fairness and reproducibility of ourexperiments. Nevertheless,evenundertheseunfavorable conditions, our open models remain competitive. Users should be aware, however, that our models may behave unpredictably, and outputs might differ from expectations. Additionally,ourmodelsdonotincludesafeguardsagainst potential misuse, e.g., for generating fake scientific content. Regardinglicensingofourtrainingdata,alargeportionof theTikZprogramsin DaTi kZv3arelicensedunderpermissive terms 4that allow redistribution. The remaining programs are distributed under the arXiv.org perpetual, non-exclusive license, which prohibits redistribution, which is why we excludethemfromthepublicreleaseof DaTi kZv3. However, sincewe releaseourdataset creationscripts,we encourage others to reproduce the full version independently. 4https : creativecommons . org licenses ;https : / opensource.org/license/mit ;https://www.gnu.org/licenses/ fdl-1.3.en.html ;https://openai.com/policies/terms-of-use 8\nAcknowledgments We extend our sincere gratitude to the following individuals (in no particular order) for their valuable contributions: Christian Greisinger, Hour Kaing, Ran Zhang, Tejaswini Medi, Yanran Chen, Sotaro Takeshita, Katharina Prasse, JiWoo Kim, Christoph Leiter, Haiyue Song, and Aida Kostikova. Their assistance with our human evaluation cam- paign, proofreading, insightful discussions, and constructive feedback has been instrumental to our work. The first author conducted part of this research during an internship at the National Institute of Information and Communications Technology (NICT), Japan. The second to last author is supported by the Federal Ministry of Education and Research (BMBF) via the research grant Metrics4NLG and the German Research Foundation (DFG) via the Heisenberg GrantEG375/5‚Äì1. Weacknowledgecomputingresources providedbythestateofBaden-W√ºrttembergthroughbwHPC and the German Research Foundation (DFG) through grant INST 35/1597‚Äì1 FUGG. Finally, we thank the OpenMoji project for the open-source icons used throughout this work. References [1] Till Tantau. The TikZ and PGF Packages , 2023. 1 [2]Jonas Belouadi, Simone Paolo Ponzetto, and Steffen Eger. DeTikZify: Synthesizing graphics programs for scientific figuresandsketcheswithTikZ. In TheThirty-eighthAnnual Conference on Neural Information Processing Systems , 2024. 1, 2, 4, 15 [3]Jonas Belouadi, Anne Lauscher, and Steffen Eger. Au- tomaTikZ: Text-guided synthesis of scientific vector graphics withTikZ. In TheTwelfthInternationalConferenceonLearn- ing Representations , 2024. 1, 4, 5, 6, 7, 16, 18 [4]Leixin Zhang, Yinjie Cheng, Weihe Zhai, Steffen Eger, Jonas Belouadi, Fahimeh Moafian, and Zhixue Zhao. ScImage: How good are multimodal large language models at scientific text-to-image generation? In The Thirteenth International Conference on Learning Representations , 2025. 1, 15 [5]Abhay Zala, Han Lin, Jaemin Cho, and Mohit Bansal. Dia- grammerGPT:Generatingopen-domain,open-platformdia- gramsviaLLMplanning. In FirstConferenceonLanguage Modeling , 2024. 1 [6]AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,PamelaMishkin,JackClark,GretchenKrueger,and Ilya Sutskever. Learning transferable visual models from naturallanguagesupervision. In Proceedingsofthe38thInter- national Conference on Machine Learning , pages 8748‚Äì8763. PMLR, 2021. 2, 8 [7]Mark Palatucci, Dean Pomerleau, Geoffrey E Hinton, and Tom M Mitchell. Zero-shot learning with semantic output codes. In AdvancesinNeuralInformationProcessingSystems . Curran Associates, Inc., 2009. 2 [8] OpenAI. GPT-4 technical report, 2023. 2, 4, 6 [9]Emilio Parisotto, Abdel rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet Kohli. Neuro-symbolic program synthesis. In International Conference on Learning Representations , 2017. 2 [10]JacobDevlin,JonathanUesato,SuryaBhupatiraju,Rishabh Singh,AbdelrahmanMohamed,andPushmeetKohli. Robust- Fill: NeuralprogramlearningundernoisyI/O. In Proceedings of the 34th International Conference on Machine Learning , pages 990‚Äì998. PMLR, 2017. [11]KevinEllis,CatherineWong,MaxwellNye,MathiasSabl√©- Meyer, Lucas Morales, Luke Hewitt, Luc Cary, Armando Solar-Lezama, and Joshua B. Tenenbaum. DreamCoder: bootstrappinginductiveprogramsynthesiswithwake-sleep library learning. In Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation , page 835‚Äì850, New York, NY, USA, 2021. Association for Computing Machinery. 2 [12]YaroslavGanin,TejasKulkarni,IgorBabuschkin,S.M.Ali Eslami, andOriolVinyals. Synthesizingprogramsforimages usingreinforcedadversariallearning. In Proceedingsofthe 35thInternationalConferenceonMachineLearning ,pages 1666‚Äì1675. PMLR, 2018. 2 [13]Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and JoshTenenbaum. Learningtoinfergraphicsprogramsfrom hand-drawnimages. In Thirty-secondConferenceonNeural Information Processing Systems , pages 6062‚Äì6071, 2018. 2 [14]KevinEllis,MaxwellNye,YewenPu, FelixSosa,JoshTenen- baum, and Armando Solar-Lezama. Write, execute, assess: Program synthesis with a REPL. In Advances in Neural In- formation Processing Systems . Curran Associates, Inc., 2019. 2 [15]Peter Kulits, Haiwen Feng, Weiyang Liu, Victoria Fernandez Abrevaya,andMichaelJ.Black. Re-thinkinginversegraph- ics with large language models. Transactions on Machine Learning Research , 2024. 2 [16]Wen-Ding Li and Kevin Ellis. Is programming by example solved by LLMs? In The Thirty-eighth Annual Conference on Neural Information Processing Systems , 2024. [17]Shreyas Kapur, Erik Jenner, and Stuart Russell. Diffusion on syntax trees for program synthesis. In The Thirteenth International Conference on Learning Representations , 2025. 2 [18]GopalSharma,RishabhGoyal,DifanLiu,EvangelosKaloger- akis,andSubhransuMaji. CSGNet: Neuralshapeparserfor constructive solid geometry. In 2018 IEEE Conference on ComputerVisionandPatternRecognition,CVPR2018,Salt Lake City, UT, USA, June 18-22, 2018 , pages 5515‚Äì5523. ComputerVisionFoundation/IEEEComputerSociety,2018. [19]Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis, William T. Freeman, Joshua B. Tenenbaum, and Jiajun Wu. Learning to infer and execute 3D shape programs. In Interna- tional Conference on Learning Representations , 2019. [20]Javier C√°mara, Javier Troya, Lola Burgue√±o, and Antonio Vallecillo. On the assessment of generative AI in modeling tasks: an experience report with chatgpt and UML. Softw. Syst. Model. , 22(3):781‚Äì793, 2023. 2 [21]HugoLauren√ßon,LeoTronchon,MatthieuCord,andVictor Sanh. What matters when building vision-language models? InTheThirty-eighthAnnualConferenceonNeuralInformation Processing Systems , 2024. 2 9\n[22]Hugo Lauren√ßon, Andr√©s Marafioti, Victor Sanh, and L√©o Tronchon. Buildingandbetterunderstandingvision-language models: insights and future directions, 2024. 5 [23]ShengbangTong,EllisLBrownII,PenghaoWu,Sanghyun Woo, Adithya Jairam Iyer, Sai Charitha Akula, Shusheng Yang,JihanYang,ManojMiddepogu,ZitengWang,Xichen Pan,RobFergus,YannLeCun,andSainingXie. Cambrian- 1: A fully open, vision-centric exploration of multimodal LLMs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems , 2024. 2 [24]Juan A. Rodriguez, Abhay Puri, Shubham Agarwal, Issam H. Laradji,PauRodriguez,SaiRajeswar,DavidVazquez,Christo- pherPal,andMarcoPedersoli. StarVector: Generatingscal- ablevectorgraphicscodefromimagesandtext.In Proceedings ofthe ComputerVisionand PatternRecognition Conference (CVPR), pages 16175‚Äì16186, 2025. 2 [25]Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang,YanghaoLi,SamDodge,KeenYou,ZhenYang,Alek- sei Timofeev, Mingze Xu, Hong-You Chen, Jean-Philippe Fauconnier,ZhengfengLai,HaoxuanYou,ZiruiWang,Afshin Dehghan, Peter Grasch, and Yinfei Yang. MM1.5: Methods, analysis&insightsfrommultimodalLLMfine-tuning. In The Thirteenth International Conference on Learning Representa- tions, 2025. 2 [26] LeiLi, Yuqi Wang, RunxinXu, Peiyi Wang, XiachongFeng, LingpengKong,andQiLiu. MultimodalArXiv: Adatasetfor improving scientific comprehension of large vision-language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 14369‚Äì14387, Bangkok, Thailand, 2024. Association for Computational Linguistics. 2, 4 [27]JaeyoungKim,JonghoLee,Hong-JunChoi,Ting-YaoHsu, Chieh-Yang Huang, Sungchul Kim, Ryan Rossi, Tong Yu, ClydeLeeGiles,Ting-Hao‚ÄòKenneth‚ÄôHuang,andSungchul Choi.Multi-LLMcollaborativecaptiongenerationinscientific documents. In AIforResearchandScalable,EfficientSystems , pages 142‚Äì160, Singapore, 2025. Springer Nature Singapore. 2 [28]S√©bastienBubeck,VarunChandrasekaran,RonenEldan,Jo- hannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee,YuanzhiLi,ScottLundberg,HarshaNori,HamidPalangi, MarcoTulioRibeiro,andYiZhang. Sparksofartificialgen- eral intelligence: Early experiments with GPT-4, 2023. 2, 6 [29]Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, and Antonio Torralba. A vision check-up for languagemodels.In ProceedingsoftheIEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR) , pages 14410‚Äì14419, 2024. [30]Tianjun Zhang, Yi Zhang, Vibhav Vineet, Neel Joshi, and XinWang. Controllabletext-to-imagegenerationwithGPT-4, 2023. 2, 6 [31]Lingjiao Chen, Matei Zaharia, and James Zou. How Is ChatGPT‚Äôs Behavior Changing Over Time? Harvard Data Science Review , 6(2), 2024. https://hdsr.mitpress.mit.edu/pub/y95zitmz. 2[32]SagiPolaczek,YuvalAlaluf,EladRichardson,YaelVinker, andDanielCohen-Or. NeuralSVG:Animplicitrepresentation for text-to-vector generation, 2025. 2 [33]RonghuanWu,WanchaoSu,KedeMa,andJingLiao. Icon- Shop: Text-guided vector icon synthesis with autoregressive transformers. ACM Trans. Graph. , 42(6), 2023. 2 [34]Ajay Jain, Amber Xie, and Pieter Abbeel. VectorFusion: Text-to-SVGbyabstractingpixel-baseddiffusionmodels. In ProceedingsoftheIEEE/CVFConferenceonComputerVision and Pattern Recognition (CVPR) , pages 1911‚Äì1920, 2023. [35]Kevin Frans, Lisa B. Soros, and Olaf Witkowski. CLIPDraw: Exploring text-to-drawing synthesis through language-image encoders. In NeurIPS, 2022. 2 [36]Henrik Voigt, Kai Lawonn, and Sina Zarrie√ü. Plots made quickly: Anefficientapproachforgeneratingvisualizations from natural language queries. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024) , pages12787‚Äì12793,Torino,Italia,2024.ELRAandICCL. 2 [37]Yuyu Luo, Nan Tang, Guoliang Li, Chengliang Chai, Wenbo Li,andXuediQin. Synthesizingnaturallanguagetovisual- ization(nl2vis)benchmarksfromNL2SQLbenchmarks. In Proceedingsofthe2021InternationalConferenceonManage- mentofData ,page1235‚Äì1247,NewYork,NY,USA,2021. Association for Computing Machinery. [38]JockMackinlay. Automatingthedesignofgraphicalpresen- tations of relational information. ACM Trans. Graph. , 5(2): 110‚Äì141, 1986. [39]Steven F. Roth, John Kolojejchick, Joe Mattis, and Jade Gold- stein. Interactive graphic design using automatic presentation knowledge. In Proceedings of the SIGCHI Conference on HumanFactorsinComputingSystems ,page112‚Äì117,New York,NY,USA,1994.AssociationforComputingMachinery. [40]Yang Wu, Yao Wan, Hongyu Zhang, Yulei Sui, Wucai Wei, Wei Zhao, Guandong Xu, and Hai Jin. Automated data visualizationfromnaturallanguagevialargelanguagemodels: Anexploratorystudy. Proc.ACMManag.Data ,2(3),2024. 2 [41]RobinRombach,AndreasBlattmann,DominikLorenz,Patrick Esser, and Bj√∂rn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022 , pages 10674‚Äì10685. IEEE, 2022. 2 [42]Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, ChelseaVoss,AlecRadford, MarkChen,andIlyaSutskever. Zero-shot text-to-image generation. In Proceedings of the 38thInternationalConferenceonMachineLearning ,pages 8821‚Äì8831. PMLR, 2021. [43]AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu, and MarkChen. Hierarchical text-conditionalimage genera- tion with CLIP latents, 2022. 2, 7 [44]MingDing,ZhuoyiYang, WenyiHong, WendiZheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang,andJieTang. CogView: Masteringtext-to-imagegen- erationviatransformers. In AdvancesinNeuralInformation ProcessingSystems34: AnnualConferenceonNeuralInfor- mationProcessingSystems2021,NeurIPS2021,December 6-14, 2021, virtual , pages 19822‚Äì19835, 2021. 10\n[45]Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. CogView2: Faster and better text-to-image generation via hierarchical transformers. In NeurIPS, 2022. 2 [46]Juan A. Rodriguez, David V√°zquez, Issam H. Laradji, Marco Pedersoli, and Pau Rodr√≠guez. FigGen: Text to scientific figure generation. In The First Tiny Papers Track at ICLR 2023, Tiny Papers @ ICLR 2023, Kigali, Rwanda, May 5, 2023. OpenReview.net, 2023. 2 [47]Juan A. Rodriguez, David V√°zquez, Issam H. Laradji, Marco Pedersoli,andPauRodr√≠guez. OCR-VQGAN:Tamingtext- within-imagegeneration. In IEEE/CVFWinterConferenceon Applications of Computer Vision, WACV 2023, Waikoloa, HI, USA, January 2-7, 2023 , pages 3678‚Äì3687. IEEE, 2023. 2 [48]Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and YoshuaBengio. Generativeadversarialnetworks. Commun. ACM, 63(11):139‚Äì144, 2020. 2 [49]Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 12873‚Äì12883, 2021. 2 [50]Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, andSuryaGanguli.Deepunsupervisedlearningusingnonequi- libriumthermodynamics. In Proceedingsofthe32ndInterna- tionalConferenceonMachineLearning ,pages2256‚Äì2265, Lille, France, 2015. PMLR. 2 [51]JiamingSong,ChenlinMeng,andStefanoErmon. Denoising diffusion implicit models. In International Conference on Learning Representations , 2021. 2 [52]YiweiHu,PaulGuerrero,MilosHasan,HollyRushmeier,and ValentinDeschaintre. Generatingproceduralmaterialsfrom text orimageprompts. In ACMSIGGRAPH2023 Conference Proceedings , New York, NY, USA, 2023. Association for Computing Machinery. 2 [53]Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models. National Science Review , 11(12):nwae403, 2024. 2 [54]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residuallearning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770‚Äì778, 2016. 3 [55]AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit, Llion Jones, Aidan N Gomez, ≈Å ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems . Curran Associates, Inc., 2017. 3 [56]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech,IainBarr,YanaHasson,KarelLenc,ArthurMensch, KatherineMillican,MalcolmReynolds,RomanRing,Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Shar- ifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, AndrewZisserman,andKarenSimonyan. Flamingo: avisual language model for few-shot learning. In Advances in Neural Information Processing Systems , 2022. 3[57]Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Ab- hinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,AkhilMathur,AlanSchelten,AlexVaughan,Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, ArthurHinsvark,ArunRao,AstonZhang,AurelienRodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron,BinhTang,BobbieChern,CharlotteCaucheteux,Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allon- sius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, DiegoGarcia-Olano,DiegoPerino,DieuwkeHupkes,Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzm√°n, FrankZhang,GabrielSynnaeve,GabrielleLee,GeorgiaLewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Ko- revaar,HuXu,HugoTouvron,IliyanZarov,ImanolArrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, JasonPark,JayMahadeokar,JeetShah,JelmervanderLinde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,JianyuHuang,JiawenLiu, JieWang,JiecaoYu,Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun,JoshuaSaxe,JuntengJia,KalyanVasudenAlwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, LawrenceChen,LiangTan,LizJenkins,LouisMartin,Lovish Madaan,LuboMalo,LukasBlecher,LukasLandzaat,Lukede Oliveira,MadelineMuzzi,MaheshPasupuleti,MannatSingh, ManoharPaluri,MarcinKardas,MariaTsimpoukelli,Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur √áelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu,RohanMaheswari,RohitGirdhar,RohitPatel,Ro- main Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sa- hanaChennabasappa,SanjaySingh,SeanBell,SeohyunSonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy,ShengShen,ShengyeWan,ShrutiBhosale,Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whit- man, Sten Sootla, Stephane Collot, Suchin Gururangan, Syd- ney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, V√≠tor Albiero, Vladan Petrovic,WeiweiChu,WenhanXiong,WenyinFu,Whitney 11\nMeers,XavierMartinet,XiaodongWang,XiaofangWang,Xi- aoqingEllenTan,XideXia,XinfengXie,XuchaoJia,Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain,AdamKelsey,AdamShajnfeld,AdithyaGangidi,Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, AmitSangani,AmosTeo,AnamYunus,AndreiLupu,Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisen- man, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi,BernieHuang,BethLoyd,BetoDePaola,Bhar- gavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Han- cock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, ChristophFeichtenhofer,CynthiaGao,DamonCivin,Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smoth- ers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer,GeorgiaSwee,GilHalpern,GrantHerman,Grigory Sizov,Guangyi,Zhang,GunaLakshminarayanan,HakanInan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb,HarrisonRudolph, Helen Suk,Henry Aspe- gren,HunterGoldman,HongyuanZhan,IbrahimDamlaj,Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang,JenniferChan,JennyZhen,JeremyReizenstein,Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, JoshGinsburg,JunjieWang,KaiWu,KamHouU,KaranSax- ena,Kartikay Khandelwal,Katayoun Zand,KathyMatosich, KaushikVeeraraghavan,KellyMichelena,KeqianLi,Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, LeiZhang, LiangpengGuo, LichengYu, LironMoshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, MatthiasReso,MaximGroshev,MaximNaumov,MayaLathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko,MichelleRestrepo,MihirPatel,MikVyatskov,Mikayel Samvelyan,MikeClark,MikeMacey,MikeWang,MiquelJu- bertHermoso,MoMetanat,MohammadRastegari,Munish Bansal,NandhiniSanthanam,NataschaParks,NatashaWhite, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Nor-manCheng,OlegChernoguz,OliviaHart,OmkarSalpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pa- van Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, PiotrDollar,PolinaZvyagina,PrashantRatanchandani,Pri- tish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, RafiAyub,RaghothamMurthy,RaghuNayani,RahulMitra, RangaprabhuParthasarathy,RaymondLi,RebekkahHogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, SaiJayeshBondu, SamyakDatta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seƒ≥i Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng,ShenghaoLin,ShengxinCindyZha,ShishirPatil,Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max,StephenChen,SteveKehoe,SteveSatterfield,Sudarshan Govindaprasad,SumitGupta,SummerDeng,SungminCho, SunnyVirk,SurajSubramanian,SyChoudhury,SydneyGold- man,TalRemez,TamarGlaser,TamaraBest,ThiloKoehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vƒ≥ai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu,VladimirIvanov,WeiLi,WenchenWang,Wen- wen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman,YanjunChen,YeHu,YeJia,YeQi,YendaLi,Yilin Zhang,YingZhang,YossiAdi,YoungjinNam,Yu,Wang,Yu Zhao,YuchenHao,YundiQian,YunluLi,YuziHe,ZachRait, ZacharyDeVito,ZefRosnbrick,ZhaoduoWen,ZhenyuYang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. 3, 4 [58]SeppHochreiterandJ√ºrgenSchmidhuber. Longshort-term memory. Neural Comput. , 9(8):1735‚Äì1780, 1997. 3 [59]Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015. 3 [60]Chuanguang Yang, Zhulin An, Libo Huang, Junyu Bi, Xin- qiang Yu, Han Yang, Boyu Diao, and Yongjun Xu. CLIP-KD: AnempiricalstudyofCLIPmodeldistillation. In Proceedings oftheIEEE/CVFConferenceonComputerVisionandPattern Recognition (CVPR) , pages 15952‚Äì15962, 2024. 3 [61]Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT,adistilledversionofBERT:smaller,faster, cheaper and lighter, 2020. 3 [62]HugoTouvron,ThibautLavril,GautierIzacard,XavierMar- tinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Roz- i√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models, 2023. 4 [63]Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language imagepre-training. InProceedingsoftheIEEE/CVFInternationalConferenceon Computer Vision (ICCV) , pages 11975‚Äì11986, 2023. 4 [64]ZheChen,WeiyunWang,HaoTian,ShenglongYe,Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, ZhengMa,JiMa,JiaqiWang,XiaoyiDong,HangYan,Hewei Guo,ConghuiHe,BotianShi,ZhenjiangJin,ChaoXu,Bin 12\nWang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, andWenhaiWang. HowfararewetoGPT-4V?closingthegap to commercial multimodal models with open-source suites. Science China Information Sciences , 67(12):220101, 2024. 4 [65]LucasBeyer,AndreasSteiner,Andr√©SusanoPinto,Alexan- derKolesnikov,XiaoWang,DanielSalz,MaximNeumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bo≈°njak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Bal- azevic,JoanPuigcerver,PinelopiPapalampidi,OlivierHenaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. PaliGemma: A versatile 3B VLM for transfer, 2024. 4 [66]Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43‚Äì76, 2021. 4 [67]StephanieFu,NetanelYakirTamir,ShobhitaSundaram,Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dream- Sim: Learning new dimensions of human visual similarity using synthetic data. In Thirty-seventh Conference on Neural Information Processing Systems , 2023. 4, 18 [68]ShobhitaSundaram,StephanieFu,LukasMuttenthaler,Ne- tanelYakirTamir,LucyChai,SimonKornblith,TrevorDarrell, and Phillip Isola. When does perceptual alignment benefit vision representations? In The Thirty-eighth Annual Con- ferenceonNeuralInformationProcessingSystems ,2024. 4, 18 [69]Miko≈ÇajBi≈Ñkowski,DougalJ.Sutherland,MichaelArbel,and Arthur Gretton. Demystifying MMD GANs. In International Conference on Learning Representations , 2018. 4 [70]Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: A reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methodsin Natural Language Pro- cessing,pages7514‚Äì7528,OnlineandPuntaCana,Dominican Republic,2021.AssociationforComputationalLinguistics. 4 [71]Aryaz Eghbali and Michael Pradel. CrystalBLEU: Precisely and efficiently measuring the similarity of code. In Pro- ceedings of the 37th IEEE/ACM International Conference on AutomatedSoftwareEngineering ,NewYork,NY,USA,2023. Association for Computing Machinery. 4 [72]KishorePapineni,SalimRoukos,ToddWard,andWei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 311‚Äì 318, Philadelphia, Pennsylvania, USA, 2002. Association for Computational Linguistics. 4 [73]Peter Stanchev, Weiyue Wang, and Hermann Ney. EED: Extended edit distance measure for machine translation. In ProceedingsoftheFourthConferenceonMachineTranslation (Volume 2: Shared Task Papers, Day 1) , pages 514‚Äì520, Flo- rence, Italy, 2019. Association for Computational Linguistics. 4[74]James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness,GuillaumeDesjardins,AndreiA.Rusu,KieranMilan, JohnQuan,TiagoRamalho,AgnieszkaGrabska-Barwinska, DemisHassabis,ClaudiaClopath,DharshanKumaran,and RaiaHadsell. Overcomingcatastrophicforgettinginneural networks. Proceedings of theNational Academy of Sciences , 114(13):3521‚Äì3526, 2017. 5 [75]Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang,BoZheng,YiboMiao,ShanghaoranQuan,Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-Coder technical report, 2024. 6, 15 [76]Michael Saxon, Fatima Jahara, Mahsa Khoshnoodi, Yujie Lu, Aditya Sharma, and William Yang Wang. Who evaluates the evaluations? objectively scoring text-to-image prompt coherencemetricswitht2IScorescore(TS2). In TheThirty- eighth Annual Conference on Neural Information Processing Systems, 2024. 6 [77]JordanJ.Louviere,TerryN.Flynn,andA.A.J.Marley. Best‚Äì WorstScaling: Theory,MethodsandApplications .Cambridge University Press, 2015. 6 [78]Svetlana Kiritchenko and Saif M. Mohammad. Capturing reliablefine-grainedsentimentassociationsbycrowdsourcing and best‚Äìworst scaling. In Proceedings of the 2016 Confer- ence of the North American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies , pages 811‚Äì817, San Diego, California, 2016. Association for Computational Linguistics. 6 [79]SvetlanaKiritchenkoandSaifMohammad.Best‚Äìworstscaling more reliable than rating scales: A case study on sentiment intensity annotation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume2: ShortPapers) ,pages465‚Äì470,Vancouver,Canada, 2017. Association for Computational Linguistics. 6 [80]Bryan K. Orme. MaxDiff analysis: Simple counting, individual-levellogit,andHB. SawtoothSoftwareResearch Paper Series , 2009. 6 [81]BruceSchneier. Appliedcryptography-protocols,algorithms, and source code in C, 2nd Edition . Wiley, 1996. 7 [82]KyleLo,LucyLuWang,MarkNeumann,RodneyKinney,and Daniel Weld. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 4969‚Äì4983, Online, 2020. Association for Computational Linguistics. 8 [83]ThaoNguyen,SamirYitzhakGadre,GabrielIlharco,Sewoong Oh,andLudwigSchmidt.Improvingmultimodaldatasetswith image captioning. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. 8 [84]Anas Awadalla, Le Xue, Manli Shu, An Yan, Jun Wang, SenthilPurushwalkam,ShengShen,HannahLee,OscarLo, JaeSungPark,EtashKumarGuha,SilvioSavarese,Ludwig Schmidt, Yejin Choi, Caiming Xiong, and Ran Xu. BLIP3- KALE: Knowledge augmented large-scale dense captions. In SyntheticDataforComputerVisionWorkshop@CVPR2025 , 2025. 13\n[85]XianhangLi,HaoqinTu,MudeHui,ZeyuWang,Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, Yuyin Zhou, and Cihang Xie. What if werecaptionbillionsofwebimageswithLLaMA-3?,2024. 8 [86]R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality re- duction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pat- tern Recognition (CVPR‚Äô06) , pages 1735‚Äì1742, 2006. 8 [87]Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. ImageBind: One embedding space to bind them all. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 15180‚Äì15190, 2023. 8 [88]Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, WANG HongFa, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zong- wei Li, Cai Wan Zhang, Zhifeng Li, Wei Liu, and Li Yuan. LanguageBind: Extending video-language pretraining to n- modality by language-based semantic alignment. In The Twelfth International Conference on Learning Representa- tions, 2024. 8 [89] OpenAI. Learning to reason with LLMs, 2024. 15 [90]DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junx- iao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, PeiyiWang,XiaoBi,XiaokangZhang,XingkaiYu,YuWu, Z. F.Wu, ZhibinGou, Zhihong Shao,Zhuoshu Li,Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, ChengdaLu,ChenggangZhao,ChengqiDeng,ChenyuZhang, ChongRuan,DamaiDai,DeliChen,DongjieJi,ErhangLi, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen,GuoweiLi,H.Zhang,HanBao,HanweiXu,Haocheng Wang,HonghuiDing,HuajianXin,HuazuoGao,HuiQu,Hui Li,JianzhongGuo,JiashiLi,JiaweiWang,JingchangChen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, MiaojunWang,MingmingLi,NingTian,PanpanHuang,Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, RuisongZhang,RuizhePan,RunjiWang,R.J.Chen,R.L.Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, ShutingPan,S.S.Li,ShuangZhou,ShaoqingWu,Shengfeng Ye, TaoYun, TianPei, Tianyu Sun, T.Wang, WangdingZeng, WanjiaZhao,WenLiu,WenfengLiang,WenjunGao,Wen- qinYu,WentaoZhang,W.L.Xiao,WeiAn,XiaodongLiu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, XuechengSu,XuhengLin,X.Q.Li,XiangyueJin,Xiaojin Shen, XiaoshaChen, Xiaowen Sun, XiaoxiangWang, Xinnan Song,XinyiZhou,XianzuWang,XinxiaShan,Y.K.Li,Y.Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, YifanShi,YiliangXiong, YingHe, YishiPiao, YisongWang, YixuanTan,YiyangMa,YiyuanLiu,YongqiangGuo,Yuan Ou,YuduanWang,YueGong,YuhengZou,YujiaHe,Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, YuyangZhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, YutingYan,Z.Z.Ren,ZehuiRen,ZhangliSha,ZheFu,Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zƒ≥ia Zhu, Zƒ≥un Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, ZhipengXu,ZhongyuZhang,andZhenZhang. DeepSeek-R1: IncentivizingreasoningcapabilityinLLMsviareinforcement learning, 2025. 15 [91]Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Ru- jie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. UltraEdit: Instruction-based fine-grained im- ageeditingatscale. In TheThirty-eightConferenceonNeural Information Processing Systems Datasets and Benchmarks Track, 2024. 15 [92]Urbano Lorenzo-Seva and Jos M. F. ten Berge. Tucker‚Äôs con- gruence coefficient as a meaningful index of factor similarity. Methodology: European Journal of Research Methods for the Behavioral and Social Sciences , 2(2):57‚Äì64, 2006. 15 [93]Jonas Belouadi and Steffen Eger. UScore: An effective ap- proachtofullyunsupervisedevaluationmetricsformachine translation. In Proceedingsofthe17thConferenceoftheEuro- peanChapteroftheAssociationforComputationalLinguistics , pages 358‚Äì374, Dubrovnik, Croatia, 2023. Association for Computational Linguistics. 15 [94]Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer,andSteffenEger. MoverScore: Textgenerationeval- uatingwithcontextualizedembeddingsandearthmoverdis- tance. In Proceedingsofthe2019ConferenceonEmpirical Methods in Natural Language Processing and the 9th Inter- nationalJointConferenceonNaturalLanguageProcessing (EMNLP-ƒ≤CNLP) , pages 563‚Äì578, Hong Kong, China, 2019. Association for Computational Linguistics. [95]WeiZhao,GoranGlava≈°,MaximePeyrard,YangGao,Robert West, and Steffen Eger. On the limitations of cross-lingual encoders as exposed by reference-free machine translation evaluation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 1656‚Äì1671, Online, 2020. Association for Computational Linguistics. [96]Yurun Song, Junchen Zhao, and Lucia Specia. SentSim: Crosslingualsemanticevaluationofmachinetranslation. In Proceedingsofthe2021ConferenceoftheNorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 3143‚Äì3156, Online, 2021. Association for Computational Linguistics. 15 [97]Y. Rubner, C. Tomasi, and L.J. Guibas. A metric for dis- tributions with applications to image databases. In Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271) , pages 59‚Äì66, 1998. 15 [98]MattKusner,YuSun,NicholasKolkin,andKilianWeinberger. Fromwordembeddingstodocumentdistances.In Proceedings ofthe32ndInternationalConferenceonMachineLearning , pages 957‚Äì966, Lille, France, 2015. PMLR. 15 14\nA. Additional Baselines and Ablation Studies Beyond the evaluation in Sec. 5, we test additional baselines, including reasoning models that have proven successful in program synthesis tasks [ 89]. We evaluate Qwen 2.5Coder (14B)[75],whichcomplements Qwen 2.5Coder(32B)from Sec.5.2,andreasoningmodelsfromthe DeepSeek-R1 Qwen family(14Band32B)[ 90]. Wealsoevaluate TikZero(Base), avariantof TikZero(Cos)withoutthetrainableprobeand gating mechanism, to assess their contributions. AsshowninTab.5, TikZero(Cos)achievesthehighest performance,surpassing TikZero(Base)onboth DreamSim andCLIPScore metrics and in average performance. These results validate the probe and gate design. Additionally, Qwen 2.5Coder(14B)performsworsethanboth TikZero (Cos) and, as expected, its 32B variant in Tab. 3. The results are consistent with our findings in Sec. 5.1 that TikZero(Cos) outperforms end-to-end trained baselines of comparable size. Notably, the reasoning models show thelowestoverallperformance,evencomparedto Qwen 2.5 Coder(14B), indicating that reasoning capabilities alone are insufficient for graphics program synthesis and more domain-specific post-training may be needed. B. Supplementary Comparison with DeTi kZify Tab.6showsindetailhow TikZero‚Äôsinversegraphicsmodel (hereafter referred to as DeTi kZify v2) compares against De- TikZify DS(7b),previouslythebestperforming DeTi kZify model, as evaluated on the test split of DaTi kZv3.De- TikZify v2clearly outperforms its predecessor across all evaluated metrics. Below, we briefly outline key differences intrainingandinferencebeyondwhatwedescribedinSec.4. For a comprehensive description of the foundation on which DeTi kZify v2builds, we refer to Belouadi et al. [2]. Training Similarto DeTi kZify,DeTi kZify v2employsa dense layer as the modality connector between the vision encoderandtextdecoder. However,forpretrainingthislayer, we replace the MetaFig dataset [2] with the substantially larger ArxivCap dataset,extracting1million(figure,caption, OCR)triplets. Duringfine-tuning,werandomlysubstitute inputs with synthetically generated sketches to support hand- drawn inputs. To generate these sketches, we fine-tune the image-editing model UltraEdit [91] on a dataset of real, human-createdscientificsketches[ 2]. Theresultingmodel, UltraSketch , achieves a congruence coefficient (CC) [ 92] of0.74withsaidsketches,comparedto0.72fortheprevious model used with DeTi kZify. Additionally, we generate synthetic sketches using traditional image transformations such as random displacement fields. While these sketches exhibit less diversity, they better preserve text rendering and achieve a comparable CC of 0.75. Averaging the sketch representations from both methods increases the CC to 0.82, demonstrating their complementary nature.Inference DeTi kZifyimplements a Monte Carlo Tree Search-based inference algorithm to iteratively refine out- puts. As a reward signal ùëü, it computes the cosine similarity ùëücos=cos(pool(ùíô),pool(ùíö))between image patch embed- dings ùíô,ùíöofinputimagesandcompiledoutputsviaalearned pooling function. Since DeTi kZify v2fully fine-tunes the vi- sionencoderandusesitspatchembeddingsdirectly,itcannot compute pooled embeddings in the same way. As an alterna- tive,inspiredbypopularmachinetranslationmetrics[ 93‚Äì96], weexperimentwithcomputingtheEarthMover‚ÄôsDistance (EMD) [97,98] with image patch embeddings. Given the distancematrix ùë´,whereùê∑ùëñ,ùëó=cos(ùë•ùëñ,ùë¶ùëó),EMDisdefined as follows: EMD(ùíô,ùíö)=√ç|ùíô| ùëñ=1√ç|ùíö| ùëó=1ùêπùëñ,ùëóùê∑ùëñ,ùëó √ç|ùíô| ùëñ=1√ç|ùíö| ùëó=1ùêπùëñ,ùëó, with min ùë≠‚â•0|ùíô|‚àëÔ∏Å ùëñ=1|ùíö|‚àëÔ∏Å ùëó=1ùêπùëñ,ùëóùê∑ùëñ,ùëó s.t.‚àÄùëñ,ùëó(√ç|ùíô| ùëñ=1ùêπùëñ,ùëó=1 |ùíö|, √ç|ùíö| ùëó=1ùêπùëñ,ùëó=1 |ùíô|.(2) Whencorrelatingrewardscorescomputedas ùëücosfrom De- TikZifyandùëüEMD=EMD(ùë•ùëñ,ùë¶ùëó)from DeTi kZify v2with human judgments from Belouadi et al. [2], we find that ùëüEMDenhances correlation with humans (0.456 segment- level and 0.911 system-level Spearman‚Äôs ùúå), compared to ùëücos(0.436and0.642, respectively). Thisdemonstratesthat DeTi kZify v2notonlysupportsthe inference algorithm but improves upon DeTi kZify‚Äôs capabilities. C. Supplementary Inference Details To instruct general-purpose models to generate Ti kZ code, weemployaconsistentpromptacrossallmodels( GPT-4o, Qwen 2.5Coder(32B), and IDEFICS 3 (8B)) originally engineeredbyZhangetal. [4]. Foreachfigure,wereplace the<caption> placeholder with the specific caption: Please generate a scientific 1 figure according to the following 2 requirements: <caption>. Your output 3 should be in Ti kZ code. Do not include 4 any text other than the Ti kZ code. 5 D. Supplementary Experimental Results Tab. 7 presents detailed evaluation metrics scores for the low-resourcetrainingexperimentsdiscussedinSec.6.2. The results show a consistent degradation in performance across allmetricsasboththeamountoftrainingdataandthenumber oflayersdecrease,atrendeffectivelycapturedbytheAVG scores also shown in Tab. 4. 15\nE. Annotator Demographics Our annotation team consists of thirteen experts with ex- tensive research experience in Machine Learning, Natural Language Processing, or Computer Vision. The team in- cludes one male faculty member, four female PhD students, four male PhD students, andfourmale researcher scientists from a research institute. We deliberately selected expert annotators based on findings by Belouadi et al. [3], which demonstrated that crowd workers often lack the necessary researchbackgroundtoprovidereliableannotationsforsci- entificfigures. Tomitigatepotentialbiases,eachannotator receivedthetuplesanditemswithinthetuplesinrandomized order. F. Additional Examples Figure 5 showcases examples 5from DaTi kZv3with per- missive licenses. Additionally, Tab. 8 presents randomly sampled tuples from our human evaluation with the highest and lowest rated instances highlighted. The results show thatAutomaTi kZv2(LLM) and TikZero(Cos) are more frequentlyselectedastheworstmodels(fourandthreetimes, respectively),while TikZero +andGPT-4oaremoreoften chosen as the best models (both three times), which aligns with our findings in Sec. 5.3. Finally, Fig. 6 illustrates exam- ple programs generated by TikZero +andAutomaTi kZv2 (LLM),demonstratinghow TikZero +utilizesadvancedTi kZ features,whereas AutomaTi kZv2(LLM)employsonlybasic, simple commands. 5sourced from https://github.com/PetarV-/TikZ ,https:// github.com/janosh/tikz ,https://tikz.net ,and https://arxiv. org 16\n. . . LSTM‚Üí LSTM‚Üí LSTM‚Üí LSTM‚Üí. . .LSTM‚Üê LSTM‚Üê LSTM‚Üê LSTM‚Üê. . . . . . /vector x2 /vector x3 /vector x4 /vector x5/vectorh5/vectorh4/vectorh3/vectorh2 /vectorh‚Üí 2/vectorh‚Üí 3/vectorh‚Üí 4 ‚Ä¶ ‚Ä¶/vectorh‚Üê 3/vectorh‚Üê 4/vectorh‚Üê 5(a) A diagram representing a recurrent neural network consisting of several LSTM blocks, processing the input sequence simultaneously forwards and backwards(toexploitbothdirectionsoftemporaldependence). Contains some rather tight manoeuvering. ‚àí6 ‚àí4 ‚àí2 2 4 60.511.5 Œ≤(Œµ1‚àí¬µ)/angbracketleftn/angbracketright Bose-Einstein Boltzmann Fermi-Dirac(b)AplotcomparingthedistributionfunctionsofBose-Einstein,Boltzmann, andFermi-Diracstatisticsasafunctionofthereducedchemicalpotential ùõΩ(ùúñ‚àíùúá). This visualiation highlights the differences between the three typesofdistributionfunctions,whichareusedtodescribethebehaviorof particles in different statistical systems. 0.40.60.50.5 0.90.10.40.60.90.1 0.250.75 0.80.2 ¬ØùëÖ ùëá ¬Øùëçùëçùê∑ ¬ØùëçùëçùëÖ ùëá ¬Øùëçùëçùê∑ ¬Øùëçùëçùúî ùëÉ¬πùúî¬∫ ùê∏1 ùê∏2 ùê∏3 fùëÖ;ùê∑;ùëçg 0¬ï015 \u000f \u000f fùëÖ;ùê∑;¬Øùëçg 0¬ï135 \u000f fùëÖ;ùëá;ùëçg 0¬ï03 \u000f \u000f fùëÖ;ùëá;¬Øùëçg 0¬ï02 \u000f f¬ØùëÖ;ùê∑;ùëçg 0¬ï04 \u000f f¬ØùëÖ;ùê∑;¬Øùëçg 0¬ï04 \u000f f¬ØùëÖ;ùëá;ùëçg 0¬ï432 \u000f \u000f f¬ØùëÖ;ùëá;¬Øùëçg 0¬ï288 \u000f 1 Given values are encircled. (c) Tree with aligned matrix. A probability tree with an aligned matrix listing the possible outcomes, their probabilities and three columns for events described in later tasks. It uses the grahdrawing library and requires LuaLaTeX. A1 I1 ... An InTE T TE T attention T IQTDaQ 1, ..., aQ m SOS, aQ 1, ..., aQ m‚àí1V1 K1 Vn Kn(d) Our approach is a modified version of meta-seq2seq . A transformer decoder (TD) is trained to produce a sequence of actions ùëéùëÑ 1,...,ùëéùëÑ ùëö given a query instruction ùêºùëÑ. The context are demonstrations (ùêºùëò,ùê¥ùëò) produced by our generative model. We use a transformer encoder-decoder (T) to encode instructions and state ùëÜand a transformer encoder (TE) to encode actions. The transformers that process instructions (pink blocks) receive stateùëÜas the input of the encoder. Figure 5. Representative examples from DaTi kZv3(also present in DaTi kZandDaTi kZv2), with permissive licenses. Models DSim ‚ÜëKID ‚ÜìCLIP ‚ÜëcBLEU ‚ÜëTED ‚ÜìMTE ‚ÜëAVG ‚Üë TikZero(Cos) 52.829 5.103 10.051 1.603 65.51 82.291 64.309 TikZero(Base) 52.373 5.225 9.428 1.589 65.286 83.128 63.129 Qwen 2.5Coder(14B) 48.352 12.988 19.761 0.229 60.304 93.285 58.894 DeepSeek-R1 Qwen (32B) 47.573 8.887 21.201 1.388 64.928 66.225 57.252 DeepSeek-R1 Qwen (14B) 44.616 15.43 21.695 0.842 63.323 36.11 31.102 Table5. System-level scores√ó100forTikZero(Cos)andadditionalbaselines. Overall, TikZeroachievesthestrongestaverageperformance across metrics. 17\nReference Figures Synthetic Sketches Models DSim ‚ÜëKID ‚ÜìcBLEU ‚ÜëTED ‚ÜìMTE ‚ÜëDSim ‚ÜëKID ‚ÜìcBLEU ‚ÜëTED ‚ÜìMTE ‚Üë DeTi kZify DS(7b) 75.46 0.842 2.953 56.851 84.019 67.379 0.766 1.541 59.589 84.401 DeTi kZify v2 80.503 0 .626 6 .105 54 .946 93 .326 74 .584 0 .751 3 .356 58 .32 93 .858 Table 6. System-level scores√ó100forDeTi kZify v2andDeTi kZify DS(7b) on both reference figures and synthetic sketches generated with UltraSketch fromthetestsplitof DaTi kZv3. Bestscoresareinbold,andarrowsindicatemetricdirectionality. Notethatwecompute DreamSim using updated models [68], whereas Belouadi et al. [3] used the original models in their work [67]. Data Intv. DSim ‚ÜëKID ‚ÜìCLIP ‚ÜëcBLEU ‚ÜëTED ‚ÜìMTE ‚ÜëAVG ‚Üë 100% 1 52.771 5.127 9.949 1.607 65.516 82.292 92.411 100% 2 52.311 5.2 9.955 1.484 65.473 82.588 87.557 100% 4 51.794 5.688 8.886 1.429 65.399 83.988 82.254 100% 8 51.59 5.933 9.818 1.371 65.608 83.679 76.545 50% 1 52.106 5.835 8.527 1.454 65.605 83.599 77.478 50% 2 52.143 5.103 9.315 1.393 65.355 82.924 85.249 50% 4 50.492 6.689 8.852 1.459 65.951 78.456 47.381 50% 8 50.093 6.738 7.999 1.379 65.963 78.923 40.816 25% 1 51.55 6.055 9.12 1.472 66.237 77.961 49.967 25% 2 51.231 6.152 8.943 1.43 65.714 77.566 54.942 25% 4 49.859 7.715 7.316 1.41 66.128 79.704 32.12 25% 8 49.179 7.764 6.495 1.434 66.009 79.9 29.774 12.5% 1 50.485 6.25 7.568 1.509 65.8 80.816 56.055 12.5% 2 50.152 7.129 6.353 1.275 66.045 81.05 33.817 12.5% 4 49.667 7.031 6.474 1.221 65.892 82.634 37.914 12.5% 8 48.827 8.154 5.054 1.11 65.813 80.738 16.25 Table 7. System-level scores√ó100TikZero(Cos) trained on varying fractions of data and intervals of cross-attention layers. Bold and underlined values denote the best and second-best scores for the whole table, respectively. Cell shading illustrates score magnitudes. Arrows indicate metric directionality. 18\nReference AutomaTi kZv2 TikZero Ti kZero + GPT-4o An illustration of the reduction from densest ùëò-subgraph to u-rcp. On the left there is a simple undirected graph ùê∫with a single edge. The 2-reduced directed graph of ùê∫is on the right. Each vertex of ùê∫is replaced by2¬∑2=4copieswithabidirectionaledge connectinganytwocopiesofthesamevertex, and an outgoing edge from each copy to the single edge-vertex ùëí. e 1 2 3 4 5 6 v eu v v1 v2v11 v12 v13 v14v21 v22 v23 v24e C1‚âª1 ‚âª2 ‚âª3 ‚âª41 1112 2 1112 3 1112 4 1112 C2‚âª5 ‚âª6 ‚âª7 ‚âª85 1112 6 1112 7 1112 8 1112 C3Upper half of C3 Lower half of C3‚âª9 ‚âª10 ‚âª11 ‚âª129 1112 101112 1112 12 C4‚âª13 ‚âª14 ‚âª15 ‚âª1613 1112 14 1112 15 1112 16 1112 t t21718 1718 1718 1718 1718 1718 1718 1718 123456789101112131415161718051015#Agents No 1 2 3 4 11 12 13 14 15 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 1234 1 2 3 4 5 6 7 8 9 10 t= 1 t= 2 t= 3 t= 41 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Domain of dependence: The closed trape- zoidalregionŒ©ùë°usedintheproofof propo- sition1, shaded with darker yellow; the domainofdependenceofpoint (0,‚Ñì)isŒ©ùë° alongwiththetopregionshadedwithlighter yellow. xy L‚Ñìy=x 0 x z0tt+r (0, ‚Ñì)œÑtœÉt xt [0,1] [0,2] [0,4] [0,8] A B C D Œ±-fraction constraint W1W2W3W4 W5W6 œâ1œâ2œâ3TW1W2W3W4 W5W6W7 œâ1œâ2 œâ3T T A sketch of iterating ùëì(ùë•)=ùë•‚àí1/ùë•. Points bigger than 1 get sent to point in [0,1]which then moves to a point ‚â§‚àí1 which moves to[‚àí1,0]which is sent to a point‚â•1and so on. ‚àí1 111 203 2 b ab ab a b a x 0 1 2 3 4 50.40.60.81 ŒªMSE/(1‚àíp)Rank-one MMSE p= 0.9,Oracle-MSE p= 0.9,RIE-MSE p= 0.2,Oracle-MSE p= 0.2,RIE-MSE 1-p x yx y 1-p 5 10 20 30 40 500.80.911.11.2¬∑10‚àí3 ntMSEŒ±= 0.1 Œ±= 0.3 Œ±= 0.5 Œ±= 0.7 Œ±= 0.9 Œ±= 0.95 05¬∑10‚àí20.1 0 .15 0 .2 0 .25 0 .3 0 .35 0 .4 0 .45 0 .50.20.40.60.81 SNR [dB]MSE RIE DC-2 DC-3 0 0 .2 0 .4 0 .6 0 .8 100.20.40.60.81 Probability pNormalized MSE A multiline diagram ùê∑withùëõ=6 columns andùë†=5rows, with content ùúÜ=(5,4,3,1,0,0)and bottom row ùúå(1)(ùê∑)=(4,0,1,5,3,0)‚ààùëÜùúÜ. It has weight w(ùê∑)=wùë•(ùê∑)wùë°(ùê∑)= ùë•3 1ùë•2 3ùë•4 4ùë•2 5ùë•2 6ùë°2. 5 4 3 1 0 0 4 0 1 5 3 0 2 2 0 4 0 2 2 0 4 0 2 2 0 4 0 2 2 0 4Ô£´ Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠Ô£∂ Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏Œª œÅ(2) œÅ(3) œÅ(4) œÅ(5) œÅ(6)D t2(x)D: x1x1x1x1x1 x2x2x2x2 x3x3x3 x4 x4 x5x6 x4 x6 0 ab a‚àób‚àó c A AA Ab acb a cc cx yzz z xxxy y y z z z a a a L(a, b ) cc‚àí c+a‚àó b‚àóa b0Ta,bJ‚àó[a, b ] aba‚àó b‚àóc Table 8. Alternating rows display randomly selected tuples from the caption and image similarity human evaluation task (cf. Sec. 5.3). The frames of highest and lowest rated instances are highlighted in green and red, respectively. 19\nTikZero +1\\documentclass [border=10pt]{standalone} 2\\usepackage {pgfplots} 3\\pgfplotsset {width=7cm,compat=1.8} 4\\begin {document} 5\\begin {tikzpicture} 6 \\begin {axis}[grid=major,view={210}{30}] 7 \\addplot 3+[domain=-5:5,surf] 8 {exp(-x^2-y^2)*x}; 9 % connect classical points 10 \\addplot 3+[mark=*,only marks,blue,samples at={-4.473,-1.463,1.463,4.473}] 11 {exp(-x^2-y^2)*x}; 12 \\end {axis} 13\\end {tikzpicture} 14\\end {document}TikZero +\\documentclass [12pt]{article} 1 \\usepackage {amsmath,amsthm,amssymb,amsfonts} 2 \\usepackage {tikz} 3 \\usepackage {pgfplots} 4 \\usetikzlibrary {arrows.meta} 5 6 \\begin {document} 7 8 \\begin {tikzpicture} 9 \\begin {axis}[view={60}{30}] 10 \\addplot 3 [surf] {3*x^2 + y^2}; 11 \\end {axis} 12 \\end {tikzpicture} 13 14 \\end {document} 15 AutomaTi kZv21\\documentclass [11pt,a4paper]{article} 2\\usepackage {amsmath} 3\\usepackage {amssymb} 4\\usepackage {color} 5\\usepackage {tikz} 6 7\\begin {document} 8 9\\begin {tikzpicture}[scale=0.5] 10\\draw [thick,->] (-2,0) -- (2,0); 11\\draw [thick,->] (0,-2) -- (0,2); 12\\draw [thick] (1.8,0) arc (0:180:1.8); 13\\node [right] at (2,0) {$\\mathrm{Re}(\\omega)$}; 14\\node [above] at (0,2) {$\\mathrm{Im}(\\omega)$}; 15\\node [right] at (1.8,1) {$\\Gamma$}; 16\\end {tikzpicture} 17 18\\end {document} AutomaTi kZv2\\documentclass {article} 1 \\usepackage {amssymb} 2 \\usepackage {amsmath} 3 \\usepackage {pgfplots} 4 \\pgfplotsset {compat=1.14} 5 \\usepackage {tikz} 6 \\usetikzlibrary {arrows} 7 8 \\begin {document} 9 10 \\begin {tikzpicture}[scale=3] 11 \\draw [fill=green!15] (1,0) -- (2,0) -- (2,2) -- (1,2) -- (1,0);12 \\draw [thick] (1,0) -- (1,2); 13 \\draw [thick] (0,1) -- (2,1); 14 \\node [below] at (1,0) {$0$}; 15 \\node [below] at (2,0) {$1$}; 16 \\node [left] at (1,2) {$1$}; 17 \\node [left] at (0,1) {$0$}; 18 \\node [above] at (1.5,1.5) {$\\gamma$}; 19 \\draw [->,thick] (0,0) -- (1,0); 20 \\draw [->,thick] (0,0) -- (0,1); 21 \\draw [->,thick] (0,0) -- (0.5,0.5); 22 \\draw [->,thick] (0,0) -- (1.5,0.5); 23 \\draw [->,thick] (0,0) -- (0.5,1.5); 24 \\draw [->,thick] (0,0) -- (1.5,1.5); 25 \\draw [->,thick] (0,0) -- (2,0); 26 \\draw [->,thick] (0,0) -- (0,2); 27 \\draw [->,thick] (0,0) -- (1,1); 28 \\end {tikzpicture} 29 30 \\end {document} 31 Figure 6. Ti kZ programs generated by TikZero +(top) and AutomaTi kZv2(LLM; bottom) corresponding to the figures shown in the first row of Fig. 1 in the same order. 20\n",
  "metadata": {
    "paper_id": "2503.11509v3",
    "downloaded_at": "2025-08-24T21:06:38.172425+00:00"
  },
  "processed_at": "2025-08-24T21:06:38.172452+00:00"
}