{
  "content": "arXiv:cs/0603132v1 [cs.GR] 31 Mar 2006Graphics Turing Test Michael McGuigan Brookhaven National Laboratory Upton NY 11973 mcguigan@bnl.gov Abstract We deﬁne a Graphics Turing Test to measure graphics performance in a similar manner to the deﬁnition of the traditional Turing Test. To pas s the test one needs to reach a computational scale, the Graphics Turin g Scale, for which Computer Generated Imagery becomes comparatively indistin guishable from real images while also being interactive. We derive an estimate fo r this computational scale which, although large, is within reach of todays supercom- puters. We consider advantagesand disadvantagesof variousco mputer systems designed to pass the Graphics Turing Test. Finally we discuss commer cial ap- plications from the creation of such a system, in particular Interac tive Cinema. Deﬁning graphics computing scales can be diﬃcult. Traditio nal methods such as counting triangles drawn per second can be misleading esp ecially if a large per- centage of the computing power is going into shading code or a physics simulation which can be involved in making the graphics image realistic . More than ﬁfty years ago Alan Turing faced a similar challeng e in evaluating the power of a computer. In short, he devised a test to see if one ca n build a computer whose intelligence was indistinguishable from human intel ligence [1]. The Turing computational scale is then deﬁned by the computational pow er required to pass this test. The beauty of this deﬁnition is that the computati onal scale is deﬁned by the indistinguishability of comparison. In particular one does not need a deﬁnition of human intelligence to implement the test. Also one does no t need to measure in terms of Flops per second or some other commonly used metho d of measuring computer power, after all integer performance, memory acce ss or network connec- tivity may be important. One only needs to statistically com pare how subjects can discriminate between a human and a computer. If the subjects can do no better than a random guess than the Turing test is passed and the Turi ng scale has been measured. Unfortunately the Turing Test is too hard for now, one cannot even use partial successes in AI to estimate the size of a computer req uired to pass the test [2]. As we shall see the corresponding situation in graphics is somewhat better. 1\nIn a similar approach to the traditional Turing test we deﬁne the Graphics Tur- ing Test as follows: The subject views and interacts with a real or computer genera ted scene. The test is passed if the subject can not determine reality from simul ated reality better than a random guess. (a) The subject operates a remotely controlled ( or simulated) robotic arm and views a computer screen. (b) The subject enters a door t o a controlled vehicle or motion simulator with computer screens for windo ws. An eye patch can be worn on one eye as stereo vision is diﬃcult to simulate. The Graphics Turing Scale is then deﬁned as the computer powe r necessary to pass the above test. The key feature in the above deﬁnition is that the subject interacts with the computer generated scene. As we shall dis cuss it is possible using a reasonably powerful system to create a computer generated image that is indistin- guishable from reality, but it may take several hours to rend er the image. It is the requirement of interactivity that accounts for the large am ount of computer power inherent in passing the Graphics Turing Test. Note that, as w ith the traditional Turing Test, in the Graphics Turing Test the computational s cale is deﬁned as one reaches an indistinguishability of comparison. Also note t hat one need not deﬁne the graphics performance in terms of triangles per second or pixel ﬁll rate or some other commonly used metric. The computational scale is deﬁn ed intrinsically by the subjects ability to determine if the scene is real or comp uter generated even by interacting and driving through the scene. It is the stati stical analysis of the subject’s determination of the reality of the scene compare d to a random guess that forms the metric in the Graphics Turing Test. All traditiona l measures of graphics power including the complexity of the geometry, shader code and lighting has al- ready been folded into this metric. Some speciﬁc implementa tions of the Graphics Turing Test which are relatively easy to set up are stated in ( a) and (b) above. Other realizations are possible. In particular, if ghostin g can be suﬃciently elimi- nated, it should be possible to setup a stereoscopic version of the Graphics Turing Test. It has also been proposed to combine realistic graphic s into the traditional Turing Test [3]. However, as it is discussed in this paper, th e Graphic Turing Test measures graphics performance only and is separate from AI. But what is this Graphics Turing Scale and what type of comput er is required to pass the Graphics Turing Test? Although the Graphics Turi ng Test is similar in structure to the traditional Turing Test it diﬀers in one impo rtant aspect. Partial success in generating non interactive photo realistic imag ery can be used to esti- mate the Graphics Turing Scale. The large computing scale is mainly the result of achieving an interactive frame rate for the computer gene rated imagery, so that the subject perceives time as continuous, and in implementi ng relatively mature 2\ngraphics algorithms. Although a hard problem the Graphics T uring Test is within reach of todays supercomputers. In contrast, for the tradit ional Turing Test the AI algorithms are less mature and non interactive AI has not bee n achieved. To estimate the size of the Graphics Turing Scale consider th e recent photo realistic renderings by Paul Debevec of the Parthenon [4]. H e used Monte Carlo illumination methods and about 1GB of complex geometry. The results were essen- tially indistinguishable from reality which is all the more impressive as they were presented to an audience of graphics professionals acutely aware of the subtleties of implementing realistic lighting. To generate the imagery r equired 2 hours on 1 CPU 2.4 Ghz Pentium IV for each frame. Thus an interactivity of 1 30 sec would require 216,000 CPUs of computing power. Although more powerful CPU s are available to- day it is also true that supercomputers typically use lower c lock rates to reduce heat and power consumption as thousands of processors are placed in close proximity to one another. Thus we estimate the Graphics Turing Scale as the computational equivalent of roughly 200k CPUs. In terms of traditional mea sures of computing power we have: Graphics Turing Scale = (216 k) ×(4.8 GFlops) ×/braceleftBigg 1 ǫ/bracerightBigg =/braceleftBigg 1036.8 TFlops Peak 518.4 TFlops Sustained/bracerightBigg whereGFlopsandTFlopsarebillionandtrillionﬂoatingpoi ntoperationspersecond respectively and ǫis the computational eﬃciency of the rendering algorithm wh ich we take to be 50%. Peak refers to the theoretical computing po wer attainable in a system and Sustained refers to the computer power attained when taking into account the ineﬃciency of the algorithm. The eﬃciency is giv en by the ratio ǫ= Sustained /Peak. One way to achieve a 200k times speedup in graphics rendering is through a in- teractive parallel graphics implementation. Another is th rough a large render farm although this would most likely be non interactive. As indic ated above most inter- active parallel graphics implementations are not 100% eﬃci ent [5]. This because the communication among the processors in distributing the geo metry or in assembling the ﬁnal image causes ineﬃciency when the processors are wai ting for data to be sent or received, before they can compute. However using eﬃc ient message passing protocols and possiblyusingassembler programming50% eﬃc iency can beachieved. An interactive parallel rendering system with 400k process ors and low latency to minimize the ineﬃciency in processor communication would s eem to be the type of system most likely to pass the Graphics Turing Test. 3\nWe consider a variety of systems and their applicability to t he Graphics Turing Test: (1)Graphics Grid . If 1 million people connect their computers together in a Graphics Grid one has more than enough rendering power to cr eate 2 hours of photo realistic imagery in 2 hours wall clock time with the Gr id operating as an extremely large render farm. The diﬃculty here is in achievi ng interactive frame rates due to the slow communication time between computers. This is because the interprocessor communication on a Grid system is determine d by the network and these times are longer than those found on today supercomput ers which are highly localized. Nevertheless some interactivity can be input in to the rendering process and it is diﬃcult to beat the low cost of this system as each par ticipant provides and maintains their own computer. (2)Graphics Cluster . Agraphicclusterresembles anordinarycomputecluster with the addition of graphics cards containing GPUs that can be used for rendering as well as forphysics basedsimulation. CurrentGPU basedph otorealistic rendering such as Gelato typically run twice as fast as CPU based render ers [6]. Physics based simulationthataddsrealismtoCGIcanalsoberunontheGPUw ithspeedupsof3-5 times those found in a CPU based system [7][8]. Fast connecti on between processors using an Inﬁniband interconnect would allow interactive pa rallel rendering as well. The diﬃculty in a system of this type is that only relatively s mall Graphics Cluster systems have been used so far, at least relative to the Graphi cs Turing Scale. The largest such system is about 256 GPUs. However the interacti vity of the GPU Cluster system as well as output to large tiled display walls would make an ideal connection system between a subject and a supercomputer. (3)Supercomputer . Todays supercomputers contain thousands of processors and extreme low latency networks. For example the QCDOC syst em at BNL con- tains 12,288 1 GFlops processors and a six dimensional commu nication network with .2 microsecond latency [9]. The Altix system at NASA con tains 10,160 Ita- nium processors in a combination shared memory inﬁnibandin terconnect. TheIBM BlueGeneL system at Livermore contains 131,072 processors each with 2.8GFlops, .03 microsecond latency between nearest neighbors and .144 microsecond latency in all-to-all communication [10]. Only the last system IBM B lueGeneL with 367 TFlops peak ( 280.6 TFlops sustained) would seem to have enou gh compute power to pass the Graphics Turing Test. The last in the BlueGene ser ies BlueGeneQ is ex- pected to have 3000 TFlops peak wouldhave morethan enough to pass theGraphics Turing Test even stereoscopically. One drawback to superco mputer systems besides their large expense is that they are typically not run intera ctively. One submits a series of requests through a batch queuing system and someti me later receives the results. Nevertheless high data output can be achieved, usu ally through a large shared memory server which serves as an intermediary to move data oﬀ the super- 4\ncomputer, and large scale parallel visualization can be con sidered on these systems [11]. (4)Combination System . Perhaps the strongest approach to the building a system that can pass the Graphics Turing Test is through a com bined system of all of the above where a large supercomputer performs fast pa rallel rendering and outputs the image data to a graphics cluster which drives the displays of the motion simulator in scenario (b). The role of the Graphics Grid woul d then be used for input data from multiple users into the simulation. This asp ect of the system is important for commercial applications as we now discuss. Commercial Applications . Why would someone build a system to pass the Graphics Turing Test? Probably not to achieve an esoteric mi lestone in computer science, especially given the large cost of acquiring a supe rcomputer system. First note that synonymous with passing the Graphics Turing Test i s the achievement of the following: Artiﬁcial reality, Virtual Reality, Augm ented Reality, Cinematic Gaming, Interactive Cinema. All these descriptions are ess entially equivalent in terms of the graphics power that is required but each suggest s a diﬀerent applica- tion area. For example Virtual Reality suggests applicatio ns to physically accurate simulators, Cinematic Gaming suggests realistic multiuse r computer based games. We ﬁnd the most intriguing application area to be Interactiv e Cinema which we discuss in more detail. When one ﬁnishes watching a movie one often has the feeling that they wished it could turn out diﬀerently. With multiple u ser input each the- atrical experience will turn out diﬀerent depending on the in put from the audience. Even if the audience input is nearly uniform it is known from t he butterﬂy aﬀect that a small random input into the plot line of movie can have a large eﬀect on the ﬁnal outcome. This is the main commercial advantage of Inter active Cinema. A theater goer will plug their laptop into a network and intera ctively eﬀect the plot line. Real time photo realistic rendering of the underlying system allows the movie to be created on the ﬂy. This is essentially the appeal of vide o games, that they can be played over and over with diﬀerent experiences each tim e, as opposed a ﬁxed experience of the current cinema. By blending the two, one se eks a commercial advantage by combining the two markets. This represents a co mmercial application of the responsive medium concept of Myron Kreuger [12]. The c ost of the graphics system with 200k times the rendering capability of todays co mputers can be esti- mated at 500 million dollars. However the system can be used m ultiple times a day perhaps for several years. The market for interactive cinem a would seem to be at least 700 million dollars based on current box oﬃce statisti cs and with an increase take due to multiple viewings. Thus economics would indicat e an advantage to the creation of such a graphics system in the near future. 5\nReferences [1] A. M. Turing ”Computing Machinery and Intelligence”, Mi nd 49, 433 (1950). [2] A. P. Saygin, I. Cicekli and V. Akman, ’Turing Test: 50 Yea rs Later’, Minds and Machines 10, 463 (2000). [3] D. Barberi, ”The Ultimate Turing Test”, http://david.barberi.com/papers/ultimate.turing.tes t (1992). [4] P. Debevec, ”The Parthenon”, SIGGRAPH 2004 Electronic T heater, http://www.debevec.org/Parthenon (2004). [5] S. Tomov , R. Bennett , M. McGuigan , A. Peskin , G. Smith and J. Spiletic, ”Application of interactive parallel visualization for co mmodity-based clusters using visualization APIs”, Computers & Graphics, 28, 273 (2 004). [6] C. Batty, M. Wiebe, B. Houston, ”Frantic Films - High Perf or- mance Production-Quality Fluid Simulation via NVIDIA’s Qu adroFX” , http://ﬁlm.nvidia.com/page/publications.html(2003). [7] S. Tomov, M. McGuigan, R. Bennett, G. Smith and J. Spileti c, ”Benchmark- ing and implementation of probability-based simulations o n programmable graphics cards”, Computers & Graphics, 29, 71 (2005). [8] Z. Fan, F. Qiu, A. Kaufman, S. Yoakum-Stover ”GPU Cluster for High Perfor- mance Computing”, Proceedings ACM IEEE Supercomputing Co nference (2004). [9] N. Christ, ”QCDOC System Overview and Project Status”, P roceedings of RIKEN BNL Research Center Workshop, High Performance Compu ting with QCDOC and BlueGene (2003). [10] A.Gara, ”BlueGeneFutureDirections”, Proceedingsof RIKENBNLResearch Center Workshop, High Performance Computing with QCDOC and BlueGene (2003). [11] J. Ahrens, C. Law, K. Martin, M. Papka, W. Schroeder, ”A p arallel approach for eﬃciently visualizing extremely large, time-varying d ata sets. Technical Report LAUR-00-1620, Los Alamos National Laboratory, NM, U SA, (2000). [12] M. Kreuger, ”Artiﬁcial Reality II”, Addison Wesley, Ma ssachusetts (1991). 6\n",
  "metadata": {
    "paper_id": "cs/0603132v1",
    "downloaded_at": "2025-08-24T23:11:37.348866+00:00"
  },
  "processed_at": "2025-08-24T23:11:37.348880+00:00"
}